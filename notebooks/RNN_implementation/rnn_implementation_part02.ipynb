{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Binary addition with a non-linear RNN\n",
    "\n",
    "This part will cover:\n",
    "* Store data in [tensor](http://peterroelants.github.io/posts/rnn_implementation_part02/#Dataset)\n",
    "* Optimization with [Rmsprop and Nesterov momentum](http://peterroelants.github.io/posts/rnn_implementation_part02/#Rmsprop-with-momentum-optimisation)\n",
    "\n",
    "While the [first part](http://peterroelants.github.io/posts/rnn_implementation_part01/) of this tutorial described a simple linear recurrent network, this tutorial will describe an RNN with non-linear transfer functions that is able to learn how to perform [binary addition](https://en.wikipedia.org/wiki/Binary_number#Addition) from examples.\n",
    "\n",
    "First we import the libraries we need and define the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import itertools\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This tutorial uses a dataset of 2000 training samples to train the RNN that can be created with the `create_dataset` method defined below. Each sample consists of two 6-bit input numbers ($x_{i1}$, $x_{i2}$) padded with a 0 to make it 7 characters long, and a 7-bit target number ($t_{i}$) so that $t_{i} = x_{i1} + x_{i2}$ ($i$ is the sample index). The numbers are represented as [binary numbers](https://en.wikipedia.org/wiki/Binary_number) with the [most significant bit](https://en.wikipedia.org/wiki/Most_significant_bit) on the right (least significant bit first). This is so that our RNN can perform the addition form left to right.\n",
    "\n",
    "The input and target vectors are stored in a 3th-order tensor. A [tensor](https://en.wikipedia.org/wiki/Tensor) is a generalisation of vectors and matrices, a vector is a 1st-order tensor, a matrix is a 2nd-order tensor. The order of a tensor is the dimensionality of the array data structure needed to represent it.   \n",
    "The dimensions of our training data (`X_train`, `T_train`) are printed after the creation of the dataset below. The first order of our data tensors goes over all the samples (2000 samples), the second order goes over the variables per unit of time (7 timesteps), and the third order goes over the variables for each timestep and sample (e.g. input variables $x_{ik1}$, $x_{ik2}$ with $i$ the sample index and $k$ the timestep). The input tensor `X_train` is visualised in the following figure:\n",
    "\n",
    "![Visualisation of input tensor X](https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN02_Tensor.png)\n",
    "\n",
    "The following code block initialises the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2000, 7, 2)\n",
      "T_train shape: (2000, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "nb_train = 2000  # Number of training samples\n",
    "# Addition of 2 n-bit numbers can result in a n+1 bit number\n",
    "sequence_len = 7  # Length of the binary sequence\n",
    "\n",
    "def create_dataset(nb_samples, sequence_len):\n",
    "    \"\"\"Create a dataset for binary addition and return as input, targets.\"\"\"\n",
    "    max_int = 2**(sequence_len-1) # Maximum integer that can be added\n",
    "    format_str = '{:0' + str(sequence_len) + 'b}' # Transform integer in binary format\n",
    "    nb_inputs = 2  # Add 2 binary numbers\n",
    "    nb_outputs = 1  # Result is 1 binary number\n",
    "    X = np.zeros((nb_samples, sequence_len, nb_inputs))  # Input samples\n",
    "    T = np.zeros((nb_samples, sequence_len, nb_outputs))  # Target samples\n",
    "    # Fill up the input and target matrix\n",
    "    for i in xrange(nb_samples):\n",
    "        # Generate random numbers to add\n",
    "        nb1 = np.random.randint(0, max_int)\n",
    "        nb2 = np.random.randint(0, max_int)\n",
    "        # Fill current input and target row.\n",
    "        # Note that binary numbers are added from right to left, but our RNN reads \n",
    "        #  from left to right, so reverse the sequence.\n",
    "        X[i,:,0] = list(reversed([int(b) for b in format_str.format(nb1)]))\n",
    "        X[i,:,1] = list(reversed([int(b) for b in format_str.format(nb2)]))\n",
    "        T[i,:,0] = list(reversed([int(b) for b in format_str.format(nb1+nb2)]))\n",
    "    return X, T\n",
    "\n",
    "# Create training samples\n",
    "X_train, T_train = create_dataset(nb_train, sequence_len)\n",
    "print('X_train shape: {0}'.format(X_train.shape))\n",
    "print('T_train shape: {0}'.format(T_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary addition\n",
    "\n",
    "Performing binary addition is a good [toy problem](https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf) to illustrate how recurrent neural networks process input streams into output streams. The network needs to learn how to carry a bit to the next state (memory) and when to output a 0 or 1 dependent on the input and state.\n",
    "\n",
    "The following code prints a visualisation of the inputs and target output we want our network to produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:   1010010   37\n",
      "x2: + 1101010   43 \n",
      "      -------   --\n",
      "t:  = 0000101   80\n"
     ]
    }
   ],
   "source": [
    "# Show an example input and target\n",
    "def printSample(x1, x2, t, y=None):\n",
    "    \"\"\"Print a sample in a more visual way.\"\"\"\n",
    "    x1 = ''.join([str(int(d)) for d in x1])\n",
    "    x2 = ''.join([str(int(d)) for d in x2])\n",
    "    t = ''.join([str(int(d[0])) for d in t])\n",
    "    if not y is None:\n",
    "        y = ''.join([str(int(d[0])) for d in y])\n",
    "    print('x1:   {:s}   {:2d}'.format(x1, int(''.join(reversed(x1)), 2)))\n",
    "    print('x2: + {:s}   {:2d} '.format(x2, int(''.join(reversed(x2)), 2)))\n",
    "    print('      -------   --')\n",
    "    print('t:  = {:s}   {:2d}'.format(t, int(''.join(reversed(t)), 2)))\n",
    "    if not y is None:\n",
    "        print('y:  = {:s}'.format(y))\n",
    "    \n",
    "# Print the first sample\n",
    "printSample(X_train[0,:,0], X_train[0,:,1], T_train[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network architecture\n",
    "\n",
    "Our recurrent network will take 2 input variables for each sample for each timepoint, transform them to states, and output a single probability that the current output is $1$ (instead of $0$). The input is transformed into the states of the RNN where it can hold information so the network knows what to output the next timestep.\n",
    "\n",
    "There are many ways to visualise the RNN we are going to build. We can visualise the network as in the [previous part](http://peterroelants.github.io/posts/rnn_implementation_part01/) of our tutorial and unfold the processing of each input, state-update and output of a single timestep separately from the other timesteps. \n",
    "\n",
    "![Structure of the RNN](https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN02_1.png)\n",
    "\n",
    "Or we can view the processing of the full input, state-updates, and full output seperately from each other. The full input tensor can be [mapped](https://en.wikipedia.org/wiki/Map_%28higher-order_function%29) in parallel to be used directly in the RNN state updates. And also the RNN states can be mapped in parallel to the output of each timestep.\n",
    "\n",
    "![Structure of the RNN tensor processing](https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN02_2.png)\n",
    "\n",
    "The steps are abstracted in different classes below. Each class has a `forward` method that performs the [forward steps](http://peterroelants.github.io/posts/neural_network_implementation_part03/#1.-Forward-step) of backpropagation, and a `backward` method that perform the [backward](http://peterroelants.github.io/posts/neural_network_implementation_part03/#2.-Backward-step) steps of backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of input and output tensors\n",
    "\n",
    "\n",
    "#### Linear transformation\n",
    "\n",
    "Neural networks typically transform input vectors by matrix multiplication and vector addition followed by a non-linear transfer function. The 2-dimensional input vectors to our network ($x_{ik1}$, $x_{ik2}$) are transformed by a $2 \\times 3$ weight matrix and a bias vector of size 3. Before they can be added to the states of the RNN. The 3-dimensional state vectors are transformed to a 1-dimensional output vector by a $3 \\times 1$ weight matrix and a bias vector of size 1 to give the output probabilities.\n",
    "\n",
    "Since we want to process all inputs for each sample and each timestep in one computation we can use the numpy [`tensordot`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.tensordot.html) function to perform the dot products. This function takes 2 tensors and the axes that need to be aggregated by summation between the elements and a product of the result. For example the transformation of input X ($2000 \\times 7 \\times 2$) to the states S ($2000 \\times 7 \\times 3$) with the help of matrix W ($2 \\times 3$) can be done by `S = tensordot(X, W, axes=((-1),(0)))`. This method will sum the elements of the last order (-1) of X with the elements of the first order (0) of W and multiply them together. This is the same as doing the matrix dot product for each [$x_{ik1}$, $x_{ik2}$] vector with W. `tensordot` can then make sure that the underlying computations can be done efficiently and in parallel.\n",
    "\n",
    "These linear tensor transformations are used to transform the input X to the states S, and from the states S to the output Y. This linear transformation, together with its gradient is implemented in the `TensorLinear` class below. Note that the weights are initialized by sampling uniformly between $\\pm \\sqrt{6.0 / (n_{in} + n_{out})}$ [as suggested by X. Glorot](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf).\n",
    "\n",
    "\n",
    "#### Logistic classification\n",
    "\n",
    "[Logistic classification](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo01/) is used to output the probability that the output at current time step k is 1. This function together with its cost and gradient is implemented in the `LogisticClassifier` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the linear tensor transformation layer\n",
    "class TensorLinear(object):\n",
    "    \"\"\"The linear tensor layer applies a linear tensor dot product and a bias to its input.\"\"\"\n",
    "    def __init__(self, n_in, n_out, tensor_order, W=None, b=None):\n",
    "        \"\"\"Initialse the weight W and bias b parameters.\"\"\"\n",
    "        a = np.sqrt(6.0 / (n_in + n_out))\n",
    "        self.W = (np.random.uniform(-a, a, (n_in, n_out)) if W is None else W)\n",
    "        self.b = (np.zeros((n_out)) if b is None else b)  # Bias paramters\n",
    "        self.bpAxes = tuple(range(tensor_order-1))  # Axes summed over in backprop\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform forward step transformation with the help of a tensor product.\"\"\"\n",
    "        # Same as: Y[i,j,:] = np.dot(X[i,j,:], self.W) + self.b (for i,j in X.shape[0:1])\n",
    "        # Same as: Y = np.einsum('ijk,kl->ijl', X, self.W) + self.b\n",
    "        return np.tensordot(X, self.W, axes=((-1),(0))) + self.b\n",
    "\n",
    "    def backward(self, X, gY):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of this layer.\"\"\"\n",
    "        # Same as: gW = np.einsum('ijk,ijl->kl', X, gY)\n",
    "        # Same as: gW += np.dot(X[:,j,:].T, gY[:,j,:]) (for i,j in X.shape[0:1])\n",
    "        gW = np.tensordot(X, gY, axes=(self.bpAxes, self.bpAxes))\n",
    "        gB = np.sum(gY, axis=self.bpAxes)\n",
    "        # Same as: gX = np.einsum('ijk,kl->ijl', gY, self.W.T)\n",
    "        # Same as: gX[i,j,:] = np.dot(gY[i,j,:], self.W.T) (for i,j in gY.shape[0:1])\n",
    "        gX = np.tensordot(gY, self.W.T, axes=((-1),(0)))  \n",
    "        return gX, gW, gB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the logistic classifier layer\n",
    "class LogisticClassifier(object):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "   \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    def backward(self, Y, T):\n",
    "        \"\"\"Return the gradient with respect to the cost function at the inputs of this layer.\"\"\"\n",
    "        # Normalise of the number of samples and sequence length.\n",
    "        return (Y - T) / (Y.shape[0] * Y.shape[1])\n",
    "    \n",
    "    def cost(self, Y, T):\n",
    "        \"\"\"Compute the cost at the output.\"\"\"\n",
    "        # Normalise of the number of samples and sequence length.\n",
    "        # Add a small number (1e-99) because Y can become 0 if the network learns\n",
    "        #  to perfectly predict the output. log(0) is undefined.\n",
    "        return - np.sum(np.multiply(T, np.log(Y+1e-99)) + np.multiply((1-T), np.log(1-Y+1e-99))) / (Y.shape[0] * Y.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfolding the recurrent states\n",
    "\n",
    "Just as in the [previous part]((http://peterroelants.github.io/posts/rnn_implementation_part01/) of this tutorial the recurrent states need to be unfolded through time. This unfolding during [backpropagation through time](https://en.wikipedia.org/wiki/Backpropagation_through_time) is done by the `RecurrentStateUnfold` class. This class holds the shared weight and bias parameters used to update each state, as well as the initial state that is also treated as a parameter and optimized during backpropagation.\n",
    "\n",
    "The `forward` method of `RecurrentStateUnfold` iteratively updates the states through time and returns the resulting state tensor. The `backward` method propagates the gradients at the outputs of each state backwards through time. Note that at each time $k$ the gradient coming from the output Y needs to be added with the gradient coming from the previous state at time $k+1$. The gradients of the weight and bias parameters are summed over all timestep since they are shared parameters in each state update. The final state gradient at time $k=0$ is used to optimise the initial state $S_0$ since the gradient of the inital state is $\\partial \\xi / \\partial S_{0}$.\n",
    "\n",
    "`RecurrentStateUnfold` makes use of the `RecurrentStateUpdate` class. The `forward` method of this class combines the transformed input and state at time $k-1$ to output state $k$. The `backward` method propagates the gradient backwards through time for one timestep and calculates the gradients of the parameters of this timestep. The non-linear transfer function used in `RecurrentStateUpdate` is the [hyperbolic tangent](https://en.wikipedia.org/wiki/Hyperbolic_function) (tanh) function. This function, like the logistic function, is a [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) that goes from $-1$ to $+1$. The [tanh function]( https://theclevermachine.wordpress.com/tag/tanh-function/) is chosen because the maximum gradient of this function is higher than the maximum gradient of the [logistic function](http://peterroelants.github.io/posts/neural_network_implementation_part02/#Logistic-function) which make vanishing gradients less likely. This tanh transfer function is implemented in the `TanH` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define tanh layer\n",
    "class TanH(object):\n",
    "    \"\"\"TanH applies the tanh function to its inputs.\"\"\"\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return np.tanh(X) \n",
    "    \n",
    "    def backward(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        gTanh = 1.0 - np.power(Y,2)\n",
    "        return np.multiply(gTanh, output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define internal state update layer\n",
    "class RecurrentStateUpdate(object):\n",
    "    \"\"\"Update a given state.\"\"\"\n",
    "    def __init__(self, nbStates, W, b):\n",
    "        \"\"\"Initialse the linear transformation and tanh transfer function.\"\"\"\n",
    "        self.linear = TensorLinear(nbStates, nbStates, 2, W, b)\n",
    "        self.tanh = TanH()\n",
    "\n",
    "    def forward(self, Xk, Sk):\n",
    "        \"\"\"Return state k+1 from input and state k.\"\"\"\n",
    "        return self.tanh.forward(Xk + self.linear.forward(Sk))\n",
    "    \n",
    "    def backward(self, Sk0, Sk1, output_grad):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of this layer.\"\"\"\n",
    "        gZ = self.tanh.backward(Sk1, output_grad)\n",
    "        gSk0, gW, gB = self.linear.backward(Sk0, gZ)\n",
    "        return gZ, gSk0, gW, gB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define layer that unfolds the states over time\n",
    "class RecurrentStateUnfold(object):\n",
    "    \"\"\"Unfold the recurrent states.\"\"\"\n",
    "    def __init__(self, nbStates, nbTimesteps):\n",
    "        \" Initialse the shared parameters, the inital state and state update function.\"\n",
    "        a = np.sqrt(6.0 / (nbStates * 2))\n",
    "        self.W = np.random.uniform(-a, a, (nbStates, nbStates))\n",
    "        self.b = np.zeros((self.W.shape[0]))  # Shared bias\n",
    "        self.S0 = np.zeros(nbStates)  # Initial state\n",
    "        self.nbTimesteps = nbTimesteps  # Timesteps to unfold\n",
    "        self.stateUpdate = RecurrentStateUpdate(nbStates, self.W, self.b)  # State update function\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Iteratively apply forward step to all states.\"\"\"\n",
    "        S = np.zeros((X.shape[0], X.shape[1]+1, self.W.shape[0]))  # State tensor\n",
    "        S[:,0,:] = self.S0  # Set initial state\n",
    "        for k in range(self.nbTimesteps):\n",
    "            # Update the states iteratively\n",
    "            S[:,k+1,:] = self.stateUpdate.forward(X[:,k,:], S[:,k,:])\n",
    "        return S\n",
    "    \n",
    "    def backward(self, X, S, gY):\n",
    "        \"\"\"Return the gradient of the parmeters and the inputs of this layer.\"\"\"\n",
    "        gSk = np.zeros_like(gY[:,self.nbTimesteps-1,:])  # Initialise gradient of state outputs\n",
    "        gZ = np.zeros_like(X)  # Initialse gradient tensor for state inputs\n",
    "        gWSum = np.zeros_like(self.W)  # Initialise weight gradients\n",
    "        gBSum = np.zeros_like(self.b)  # Initialse bias gradients\n",
    "        # Propagate the gradients iteratively\n",
    "        for k in range(self.nbTimesteps-1, -1, -1):\n",
    "            # Gradient at state output is gradient from previous state plus gradient from output\n",
    "            gSk += gY[:,k,:]\n",
    "            # Propgate the gradient back through one state\n",
    "            gZ[:,k,:], gSk, gW, gB = self.stateUpdate.backward(S[:,k,:], S[:,k+1,:], gSk)\n",
    "            gWSum += gW  # Update total weight gradient\n",
    "            gBSum += gB  # Update total bias gradient\n",
    "        gS0 = np.sum(gSk, axis=0)  # Get gradient of initial state over all samples\n",
    "        return gZ, gWSum, gBSum, gS0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The full network\n",
    "\n",
    "The full network that will be trained to perform binary addition of two number is defined in the `RnnBinaryAdder` class below. It initialises all the layers upon creation. The `forward` method performs the full backpropagation forward step through all layers and timesteps and returns the intermediary outputs. The `backward` method performs the backward step through all layers and timesteps and returns the gradients of all the parameters. The `getParamGrads` method performs both steps and returns the gradients of the parameters in a list. The order of this list corresponds to the order of the iterator returned by `get_params_iter`. The parameters returned in the iterator of that last method are the same as the parameters of the network and can be used to change the parameters of the network manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the full network\n",
    "class RnnBinaryAdder(object):\n",
    "    \"\"\"RNN to perform binary addition of 2 numbers.\"\"\"\n",
    "    def __init__(self, nb_of_inputs, nb_of_outputs, nb_of_states, sequence_len):\n",
    "        \"\"\"Initialse the network layers.\"\"\"\n",
    "        self.tensorInput = TensorLinear(nb_of_inputs, nb_of_states, 3)  # Input layer\n",
    "        self.rnnUnfold = RecurrentStateUnfold(nb_of_states, sequence_len)  # Recurrent layer\n",
    "        self.tensorOutput = TensorLinear(nb_of_states, nb_of_outputs, 3)  # Linear output transform\n",
    "        self.classifier = LogisticClassifier()  # Classification output\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Perform the forward propagation of input X through all layers.\"\"\"\n",
    "        recIn = self.tensorInput.forward(X)  # Linear input transformation\n",
    "        # Forward propagate through time and return states\n",
    "        S = self.rnnUnfold.forward(recIn)\n",
    "        Z = self.tensorOutput.forward(S[:,1:sequence_len+1,:])  # Linear output transformation\n",
    "        Y = self.classifier.forward(Z)  # Get classification probabilities\n",
    "        # Return: input to recurrent layer, states, input to classifier, output\n",
    "        return recIn, S, Z, Y\n",
    "    \n",
    "    def backward(self, X, Y, recIn, S, T):\n",
    "        \"\"\"Perform the backward propagation through all layers.\n",
    "        Input: input samples, network output, intput to recurrent layer, states, targets.\"\"\"\n",
    "        gZ = self.classifier.backward(Y, T)  # Get output gradient\n",
    "        gRecOut, gWout, gBout = self.tensorOutput.backward(S[:,1:sequence_len+1,:], gZ)\n",
    "        # Propagate gradient backwards through time\n",
    "        gRnnIn, gWrec, gBrec, gS0 = self.rnnUnfold.backward(recIn, S, gRecOut)\n",
    "        gX, gWin, gBin = self.tensorInput.backward(X, gRnnIn)\n",
    "        # Return the parameter gradients of: linear output weights, linear output bias,\n",
    "        #  recursive weights, recursive bias, linear input weights, linear input bias, initial state.\n",
    "        return gWout, gBout, gWrec, gBrec, gWin, gBin, gS0\n",
    "    \n",
    "    def getOutput(self, X):\n",
    "        \"\"\"Get the output probabilities of input X.\"\"\"\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        return Y  # Only return the output.\n",
    "    \n",
    "    def getBinaryOutput(self, X):\n",
    "        \"\"\"Get the binary output of input X.\"\"\"\n",
    "        return np.around(self.getOutput(X))\n",
    "    \n",
    "    def getParamGrads(self, X, T):\n",
    "        \"\"\"Return the gradients with respect to input X and target T as a list.\n",
    "        The list has the same order as the get_params_iter iterator.\"\"\"\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        gWout, gBout, gWrec, gBrec, gWin, gBin, gS0 = self.backward(X, Y, recIn, S, T)\n",
    "        return [g for g in itertools.chain(\n",
    "                np.nditer(gS0),\n",
    "                np.nditer(gWin),\n",
    "                np.nditer(gBin),\n",
    "                np.nditer(gWrec),\n",
    "                np.nditer(gBrec),\n",
    "                np.nditer(gWout),\n",
    "                np.nditer(gBout))]\n",
    "    \n",
    "    def cost(self, Y, T):\n",
    "        \"\"\"Return the cost of input X w.r.t. targets T.\"\"\"\n",
    "        return self.classifier.cost(Y, T)\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return itertools.chain(\n",
    "            np.nditer(self.rnnUnfold.S0, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorOutput.W, op_flags=['readwrite']), \n",
    "            np.nditer(self.tensorOutput.b, op_flags=['readwrite']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "As in [part 4 of our previous tutorial on feedforward nets](http://peterroelants.github.io/posts/neural_network_implementation_part04/#Gradient-checking) the gradient computed by backpropagation is compared with the [numerical gradient](https://en.wikipedia.org/wiki/Numerical_differentiation) to assert that there are no bugs in the code to compute the gradients. This is done by the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gradient errors found\n"
     ]
    }
   ],
   "source": [
    "# Do gradient checking\n",
    "# Define an RNN to test\n",
    "RNN = RnnBinaryAdder(2, 1, 3, sequence_len)\n",
    "# Get the gradients of the parameters from a subset of the data\n",
    "backprop_grads = RNN.getParamGrads(X_train[0:100,:,:], T_train[0:100,:,:])\n",
    "\n",
    "eps = 1e-7  # Set the small change to compute the numerical gradient\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for p_idx, param in enumerate(RNN.get_params_iter()):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    # + eps\n",
    "    param += eps\n",
    "    plus_cost = RNN.cost(RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])\n",
    "    # - eps\n",
    "    param -= 2 * eps\n",
    "    min_cost = RNN.cost(RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])\n",
    "    # reset param value\n",
    "    param += eps\n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_cost - min_cost)/(2*eps)\n",
    "    # Raise error if the numerical grade is not close to the backprop gradient\n",
    "    if not np.isclose(grad_num, grad_backprop):\n",
    "        raise ValueError('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\n",
    "print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rmsprop with momentum optimisation\n",
    "\n",
    "While the [first part](http://peterroelants.github.io/posts/rnn_implementation_part01/) of this tutorial used [Rprop](https://en.wikipedia.org/wiki/Rprop) to optimise the network, this part will use the [Rmsprop](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) algorithm with [Nesterov's accelerated gradient](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf) to perform the optimisation. We replaced the Rprop algorithm because Rprop doesn't work well with [minibatches](http://peterroelants.github.io/posts/neural_network_implementation_part05/#Stochastic-gradient-descent-backpropagation) due to the stochastic nature of the error surface that can result in sign changes of the gradient.\n",
    "\n",
    "The Rmsprop algorithm was inspired by the Rprop algorithm. It keeps a [moving average](https://en.wikipedia.org/wiki/Moving_average) (MA) of the squared gradient for each parameter $\\theta$ ($MA = \\lambda * MA + (1-\\lambda) * (\\partial \\xi / \\partial \\theta)^2$, with $\\lambda$ the moving average hyperparameter). The gradient is then normalised by dividing by the square root of this moving average (`maSquare` = $(\\partial \\xi / \\partial \\theta)/\\sqrt{MA}$). This normalised gradient is then used to update the parameters. Note that if $\\lambda=0$ the gradient is reduced to its sign.\n",
    "\n",
    "This transformed gradient is not used directly to update the parameters, but it is used to update a velocity parameter (`Vs`) for each parameter of the network. This parameter is similar to the velocity parameter from our [previous tutorial](http://peterroelants.github.io/posts/neural_network_implementation_part04/#Backpropagation-updates-with-momentum) but it is used in a slightly different way. Nesterov's accelerated gradient is different from regular momentum in that it applies updates in a different way. While the regular momentum algorithm calculates the gradient at the beginning of the iteration, updates the velocity and moves the parameters according to this velocity, Nesterov's accelerated gradient moves the parameters according to the reduced velocity, then calculates the gradients, updates the velocity, and then moves again according to the local gradient. This has as benefit that the gradient is more informative to do the local update, and can correct for a bad velocity update. The Nesterov updates can be described as:\n",
    "\n",
    "$$\\begin{split}\n",
    "V_{i+1} & = \\lambda V_i - \\mu \\nabla(\\theta_i + \\lambda V_i) \\\\\n",
    "\\theta_{i+1} & = \\theta_i + V_{i+1} \\\\\n",
    "\\end{split}$$\n",
    "\n",
    "With $\\nabla(\\theta)$ the local gradient at position $\\theta$ in the parameter space. And $i$ the iteration number. This formula can be visualised as in the following illustration (See [Sutskever I.](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)):\n",
    "\n",
    "![Illustration of Nesterov Momentum updates](https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/NesterovMomentum.png)\n",
    "\n",
    "Note that the training converges to a cost of 0. This convergence is actually not guaranteed. If the parameters of the network start out in a bad position the network might convert to a local minimum that is far from the global minimum. The training is also sensitive to the meta parameters `lmbd`, `learning_rate`, `momentum_term`, `eps`. Try rerunning this yourself to see how many times it actually converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set hyper-parameters\n",
    "lmbd = 0.5  # Rmsprop lambda\n",
    "learning_rate = 0.05  # Learning rate\n",
    "momentum_term = 0.80  # Momentum term\n",
    "eps = 1e-6  # Numerical stability term to prevent division by zero\n",
    "mb_size = 100  # Size of the minibatches (number of samples)\n",
    "\n",
    "# Create the network\n",
    "nb_of_states = 3  # Number of states in the recurrent layer\n",
    "RNN = RnnBinaryAdder(2, 1, nb_of_states, sequence_len)\n",
    "# Set the initial parameters\n",
    "nbParameters =  sum(1 for _ in RNN.get_params_iter())  # Number of parameters in the network\n",
    "maSquare = [0.0 for _ in range(nbParameters)]  # Rmsprop moving average\n",
    "Vs = [0.0 for _ in range(nbParameters)]  # Velocity\n",
    "\n",
    "# Create a list of minibatch costs to be plotted\n",
    "ls_of_costs = [RNN.cost(RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])]\n",
    "# Iterate over some iterations\n",
    "for i in range(5):\n",
    "    # Iterate over all the minibatches\n",
    "    for mb in range(nb_train/mb_size):\n",
    "        X_mb = X_train[mb:mb+mb_size,:,:]  # Input minibatch\n",
    "        T_mb = T_train[mb:mb+mb_size,:,:]  # Target minibatch\n",
    "        V_tmp = [v * momentum_term for v in Vs]\n",
    "        # Update each parameters according to previous gradient\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            P += V_tmp[pIdx]\n",
    "        # Get gradients after following old velocity\n",
    "        backprop_grads = RNN.getParamGrads(X_mb, T_mb)  # Get the parameter gradients    \n",
    "        # Update each parameter seperately\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            # Update the Rmsprop moving averages\n",
    "            maSquare[pIdx] = lmbd * maSquare[pIdx] + (1-lmbd) * backprop_grads[pIdx]**2\n",
    "            # Calculate the Rmsprop normalised gradient\n",
    "            pGradNorm = learning_rate * backprop_grads[pIdx] / np.sqrt(maSquare[pIdx] + eps)\n",
    "            # Update the momentum velocity\n",
    "            Vs[pIdx] = V_tmp[pIdx] - pGradNorm     \n",
    "            P -= pGradNorm   # Update the parameter\n",
    "        ls_of_costs.append(RNN.cost(RNN.getOutput(X_mb), T_mb))  # Add cost to list to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEZCAYAAABvpam5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhIh6l4iIhIh6l4iIhIh/0/3zrCEX9a5lEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105077bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cost over the iterations\n",
    "plt.plot(ls_of_costs, 'b-')\n",
    "plt.xlabel('minibatch iteration')\n",
    "plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Decrease of cost over backprop iteration')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test examples\n",
    "\n",
    "The figure above shows that the training converged to a cost of 0. We expect the network to have learned how to perfectly do binary addition for our training examples. If we put some independent test cases through the network and print them out we can see that the network also outputs the correct output for these test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:   0100010   34\n",
      "x2: + 1100100   19 \n",
      "      -------   --\n",
      "t:  = 1010110   53\n",
      "y:  = 1010110\n",
      "\n",
      "x1:   1010100   21\n",
      "x2: + 1110100   23 \n",
      "      -------   --\n",
      "t:  = 0011010   44\n",
      "y:  = 0011010\n",
      "\n",
      "x1:   1111010   47\n",
      "x2: + 0000000    0 \n",
      "      -------   --\n",
      "t:  = 1111010   47\n",
      "y:  = 1111010\n",
      "\n",
      "x1:   1000000    1\n",
      "x2: + 1111110   63 \n",
      "      -------   --\n",
      "t:  = 0000001   64\n",
      "y:  = 0000001\n",
      "\n",
      "x1:   1010100   21\n",
      "x2: + 1010100   21 \n",
      "      -------   --\n",
      "t:  = 0101010   42\n",
      "y:  = 0101010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test samples\n",
    "nb_test = 5\n",
    "Xtest, Ttest = create_dataset(nb_test, sequence_len)\n",
    "# Push test data through network\n",
    "Y = RNN.getBinaryOutput(Xtest)\n",
    "Yf = RNN.getOutput(Xtest)\n",
    "\n",
    "# Print out all test examples\n",
    "for i in range(Xtest.shape[0]):\n",
    "    printSample(Xtest[i,:,0], Xtest[i,:,1], Ttest[i,:,:], Y[i,:,:])\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post at [peterroelants.github.io](http://peterroelants.github.io/posts/rnn_implementation_part02/) is generated from an IPython notebook file. [Link to the full IPython notebook file](https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/RNN_implementation/rnn_implementation_part02.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

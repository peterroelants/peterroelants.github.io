{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Matching: A visual introduction\n",
    "\n",
    "Flow Matching (FM) has become a prevalent technique to train a certain class of generative models. In this post we'll try to explore the intuition behind flow matching and how it works.\n",
    "\n",
    "We'll use this notebook to build a simple flow matching model illustrating linear flow matching based on a minimal toy example. Our goal is to try to keep things simple, intuitive, and visual. We won't be doing any deep dive into the mathematical details of the model, if you're interested in the mathematical details I recommend checking out the references at the end of this post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import base64\n",
    "import functools\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style(\"darkgrid\")  # Set the style of the plots\n",
    "pd.options.display.float_format = \"{:,.3f}\".format  # Table display format\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(626)\n",
    "\n",
    "# PyTorch Device configuration\n",
    "DEVICE: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow matching\n",
    "\n",
    "Flow matching is a technique to learn how to transport samples from one distribution to another. For example we could learn how to transport samples from a simple distribution we can easily sample from (e.g. Gaussian noise) to a complex distribution (e.g. [images](https://arxiv.org/abs/2210.02747), [videos](https://arxiv.org/abs/2410.13720), [robot actions](https://arxiv.org/abs/2409.01083), etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example: Mapping Gaussian noise to a bimodal distribution\n",
    "\n",
    "In this post we'll build a simple toy example of a generative model using flow matching. For illustrative purposes we'll start with a simple 1D bimodal target distribution $π_1$ and learn how to transport samples from a 1D Gaussian noise distribution $π_0$ to this target distribution.\n",
    "\n",
    "In practice the target points $x_1 \\sim π_1$ are approximated by sampling from a limited dataset of training points $X_1$ and the noise points $x_0 \\sim π_0$ are sampled from a chosen noise distribution $π_0$ that is easy to sample from (e.g. Gaussian noise).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 1D bimodal target distribution\n",
    "mixture_prob = np.array([0.55, 0.45], dtype=float)  # Mixture weights\n",
    "mixture_mus = np.array([-0.85, 1.5], dtype=float)  # Means of the two Gaussian modes\n",
    "mixture_sigmas = np.array([0.65, 0.25], dtype=float)  # Standard deviations of the modes\n",
    "\n",
    "\n",
    "def mixture_pdf(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute the PDF of a mixture of Gaussians.\"\"\"\n",
    "    comps = scipy.stats.norm.pdf(x[None, :], loc=mixture_mus[:, None], scale=mixture_sigmas[:, None])\n",
    "    return np.sum(mixture_prob[:, None] * comps, axis=0)\n",
    "\n",
    "\n",
    "def mixture_sample(size: int) -> np.ndarray:\n",
    "    \"\"\"Sample from a mixture of Gaussians.\"\"\"\n",
    "    rand_idx = np.random.choice(range(len(mixture_prob)), size=size, p=mixture_prob)\n",
    "    means = mixture_mus[rand_idx]\n",
    "    stds = mixture_sigmas[rand_idx]\n",
    "    return np.random.normal(loc=means, scale=stds)\n",
    "\n",
    "\n",
    "# Plot data distribution. This is the TARGET distribution (π₁)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 3), constrained_layout=True, dpi=100)\n",
    "x_all_steps = np.linspace(-3, 3, 1000)\n",
    "pdf_noise = scipy.stats.norm.pdf(x_all_steps, loc=0, scale=1)\n",
    "ax.plot(x_all_steps, pdf_noise, label=\"PDF Noise π₀\", color=\"tab:orange\")\n",
    "ax.fill_between(x_all_steps, pdf_noise, alpha=0.4, color=\"tab:orange\")\n",
    "pdf_target = mixture_pdf(x=x_all_steps)\n",
    "ax.plot(x_all_steps, pdf_target, label=\"PDF Target π₁\", color=\"tab:blue\")\n",
    "ax.fill_between(x_all_steps, pdf_target, alpha=0.4, color=\"tab:blue\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Toy Example Data: Noise (π₀) vs Target (π₁) Distributions\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"density\")\n",
    "plt.show()\n",
    "del fig, ax, x_all_steps, pdf_noise, pdf_target\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The flow matching model predicts a velocity field\n",
    "\n",
    "A flow matching model does not predict flow paths directly, but instead predicts a velocity field that can be used to sample the flow paths. The velocity field describes how to move a sample from the noise distribution to the target distribution.\n",
    "\n",
    "We can describe the flow matching model with learnable parameters $\\theta$ as a function:\n",
    "$${FM}_{\\theta}(x_t, t) = v(x_t, t)$$\n",
    "This function takes a sample $x_t$ at flow step $t$ and predicts the velocity vector $v(x_t, t) = dx_t / dt$ that describes how to move the sample $x_t$ closer to the target distribution at step $t$.\n",
    "\n",
    "The step $t$ is a value between 0 and 1 that describes the progress of the sample $x_t$ along the flow path from the noise distribution to the target distribution. When $t=0$ the sample $x_t = x_0$ is a sample from the noise distribution $π_0$ and when $t=1$ the sample $x_t = x_1$ is a sample from the target distribution $π_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At inference time we can sample a starting point $x_0$ from the noise distribution $π_0$ and then use the predicted velocity field ${FM}_{\\theta}(x_t, t)$ to iteratively move the sample towards the target distribution $π_1$ in small steps $dt$\n",
    "\n",
    "This is illustrated in the following animation ([generated further down in the notebook](#Sampling-from-the-trained-model)) which shows the integration of a sample from the noise distribution $π_0$ on the left towards the target distribution $π_1$ on the right using the predicted velocity field ${FM}_{\\theta}(x_t, t)$. The velocity field is visualized as a heatmap where the vertical axis represents the position of the sample $x_t$ and the horizontal axis represents the flow step $t$ going from 0 on the left to 1 on the right. Red means a positive velocity (sample pushed up towards higher $x$) and blue means a negative velocity (sample pulled down towards lower $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the animation in the notebook\n",
    "# This animation is generated further down in the notebook, if it doesn't exist yet we'll skip the display\n",
    "\n",
    "# Embed the GIF directly in the notebook by encoding the bytes as base64, this way it should hopefully also be exported\n",
    "ANIMATION_FILE = Path(\"flow_matching_path_integration.mp4\")\n",
    "if ANIMATION_FILE.exists():\n",
    "    with ANIMATION_FILE.open(\"rb\") as f:\n",
    "        gif_data = f.read()\n",
    "    display(\n",
    "        HTML(f\"\"\"\n",
    "    <video alt=\"Flow matching path integration\" autoplay muted loop playsinline\n",
    "    style=\"width:100%;height:auto;max-height:90vh;object-fit:contain;display:block;margin:0 auto;\">\n",
    "        <source type=\"video/mp4\" src=\"data:video/mp4;base64,{base64.b64encode(gif_data).decode()}\">\n",
    "    </video>\n",
    "    \"\"\")\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Animation file not yet created, it is generated further down in the notebook. Run the full notebook to generate it.\"\n",
    "    )\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the flow matching model is learning the velocity field\n",
    "\n",
    "Since the flow matching model ${FM}_{\\theta}(x_t, t)$ should predict the velocity field $v(x_t, t) = dx_t / dt$ we can train the model on samples of velocity vectors $\\mathbf{v}(x_t, t)$.\n",
    "\n",
    "The flow matching training objective is to minimize the expected reconstruction error of the velocity field:\n",
    "$$\n",
    "\\underset{\\theta}{\\text{argmin}} \\; \\mathbb{E}_{t, x_t} \\Big\\| {FM}_{\\theta}(x_t, t) - v(x_t, t) \\Big\\|^2\n",
    "$$\n",
    "\n",
    "with $t \\sim \\mathcal{U}[0, 1]$ and $x_t$ taken from a sampled reference path evaluated at flow step $t$.\n",
    "\n",
    "We'll be using straight line reference paths in this post since they are simple and common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training: Straight line reference paths\n",
    "We're going to focus on a common variant of [flow matching](https://arxiv.org/abs/2210.02747) where we learn a flow matching model based on straight line reference paths. Training flow matching with straight-line conditional paths and independent couplings is also equivalent to the [rectified flow](https://arxiv.org/abs/2209.03003) training objective.\n",
    "\n",
    "Linear (straight line) flow matching is trained on a set of reference paths between the noise and target distributions. More specifically, linear flow matching prefers learning from straight line trajectories between the noise and target distributions because they tend to give straighter paths that require fewer steps to reconstruct the target distribution.\n",
    "\n",
    "To sample a reference path we can independently sample a target point $x_1$ from our target distribution $π_1$ and independently sample a noise point $x_0$ from the noise distribution $π_0$. This gives us a single coupling $(x_0, x_1)$ that allows us to define a straight line reference path between the noise and target samples. During training we'll sample a large set of coupling-inducing paths $(X_0, X_1)$ and use these to train the flow matching model.\n",
    "\n",
    "\n",
    "The following code illustrates how we define the straight line reference path between a noise and target sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_linear(x_0, x_1, t):\n",
    "    \"\"\"Evaluates the linear interpolation path between x_0 and x_1 at step t.\"\"\"\n",
    "    x_t = ((1 - t) * x_0) + (t * x_1)\n",
    "    return x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows a few sampled straight-line reference paths, as well as the reference path distribution approximated by sampling a large number of straight-line reference paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of the sampled reference paths\n",
    "# Set up the plot\n",
    "fig, ((ax11, ax12, ax13), (ax21, ax22, ax23)) = plt.subplots(\n",
    "    2,\n",
    "    3,\n",
    "    figsize=(12, 8),\n",
    "    gridspec_kw={\"width_ratios\": [1, 5, 1]},\n",
    "    sharey=True,\n",
    "    dpi=100,\n",
    ")\n",
    "fig.subplots_adjust(wspace=0)\n",
    "x_min, x_max = -2.5, 2.5\n",
    "x_all_steps = np.linspace(x_min, x_max, 1000)\n",
    "\n",
    "# Sample set of noise and target points\n",
    "data_size: int = 100_000\n",
    "np.random.seed(1)  # Set random seeds for reproducibility\n",
    "data_x_0 = np.random.randn(data_size)\n",
    "data_x_1 = mixture_sample(size=data_size)\n",
    "\n",
    "# Plot a few sample paths ##########################################\n",
    "# Plot Noise distribution π₀\n",
    "ax11.set_title(\"Noise Distribution π₀\")\n",
    "pdf_noise = scipy.stats.norm.pdf(x_all_steps, loc=0, scale=1)\n",
    "ax11.plot(pdf_noise, x_all_steps, label=\"PDF Noise π₀\", color=\"tab:orange\")\n",
    "ax11.fill_betweenx(x_all_steps, pdf_noise, alpha=0.4, color=\"tab:orange\")\n",
    "ax11.invert_xaxis()\n",
    "ax11.set_ylabel(\"x\")\n",
    "ax11.set_xlabel(\"\")\n",
    "# ax11.set_xlim(0, 1)\n",
    "\n",
    "# Plot final distribution x1\n",
    "ax13.set_title(\"Target Distribution π₁\")\n",
    "pdf_target = mixture_pdf(x=x_all_steps)\n",
    "ax13.plot(pdf_target, x_all_steps, label=\"PDF Target π₁\", color=\"tab:blue\")\n",
    "ax13.fill_betweenx(x_all_steps, pdf_target, alpha=0.4, color=\"tab:blue\")\n",
    "ax13.set_xlabel(\"\")\n",
    "ax13.yaxis.set_label_position(\"right\")\n",
    "ax13.set_ylabel(\"x\")\n",
    "# Also show y-axis values on the right side\n",
    "ax13.yaxis.set_visible(True)\n",
    "ax13.yaxis.set_tick_params(labelright=True)\n",
    "# ax13.set_xlim(0, 1)\n",
    "\n",
    "# Plot the Sample paths\n",
    "nb_samples = 7\n",
    "t = np.arange(0, 1, 0.01)\n",
    "ax12.set_title(\"Sample of straight line reference paths\")\n",
    "colors = plt.cm.tab10.colors\n",
    "for i in range(nb_samples):\n",
    "    color = colors[i % len(colors)]\n",
    "    ax12.plot(t, interpolate_linear(x_0=data_x_0[i], x_1=data_x_1[i], t=t), alpha=0.5, color=color)\n",
    "    ax12.scatter([0, 1], [data_x_0[i], data_x_1[i]], color=color)\n",
    "ax12.set_ylim(x_min, x_max)\n",
    "ax12.set_xlim(0, 1)\n",
    "ax12.set_xlabel(\"$t$: flow step\")\n",
    "\n",
    "\n",
    "# Plot the full data distribution ##################################\n",
    "# Plot Noise samples X0\n",
    "ax21.set_title(\"Noise Samples X₀\")\n",
    "ax21.hist(\n",
    "    data_x_0.flatten(),\n",
    "    bins=100,\n",
    "    alpha=0.5,\n",
    "    label=\"Noise π₀\",\n",
    "    color=\"tab:orange\",\n",
    "    density=True,\n",
    "    orientation=\"horizontal\",\n",
    ")\n",
    "ax21.invert_xaxis()\n",
    "ax21.set_ylabel(\"x\")\n",
    "ax21.set_xlabel(\"density\")\n",
    "ax21.sharex(ax11)\n",
    "# ax21.set_xlim(0, 1)\n",
    "\n",
    "# Plot target data distribution x1\n",
    "ax23.set_title(\"Target Data X₁\")\n",
    "ax23.hist(\n",
    "    data_x_1.flatten(), bins=100, alpha=0.5, label=\"Target π₁\", color=\"tab:blue\", density=True, orientation=\"horizontal\"\n",
    ")\n",
    "ax23.set_xlabel(\"density\")\n",
    "ax23.yaxis.set_label_position(\"right\")\n",
    "ax23.set_ylabel(\"x\")\n",
    "# Also show y-axis values on the right side\n",
    "ax23.yaxis.set_visible(True)\n",
    "ax23.yaxis.set_tick_params(labelright=True)\n",
    "ax23.sharex(ax13)\n",
    "# ax23.set_xlim(0, 1)\n",
    "\n",
    "# Plot path density\n",
    "n_samples = int(data_x_1.shape[0])\n",
    "dt: float = 0.01  # Step size for Euler integration\n",
    "n_flow_steps = int(1 / dt)\n",
    "# Set up the path density histogram parameters\n",
    "img_hist_size = 480\n",
    "path_density_bins = np.zeros((n_flow_steps + 1, img_hist_size))\n",
    "flow_field_img_x_bin_edges = np.linspace(x_min, x_max, img_hist_size + 1)\n",
    "\n",
    "# Add the histogram of the initial distribution\n",
    "path_density_bins[0] = np.histogram(data_x_0, bins=flow_field_img_x_bin_edges)[0]\n",
    "path_density_bins[-1] = np.histogram(data_x_1, bins=flow_field_img_x_bin_edges)[0]\n",
    "\n",
    "# Build up the histogram of the reference paths by going over the discretized t-bins\n",
    "for i in range(n_flow_steps):\n",
    "    t = np.full((n_samples,), i * dt)\n",
    "    x_t = interpolate_linear(x_0=data_x_0, x_1=data_x_1, t=t)\n",
    "    path_density_bins[i] = np.histogram(x_t, bins=flow_field_img_x_bin_edges)[0]\n",
    "\n",
    "im = ax22.imshow(path_density_bins.T, aspect=\"auto\", origin=\"lower\", extent=[0, 1, x_min, x_max], cmap=\"viridis\")\n",
    "ax22.set_xlabel(\"$t$: flow step\")\n",
    "ax22.set_title(\"Reference path density between X₀ and X₁\")\n",
    "ax22.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del (fig, ax11, ax12, ax13, ax21, ax22, ax23, x_all_steps, data_x_0, data_x_1, n_samples, dt, n_flow_steps,  # fmt: skip\n",
    "    img_hist_size, path_density_bins, flow_field_img_x_bin_edges, i, t, x_t, im)  # fmt: skip\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training: Sampling velocity vectors\n",
    "\n",
    "Since we are using straight-line reference paths, the sampled velocity vectors $\\mathbf{v}(x_t, t)$ have a very simple form. Given a sample from the noise distribution $x_0$ and a sample from the target distribution $x_1$ we can describe the conditional velocity vector along the straight-line connecting $x_0$ and $x_1$ as: $\\mathbf{v}(x_t, t) = x_1 - x_0$ as illustrated in the following code and figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_velocity(x_0, x_1):\n",
    "    \"\"\"\n",
    "    Get the velocity for a given pair of noise and target points.\n",
    "    This is the per-pair (conditional) velocity along the straight path.\n",
    "    \"\"\"\n",
    "    return x_1 - x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate the flow matching target velocity vector\n",
    "# Set up the plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(\n",
    "    1,\n",
    "    3,\n",
    "    figsize=(12, 4),\n",
    "    gridspec_kw={\"width_ratios\": [1, 5, 1]},\n",
    "    sharey=True,\n",
    "    dpi=100,\n",
    ")\n",
    "fig.subplots_adjust(wspace=0)\n",
    "x_min, x_max = -2.5, 2.5\n",
    "x_all_steps = np.linspace(x_min, x_max, 1000)\n",
    "\n",
    "\n",
    "# Plot a few sample paths ##########################################\n",
    "# Plot Noise distribution π₀\n",
    "ax1.set_title(\"Noise Distribution π₀\")\n",
    "pdf_noise = scipy.stats.norm.pdf(x_all_steps, loc=0, scale=1)\n",
    "ax1.plot(pdf_noise, x_all_steps, label=\"PDF Noise π₀\", color=\"tab:orange\")\n",
    "ax1.fill_betweenx(x_all_steps, pdf_noise, alpha=0.4, color=\"tab:orange\")\n",
    "ax1.invert_xaxis()\n",
    "ax1.set_ylabel(\"x\")\n",
    "ax1.set_xlabel(\"\")\n",
    "# ax11.set_xlim(0, 1)\n",
    "\n",
    "# Plot final distribution x1\n",
    "ax3.set_title(\"Target Distribution π₁\")\n",
    "pdf_target = mixture_pdf(x=x_all_steps)\n",
    "ax3.plot(pdf_target, x_all_steps, label=\"PDF Target π₁\", color=\"tab:blue\")\n",
    "ax3.fill_betweenx(x_all_steps, pdf_target, alpha=0.4, color=\"tab:blue\")\n",
    "ax3.set_xlabel(\"\")\n",
    "ax3.yaxis.set_label_position(\"right\")\n",
    "ax3.set_ylabel(\"x\")\n",
    "# Also show y-axis values on the right side\n",
    "ax3.yaxis.set_visible(True)\n",
    "ax3.yaxis.set_tick_params(labelright=True)\n",
    "# ax13.set_xlim(0, 1)\n",
    "\n",
    "# Plot the Sample paths\n",
    "x_0_examplar = 1.32\n",
    "x_1_examplar = -1.92\n",
    "t_examplar = 0.67\n",
    "x_t_examplar = interpolate_linear(x_0=x_0_examplar, x_1=x_1_examplar, t=t_examplar)\n",
    "\n",
    "# Annotate the path between x_0 and x_1\n",
    "ax2.set_title(\"Flow matching target for a sample path: velocity $v(x_t, t) = x_1 - x_0$\")\n",
    "ax2.plot([0, 1], [x_0_examplar, x_1_examplar], alpha=0.8, color=\"tab:green\", label=\"Path(x₀, x₁)\")\n",
    "ax2.scatter([0, 1], [x_0_examplar, x_1_examplar], color=\"tab:green\")\n",
    "# ax2.scatter([1], [x_0_examplar], color=\"tab:green\", marker=\"_\")\n",
    "ax2.legend()\n",
    "ax2.plot([0, 1], [x_0_examplar, x_0_examplar], alpha=0.5, color=\"tab:green\", linestyle=\"dotted\")\n",
    "ax2.annotate(\n",
    "    \"$x_0$\",\n",
    "    xy=(0, x_0_examplar),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(-20, x_0_examplar - 3),  # Shift the text to the left of the point\n",
    "    textcoords=\"offset points\",\n",
    "    fontsize=16,\n",
    "    color=\"tab:green\",\n",
    "    annotation_clip=False,\n",
    ")\n",
    "ax2.annotate(\n",
    "    \" $x_0$\",\n",
    "    xy=(1, x_0_examplar),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(1, x_0_examplar - 3),\n",
    "    textcoords=\"offset points\",\n",
    "    fontsize=16,\n",
    "    color=\"tab:green\",\n",
    "    annotation_clip=False,\n",
    ")\n",
    "ax2.annotate(\n",
    "    \" $x_1$\",\n",
    "    xy=(1, x_1_examplar),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(1, x_1_examplar - 3),\n",
    "    textcoords=\"offset points\",\n",
    "    fontsize=16,\n",
    "    color=\"tab:green\",\n",
    "    annotation_clip=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Annotate x_t\n",
    "ax2.annotate(\n",
    "    \"$x_t$\",\n",
    "    xy=(0, x_t_examplar),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(-20, x_t_examplar),  # Shift the text to the left of the point\n",
    "    textcoords=\"offset points\",\n",
    "    fontsize=16,\n",
    "    color=\"tab:gray\",\n",
    "    annotation_clip=False,\n",
    ")\n",
    "ax2.plot([0, t_examplar], [x_t_examplar, x_t_examplar], linestyle=\":\", color=\"tab:gray\")\n",
    "# Annotate t\n",
    "ax2.annotate(\n",
    "    \"$t$\",\n",
    "    xy=(t_examplar, x_min),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(t_examplar - 4, x_min - 12),  # Shift the text below the point\n",
    "    textcoords=\"offset points\",\n",
    "    fontsize=16,\n",
    "    color=\"tab:gray\",\n",
    "    annotation_clip=False,\n",
    ")\n",
    "ax2.plot([t_examplar, t_examplar], [x_min, x_t_examplar], linestyle=\":\", color=\"tab:gray\")\n",
    "\n",
    "\n",
    "# Annotate the velocity vector\n",
    "ax2.annotate(\n",
    "    \"\",\n",
    "    xy=(1, x_1_examplar),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(1, x_0_examplar),\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"tab:red\", linewidth=2),\n",
    "    annotation_clip=False,\n",
    ")\n",
    "ax2.annotate(\n",
    "    \" $x_1 - x_0$\",\n",
    "    xy=(1, (x_0_examplar + x_1_examplar) / 2),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(1, (x_0_examplar + x_1_examplar) / 2),\n",
    "    textcoords=\"offset points\",\n",
    "    fontsize=16,\n",
    "    color=\"tab:red\",\n",
    "    annotation_clip=False,\n",
    ")\n",
    "ax2.scatter([t_examplar], [x_t_examplar], color=\"tab:red\", marker=\"D\", zorder=10)\n",
    "ax2.annotate(\n",
    "    r\" $\\mathbf{v}(x_t, t) = x_1 - x_0$\",\n",
    "    xy=(t_examplar, x_t_examplar),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(t_examplar, x_t_examplar),\n",
    "    textcoords=\"offset points\",\n",
    "    fontsize=16,\n",
    "    color=\"tab:red\",\n",
    "    annotation_clip=False,\n",
    ")\n",
    "\n",
    "ax2.set_ylim(x_min, x_max)\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_xlabel(r\"$t$: flow step\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del (fig, ax1, ax2, ax3, x_min, x_max, x_all_steps,  # fmt: skip\n",
    "    x_0_examplar, x_1_examplar, t_examplar, x_t_examplar)  # fmt: skip\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training: Flow matching objective\n",
    "We can now write out our objective as a function of the samples from the noise distribution $x_0$ and the target distribution $x_1$:\n",
    "$$\n",
    "\\underset{\\theta}{\\text{argmin}} \\;  \\mathbb{E}_{t, X_0, X_1} \\Big\\| {FM}_{\\theta}(x_t, t) - (X_1 - X_0) \\Big\\|^2  \n",
    "\\quad\\quad \n",
    "$$\n",
    "with $t \\sim \\mathcal{U}[0, 1]$, $X_0 \\sim \\pi_0$, $X_1 \\sim \\pi_1$, and $x_t = (1 - t) X_0 + t X_1$.\n",
    "\n",
    "Note that the flow matching model ${FM}_{\\theta}(x_t, t)$ is trained conditionally on specific straight-line couplings $(X_0, X_1)$, but since these are averaged out in the training objective, the flow matching model will learn an approximation of the velocity field independent of any specific coupling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple toy example we could even approximate the flow field directly by sampling a large number of reference paths and computing the average velocity for fixed bins over the flow field. This approximated expectation is illustrated in the following figure, which shows the average flow field. Red means a positive velocity (sample pushed up towards higher $x$) and blue means a negative velocity (sample pulled down towards lower $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow field illustration, approximate the flow field by sampling a large number of reference paths over discretized flow step bins.\n",
    "\n",
    "# Sample set of noise and target points\n",
    "data_size: int = 100_000\n",
    "np.random.seed(1)  # Set random seeds for reproducibility\n",
    "data_x_0 = np.random.randn(data_size)\n",
    "data_x_1 = mixture_sample(size=data_size)\n",
    "\n",
    "\n",
    "# Set up the plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(\n",
    "    1,\n",
    "    3,\n",
    "    figsize=(12, 5),\n",
    "    gridspec_kw={\"width_ratios\": [1, 5, 1]},\n",
    "    sharey=True,\n",
    "    dpi=100,\n",
    "    constrained_layout=True,\n",
    ")\n",
    "x_min, x_max = -2.2, 2.2  # Narrow view for flow field image\n",
    "x_min_wide, x_max_wide = -3.2, 3.2  # Wide view for sample paths\n",
    "\n",
    "# Plot the velocity field ##########################################\n",
    "# Plot Noise samples X0\n",
    "ax1.set_title(\"Noise samples X₀\")\n",
    "ax1.hist(\n",
    "    data_x_0.flatten(),\n",
    "    bins=100,\n",
    "    alpha=0.5,\n",
    "    label=\"Noise π₀\",\n",
    "    color=\"tab:orange\",\n",
    "    density=True,\n",
    "    orientation=\"horizontal\",\n",
    ")\n",
    "ax1.invert_xaxis()\n",
    "ax1.set_ylabel(\"x\")\n",
    "ax1.set_xlabel(\"density\")\n",
    "\n",
    "# Plot target data distribution x1\n",
    "ax3.set_title(\"Target data X₁\")\n",
    "ax3.hist(\n",
    "    data_x_1.flatten(), bins=100, alpha=0.5, label=\"Target π₁\", color=\"tab:blue\", density=True, orientation=\"horizontal\"\n",
    ")\n",
    "ax3.set_xlabel(\"density\")\n",
    "ax3.yaxis.set_label_position(\"right\")\n",
    "ax3.set_ylabel(\"x\")\n",
    "# Also show y-axis values on the right side\n",
    "ax3.yaxis.set_visible(True)\n",
    "ax3.yaxis.set_tick_params(labelright=True)\n",
    "\n",
    "# Plot path density\n",
    "n_samples = int(data_x_1.shape[0])\n",
    "dt: float = 0.05  # Step size for Euler integration\n",
    "n_flow_steps = int(1 / dt)\n",
    "# Set up the path density histogram parameters\n",
    "img_hist_size = 200\n",
    "# Narrow view for flow field image\n",
    "flow_field_img_bins = np.zeros((n_flow_steps + 1, img_hist_size))\n",
    "flow_field_img_x_bin_edges = np.linspace(x_min, x_max, img_hist_size + 1)\n",
    "# Wide view to compute the pathlines, need to be wide enough because paths can flow out of the narrow view\n",
    "flow_field_for_path_bins = np.zeros((n_flow_steps + 1, img_hist_size))\n",
    "flow_field_for_path_x_bin_edges = np.linspace(x_min_wide, x_max_wide, img_hist_size + 1)\n",
    "\n",
    "# Build up the histogram of the reference paths by going over the discretized t-bins\n",
    "# We're building 2 histograms:\n",
    "# - `flow_field_img_bins` narrow view for the flow field image, might have NaNs if there are no samples in a bin\n",
    "# - `flow_field_for_path_bins` wide view for the pathlines, avoid NaNs by setting the velocity to 0 if there are no samples in a bin\n",
    "v = get_target_velocity(x_0=data_x_0, x_1=data_x_1)\n",
    "for i in range(n_flow_steps + 1):\n",
    "    t = np.full((n_samples,), i * dt)\n",
    "    x_t = interpolate_linear(x_0=data_x_0, x_1=data_x_1, t=t)\n",
    "    # Get the average velocity for each bin in the narrow view for the flow field image\n",
    "    x_t_bin_indices = np.digitize(x=x_t, bins=flow_field_img_x_bin_edges[1:-1], right=False)\n",
    "    counts = np.bincount(x_t_bin_indices, minlength=img_hist_size)\n",
    "    sums = np.bincount(x_t_bin_indices, weights=v, minlength=img_hist_size)\n",
    "    flow_field_img_bins[i] = np.divide(\n",
    "        sums,\n",
    "        counts,\n",
    "        out=np.full(img_hist_size, np.nan, dtype=float),\n",
    "        where=counts > 0,\n",
    "    )\n",
    "\n",
    "    # Get the average velocity for each bin in the wide view for the pathlines\n",
    "    x_t_bin_indices_wide = np.digitize(x=x_t, bins=flow_field_for_path_x_bin_edges[1:-1], right=False)\n",
    "    counts_wide = np.bincount(x_t_bin_indices_wide, minlength=img_hist_size)\n",
    "    sums_wide = np.bincount(x_t_bin_indices_wide, weights=v, minlength=img_hist_size)\n",
    "    flow_field_for_path_bins[i] = np.divide(\n",
    "        sums_wide,\n",
    "        counts_wide,\n",
    "        out=np.zeros(img_hist_size, dtype=float),  # Avoid NaNs for any path sampling\n",
    "        where=counts_wide > 0,\n",
    "    )\n",
    "\n",
    "# Plot the flow field\n",
    "max_abs_flow_field = np.nanmax(np.abs(flow_field_img_bins))\n",
    "color_norm = matplotlib.colors.TwoSlopeNorm(vmin=-max_abs_flow_field, vcenter=0.0, vmax=max_abs_flow_field)\n",
    "im = ax2.imshow(\n",
    "    flow_field_img_bins.T,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    extent=[0, 1, x_min, x_max],\n",
    "    cmap=\"coolwarm\",\n",
    "    norm=color_norm,\n",
    ")\n",
    "ax2.set_ylim(x_min, x_max)\n",
    "cbar = fig.colorbar(\n",
    "    im,\n",
    "    ax=[ax1, ax2, ax3],\n",
    "    orientation=\"horizontal\",\n",
    "    fraction=0.08,\n",
    "    aspect=40,\n",
    "    pad=0.04,\n",
    ")\n",
    "cbar.set_label(\"Velocity field value (red pushes up, blue pulls down)\")\n",
    "\n",
    "# Sample some paths from the mean flow field to show the pathlines\n",
    "n_paths = 9\n",
    "paths = np.zeros((n_flow_steps + 1, n_paths))\n",
    "paths[0] = np.linspace(start=x_min_wide, stop=x_max_wide, num=n_paths)\n",
    "for i in range(n_flow_steps):\n",
    "    t = np.full((n_samples,), i * dt)\n",
    "    x_tm1 = paths[i]\n",
    "    x_tm1_bin_indices = np.digitize(x=x_tm1, bins=flow_field_for_path_x_bin_edges[1:-1], right=False)\n",
    "    v = flow_field_for_path_bins[i, x_tm1_bin_indices]\n",
    "    paths[i + 1] = x_tm1 + v * dt\n",
    "\n",
    "# Plot the pathlines with arrows using quiver, following best practices\n",
    "t_coords = np.linspace(0, 1, n_flow_steps + 1)\n",
    "arrow_stride = 7  # space out arrows for clarity\n",
    "for i in range(n_paths):\n",
    "    y = paths[:, i]\n",
    "    idx = np.arange(0, len(t_coords) - 1, arrow_stride)\n",
    "    x0 = t_coords[idx]\n",
    "    y0 = y[idx]\n",
    "    u = np.diff(t_coords)[idx]\n",
    "    v = np.diff(y)[idx]\n",
    "    ax2.quiver(\n",
    "        x0,\n",
    "        y0,\n",
    "        u,\n",
    "        v,\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=0.6,\n",
    "        units=\"inches\",\n",
    "        width=0.015,\n",
    "        headwidth=6,\n",
    "        headlength=9,\n",
    "        headaxislength=7,\n",
    "        pivot=\"tail\",\n",
    "        color=\"dimgray\",\n",
    "        alpha=0.9,\n",
    "        zorder=2,\n",
    "    )\n",
    "    # overlay original line on top of arrows for clarity\n",
    "    ax2.plot(\n",
    "        t_coords,\n",
    "        y,\n",
    "        linestyle=\"-\",\n",
    "        color=\"dimgray\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.7,\n",
    "        zorder=3,\n",
    "        label=f\"Sample path {i + 1}\",\n",
    "    )\n",
    "\n",
    "\n",
    "ax2.set_xlabel(\"$t$: flow step\")\n",
    "ax2.set_title(\"Average velocity field with pathlines\")\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.show()\n",
    "del (fig, ax1, ax2, ax3, x_min, x_max, x_min_wide, x_max_wide, data_size,  # fmt: skip\n",
    "    data_x_0, data_x_1, n_samples, dt, n_flow_steps, img_hist_size, # fmt: skip\n",
    "    flow_field_img_bins, flow_field_img_x_bin_edges, flow_field_for_path_bins, # fmt: skip\n",
    "    flow_field_for_path_x_bin_edges, v, t, x_t, x_t_bin_indices, counts, sums, # fmt: skip\n",
    "    x_t_bin_indices_wide, counts_wide, sums_wide)  # fmt: skip\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Flow Matching model\n",
    "\n",
    "Now that we have defined our optimization objective, and how we can sample the data to train the model, we can define the flow matching model and train it. We'll create a simple neural network with a single hidden layer that we can train to predict the velocity field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatchingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Flow Matching model to predict the velocity field at time t and position x_t.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        # Simple MLP\n",
    "        self.net: nn.Sequential = nn.Sequential(\n",
    "            nn.Linear(data_dim + 1, hidden_dim),  # +1 for time embedding\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, data_dim),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        t: torch.Tensor,  # Denoising step [batch_size, 1]\n",
    "        x_t: torch.Tensor,  # Interpolated samples [batch_size, data_dim]\n",
    "    ) -> torch.Tensor:  # [batch_size, data_dim]\n",
    "        \"\"\"\n",
    "        Predicts the velocity field at time t and position x_t.\n",
    "        \"\"\"\n",
    "        tx: torch.Tensor = torch.cat([t, x_t], dim=-1)\n",
    "        return self.net(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the loss function as a function of the flow matching model, the noise samples $X_0$, the target samples $X_1$, and the flow steps $T$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(\n",
    "    flow_matching_model: FlowMatchingModel,\n",
    "    x_0: torch.Tensor,\n",
    "    x_1: torch.Tensor,\n",
    "    t: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the loss for a single batch of (X_0, X_1) couplings and flow steps T.\n",
    "    \"\"\"\n",
    "    # Interpolate the data at the sampled time step\n",
    "    x_t = interpolate_linear(x_0=x_0, x_1=x_1, t=t)\n",
    "    # Get the target velocity\n",
    "    v_target = get_target_velocity(x_0=x_0, x_1=x_1)\n",
    "    # Predict the velocity\n",
    "    v_pred = flow_matching_model(t=t, x_t=x_t)\n",
    "    # Compute the loss\n",
    "    loss = ((v_pred - v_target) ** 2).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this loss function we can now train the flow matching model in a straightforward gradient-based optimization loop. We'll use a standard Adam optimizer to optimize the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the flow matching model\n",
    "\n",
    "# Hyperparameters\n",
    "data_dim: int = 1  # 1D data\n",
    "hidden_dim: int = 64\n",
    "nb_train_iterations: int = 10_000\n",
    "lr: float = 1e-3\n",
    "batch_size: int = 256\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(626)\n",
    "\n",
    "# Initialize the vector field network and optimizer\n",
    "flow_matching_model = FlowMatchingModel(data_dim=data_dim, hidden_dim=hidden_dim).to(DEVICE).train()\n",
    "optimizer = optim.Adam(flow_matching_model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "losses: list[float] = []\n",
    "with tqdm(range(nb_train_iterations), desc=\"Training\", unit=\"iteration\") as progress_bar:\n",
    "    for i in progress_bar:\n",
    "        # Sample a batch of target and noise samples\n",
    "        x_1 = torch.from_numpy(mixture_sample(size=batch_size)).to(dtype=torch.float32, device=DEVICE).unsqueeze(-1)\n",
    "        x_0 = torch.randn_like(x_1)\n",
    "        # Sample a random time step for each sample in the batch\n",
    "        t = torch.rand(x_1.shape[0], device=DEVICE).unsqueeze(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = compute_loss(flow_matching_model=flow_matching_model, x_0=x_0, x_1=x_1, t=t)\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.2f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve after training\n",
    "fig, ax = plt.subplots(figsize=(12, 3), dpi=100)\n",
    "ax.plot(losses, color=\"tab:blue\", alpha=0.5, label=\"Loss\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training Loss Curve\")\n",
    "# Plot a smoothed loss curve using a simple moving average\n",
    "window_size = 100\n",
    "smoothed_losses = np.convolve(losses, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "ax.plot(np.arange(window_size - 1, len(losses)), smoothed_losses, color=\"tab:blue\", label=\"Loss (moving avg)\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlim(0, len(losses))\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "del fig, ax, window_size, smoothed_losses\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the trained flow matching model\n",
    "\n",
    "Now that we have trained this simple flow matching model we can visualize the learned velocity field by getting the predicted velocity field ${FM}_{\\theta}(x_t, t)$ at a grid of points $(t, x_t)$ and plotting this grid of velocities as a color image. Red means a positive velocity (sample pushed up towards higher $x$) and blue means a negative velocity (sample pulled down towards lower $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow field visualization of trained model\n",
    "# Set up the plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(\n",
    "    1,\n",
    "    3,\n",
    "    figsize=(12, 5),\n",
    "    gridspec_kw={\"width_ratios\": [1, 5, 1]},\n",
    "    sharey=True,\n",
    "    dpi=100,\n",
    "    constrained_layout=True,\n",
    ")\n",
    "x_min, x_max = -2.2, 2.2  # Narrow view for flow field image\n",
    "x_min_wide, x_max_wide = -3.2, 3.2  # Wide view for sample paths\n",
    "\n",
    "\n",
    "# Sample set of noise and target points\n",
    "data_size: int = 100_000\n",
    "np.random.seed(1)  # Set random seeds for reproducibility\n",
    "data_x_0 = np.random.randn(data_size)\n",
    "\n",
    "# Plot the velocity field ##########################################\n",
    "# Plot Noise samples X0\n",
    "ax1.set_title(\"Noise Samples X₀\")\n",
    "ax1.hist(\n",
    "    data_x_0.flatten(),\n",
    "    bins=100,\n",
    "    alpha=0.5,\n",
    "    label=\"Noise Samples X₀\",\n",
    "    color=\"tab:orange\",\n",
    "    density=True,\n",
    "    orientation=\"horizontal\",\n",
    ")\n",
    "ax1.invert_xaxis()\n",
    "ax1.set_ylabel(\"x\")\n",
    "ax1.set_xlabel(\"density\")\n",
    "\n",
    "\n",
    "# Plot Flow Field\n",
    "n_flow_steps = 100\n",
    "# Set up the flow field histogram parameters\n",
    "img_hist_size = 200\n",
    "# Narrow view for flow field image\n",
    "flow_field_img_bins = np.zeros((n_flow_steps + 1, img_hist_size))\n",
    "flow_field_img_x_bin_edges = np.linspace(x_min, x_max, img_hist_size + 1)\n",
    "flow_field_img_x_bin_centers = (\n",
    "    torch.from_numpy((flow_field_img_x_bin_edges[:-1] + flow_field_img_x_bin_edges[1:]) / 2)\n",
    "    .float()\n",
    "    .to(DEVICE)\n",
    "    .unsqueeze(-1)\n",
    ")\n",
    "\n",
    "# Wide view to compute the pathlines, need to be wide enough because paths can flow out of the narrow view\n",
    "flow_field_for_path_bins = np.zeros((n_flow_steps + 1, img_hist_size))\n",
    "flow_field_for_path_x_bin_edges = np.linspace(x_min_wide, x_max_wide, img_hist_size + 1)\n",
    "flow_field_for_path_x_bin_centers = (\n",
    "    torch.from_numpy((flow_field_for_path_x_bin_edges[:-1] + flow_field_for_path_x_bin_edges[1:]) / 2)\n",
    "    .float()\n",
    "    .to(DEVICE)\n",
    "    .unsqueeze(-1)\n",
    ")\n",
    "\n",
    "# Build up the histogram of the reference paths by going over the discretized t-bins\n",
    "# We're building 2 histograms:\n",
    "# - `flow_field_img_bins` narrow view for the flow field image, might have NaNs if there are no samples in a bin\n",
    "# - `flow_field_for_path_bins` wide view for the pathlines, avoid NaNs by setting the velocity to 0 if there are no samples in a bin\n",
    "with torch.inference_mode():\n",
    "    flow_matching_model.eval()\n",
    "    for i, t in enumerate(torch.linspace(0, 1, n_flow_steps + 1)):\n",
    "        t = t.expand_as(flow_field_img_x_bin_centers).to(DEVICE)\n",
    "        # Get the model's prediction for the velocity field at the bin centers\n",
    "        flow_field_img_bins[i] = flow_matching_model(t=t, x_t=flow_field_img_x_bin_centers).cpu().numpy().squeeze()\n",
    "        flow_field_for_path_bins[i] = (\n",
    "            flow_matching_model(t=t, x_t=flow_field_for_path_x_bin_centers).cpu().numpy().squeeze()\n",
    "        )\n",
    "\n",
    "# Plot the flow field\n",
    "max_abs_flow_field = np.nanmax(np.abs(flow_field_img_bins))\n",
    "color_norm = matplotlib.colors.TwoSlopeNorm(vmin=-max_abs_flow_field, vcenter=0.0, vmax=max_abs_flow_field)\n",
    "im = ax2.imshow(\n",
    "    flow_field_img_bins.T,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    extent=[0, 1, x_min, x_max],\n",
    "    cmap=\"coolwarm\",\n",
    "    norm=color_norm,\n",
    ")\n",
    "ax2.set_ylim(x_min, x_max)\n",
    "cbar = fig.colorbar(\n",
    "    im,\n",
    "    ax=[ax1, ax2, ax3],\n",
    "    orientation=\"horizontal\",\n",
    "    fraction=0.08,\n",
    "    aspect=40,\n",
    "    pad=0.04,\n",
    ")\n",
    "cbar.set_label(\"Velocity field value (red pushes up, blue pulls down)\")\n",
    "\n",
    "\n",
    "# Create pathlines from the flow field\n",
    "n_paths = 9\n",
    "# Initialize tensor to store the flow process\n",
    "paths = np.zeros((n_flow_steps + 1, n_paths))\n",
    "x_t = torch.linspace(start=x_min_wide, end=x_max_wide, steps=n_paths, device=DEVICE).unsqueeze(-1)\n",
    "paths[0] = x_t.cpu().squeeze().numpy()\n",
    "# Generate the flow process\n",
    "with torch.inference_mode():\n",
    "    flow_matching_model.eval()\n",
    "    ts = torch.linspace(0, 1, n_flow_steps + 1)\n",
    "    for i in range(n_flow_steps):\n",
    "        t = ts[i].expand_as(x_t).to(DEVICE)\n",
    "        dt = ts[i + 1] - ts[i]\n",
    "        x_t = x_t + flow_matching_model(t=t, x_t=x_t.to(DEVICE)) * dt\n",
    "        paths[i + 1] = x_t.cpu().numpy().squeeze()\n",
    "\n",
    "# Plot the pathlines with arrows using quiver, following best practices\n",
    "t_coords = np.linspace(0, 1, n_flow_steps + 1)\n",
    "arrow_stride = 35  # space out arrows for clarity\n",
    "arrow_offset = 7  # offset the arrows to the right for clarity\n",
    "for i in range(n_paths):\n",
    "    y = paths[:, i]\n",
    "    idx = np.arange(arrow_offset, len(t_coords) - 1, arrow_stride)\n",
    "    x0 = t_coords[idx]\n",
    "    y0 = y[idx]\n",
    "    u = np.diff(t_coords)[idx]\n",
    "    v = np.diff(y)[idx]\n",
    "    ax2.quiver(\n",
    "        x0,\n",
    "        y0,\n",
    "        u,\n",
    "        v,\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=0.6,\n",
    "        units=\"inches\",\n",
    "        width=0.015,\n",
    "        headwidth=6,\n",
    "        headlength=9,\n",
    "        headaxislength=7,\n",
    "        pivot=\"tail\",\n",
    "        color=\"dimgray\",\n",
    "        alpha=0.9,\n",
    "        zorder=2,\n",
    "    )\n",
    "    # overlay original line on top of arrows for clarity\n",
    "    ax2.plot(\n",
    "        t_coords,\n",
    "        y,\n",
    "        linestyle=\"-\",\n",
    "        color=\"dimgray\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.7,\n",
    "        zorder=3,\n",
    "        label=f\"Sample Path {i + 1}\",\n",
    "    )\n",
    "\n",
    "\n",
    "ax2.set_xlabel(\"$t$: flow step\")\n",
    "ax2.set_title(r\"Predicted velocity field ${FM}_{\\theta}(x_t, t)$ with pathlines\")\n",
    "ax2.grid(False)\n",
    "\n",
    "\n",
    "# Plot target data distribution x1\n",
    "\n",
    "# Sample target data distribution x1\n",
    "x_t = torch.from_numpy(data_x_0).float().to(DEVICE).unsqueeze(-1)\n",
    "dt: float = 0.01  # Step size for Euler integration\n",
    "n_flow_steps = int(1 / dt)\n",
    "# Generate the flow process\n",
    "with torch.inference_mode():\n",
    "    for i in range(n_flow_steps):\n",
    "        t = torch.full_like(x_t, i * dt, device=DEVICE)\n",
    "        x_t = x_t + flow_matching_model(t=t, x_t=x_t.to(DEVICE)) * dt\n",
    "data_x_1 = x_t.cpu().numpy().squeeze()\n",
    "\n",
    "ax3.set_title(r\"Predicted Data $\\hat{X}_1$\")\n",
    "ax3.hist(\n",
    "    data_x_1.flatten(),\n",
    "    bins=100,\n",
    "    alpha=0.5,\n",
    "    label=r\"Predicted Data $\\hat{X}_1$\",\n",
    "    color=\"tab:blue\",\n",
    "    density=True,\n",
    "    orientation=\"horizontal\",\n",
    ")\n",
    "ax3.set_xlabel(\"density\")\n",
    "ax3.yaxis.set_label_position(\"right\")\n",
    "ax3.set_ylabel(\"x\")\n",
    "# Also show y-axis values on the right side\n",
    "ax3.yaxis.set_visible(True)\n",
    "ax3.yaxis.set_tick_params(labelright=True)\n",
    "\n",
    "plt.show()\n",
    "del (\n",
    "    fig,\n",
    "    ax1,\n",
    "    ax2,\n",
    "    ax3,\n",
    "    x_min,\n",
    "    x_max,\n",
    "    x_min_wide,\n",
    "    x_max_wide,\n",
    "    data_size,  # fmt: skip\n",
    "    data_x_0,\n",
    "    data_x_1,\n",
    "    dt,\n",
    "    n_flow_steps,\n",
    "    img_hist_size,\n",
    "    flow_field_img_bins,  # fmt: skip\n",
    "    flow_field_img_x_bin_edges,\n",
    "    flow_field_for_path_bins,\n",
    "    flow_field_for_path_x_bin_edges,  # fmt: skip\n",
    "    y,\n",
    "    idx,\n",
    "    x0,\n",
    "    y0,\n",
    "    u,\n",
    "    i,\n",
    "    t,\n",
    "    x_t,  # fmt: skip\n",
    ")\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the trained model\n",
    "\n",
    "At inference time we can sample a starting point $x_0$ from the noise distribution $π_0$ and then use the predicted velocity field ${FM}_{\\theta}(x_t, t)$ to iteratively move (integrate) the sample towards a sample $\\hat{x}_1$ from the target distribution $π_1$.\n",
    "\n",
    "The code below starts with noise $ x_0 \\sim \\mathcal{N}(0, 1)$ and integrates the learned ODE using the simple [Euler method](https://en.wikipedia.org/wiki/Euler_method). The Euler method is a simple integration method that at each step $t$ takes the velocity field prediction ${FM}_{\\theta}(x_t, t)$ at the current position $x_t$ and moves the sample a small step $dt$ in the direction of the velocity field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration on how to sample x_1 from x_0 using the learned velocity field\n",
    "nb_steps = 15\n",
    "path_x = np.zeros(nb_steps + 1)  # Array to store the full sampled path\n",
    "t_steps = np.linspace(0, 1, nb_steps + 1)  # Steps $t$ in the range [0, 1]\n",
    "\n",
    "# x_0 starting point (Pre-selected here for the example, but ideally x_0 ~ N(0, I\n",
    "x_0 = torch.Tensor([[0.85]]).to(DEVICE)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    flow_matching_model.eval()\n",
    "    x_t = x_0  # Initialize the sample at the starting point\n",
    "    path_x[0] = x_t.squeeze().cpu().numpy()\n",
    "    # Integrate the velocity field using Euler integration from t=0 to t=1\n",
    "    for i in range(nb_steps):\n",
    "        t = t_steps[i]  # Current step $t$\n",
    "        dt = t_steps[i + 1] - t_steps[i]  # Step size\n",
    "        t_batch = torch.Tensor([[t]]).to(DEVICE)  # Expand the step to a batch dimension\n",
    "        # Get the velocity field prediction at the current position and time step and move the sample a small step dt in the direction of the velocity field\n",
    "        x_t = x_t + flow_matching_model(t=t_batch, x_t=x_t) * dt\n",
    "        path_x[i + 1] = x_t.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "display(HTML(pd.DataFrame({\"t\": t_steps, \"x\": path_x}).transpose().to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can illustrate this sampled path in the following animation which shows the integration from the noise sample $x_0$ towards the target distribution $\\hat{x}_1$ using the predicted velocity field ${FM}_{\\theta}(x_t, t)$ above. The velocity field is visualized as a heatmap where the vertical axis represents the position of the sample $x_t$ and the horizontal axis represents the flow step $t$ going from 0 on the left to 1 on the right. Red means a positive velocity (sample pushed up towards higher $x$) and blue means a negative velocity (sample pulled down towards lower $x$).\n",
    "\n",
    "Notice that while we trained on straight-line paths, the sampled path it not necessarily a straight line. This is because we don't learn the paths directly but learn the unconditioned velocity field by training on a large set of straight-line reference paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize animation of the flow matching integration (denoising) using Euler integration\n",
    "# Set up the plot\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(\n",
    "    1,\n",
    "    3,\n",
    "    figsize=(12, 5),\n",
    "    gridspec_kw={\"width_ratios\": [1, 5, 1]},\n",
    "    sharey=True,\n",
    "    dpi=100,\n",
    "    constrained_layout=True,\n",
    ")\n",
    "x_min, x_max = -2.2, 2.2  # Narrow view for flow field image\n",
    "x_all_steps = np.linspace(-3, 3, 1000)\n",
    "\n",
    "# Sample set of noise and target points\n",
    "data_size: int = 100_000\n",
    "np.random.seed(1)  # Set random seeds for reproducibility\n",
    "data_x_0 = np.random.randn(data_size)\n",
    "\n",
    "# Plot the velocity field ##########################################\n",
    "# Plot Noise samples X0\n",
    "# Plot Noise distribution π₀\n",
    "ax1.set_title(\"Noise Distribution π₀\")\n",
    "pdf_noise = scipy.stats.norm.pdf(x_all_steps, loc=0, scale=1)\n",
    "ax1.plot(pdf_noise, x_all_steps, label=\"PDF Noise π₀\", color=\"tab:orange\")\n",
    "ax1.fill_betweenx(x_all_steps, pdf_noise, alpha=0.4, color=\"tab:orange\")\n",
    "ax1.invert_xaxis()\n",
    "ax1.set_ylabel(\"x\")\n",
    "ax1.set_xlabel(\"\")\n",
    "# ax11.set_xlim(0, 1)\n",
    "\n",
    "# Plot final distribution x1\n",
    "ax3.set_title(\"Target Distribution π₁\")\n",
    "pdf_target = mixture_pdf(x=x_all_steps)\n",
    "ax3.plot(pdf_target, x_all_steps, label=\"PDF Target π₁\", color=\"tab:blue\")\n",
    "ax3.fill_betweenx(x_all_steps, pdf_target, alpha=0.4, color=\"tab:blue\")\n",
    "ax3.set_xlabel(\"\")\n",
    "ax3.yaxis.set_label_position(\"right\")\n",
    "ax3.set_ylabel(\"x\")\n",
    "# Also show y-axis values on the right side\n",
    "ax3.yaxis.set_visible(True)\n",
    "ax3.yaxis.set_tick_params(labelright=True)\n",
    "# ax13.set_xlim(0, 1)\n",
    "\n",
    "\n",
    "# Plot Flow Field\n",
    "n_flow_steps = 100\n",
    "# Set up the flow field histogram parameters\n",
    "img_hist_size = 200\n",
    "# Narrow view for flow field image\n",
    "flow_field_img_bins = np.zeros((n_flow_steps + 1, img_hist_size))\n",
    "flow_field_img_x_bin_edges = np.linspace(x_min, x_max, img_hist_size + 1)\n",
    "flow_field_img_x_bin_centers = (\n",
    "    torch.from_numpy((flow_field_img_x_bin_edges[:-1] + flow_field_img_x_bin_edges[1:]) / 2)\n",
    "    .float()\n",
    "    .to(DEVICE)\n",
    "    .unsqueeze(-1)\n",
    ")\n",
    "\n",
    "# Build up the histogram of the reference paths by going over the discretized t-bins\n",
    "# We're building 2 histograms:\n",
    "# - `flow_field_img_bins` narrow view for the flow field image, might have NaNs if there are no samples in a bin\n",
    "# - `flow_field_for_path_bins` wide view for the pathlines, avoid NaNs by setting the velocity to 0 if there are no samples in a bin\n",
    "with torch.inference_mode():\n",
    "    ts = torch.linspace(0, 1, n_flow_steps + 1, device=DEVICE)\n",
    "    for i in range(n_flow_steps + 1):\n",
    "        t = ts[i].expand_as(flow_field_img_x_bin_centers).to(DEVICE)\n",
    "        # Get the model's prediction for the velocity field at the bin centers\n",
    "        flow_field_img_bins[i] = flow_matching_model(t=t, x_t=flow_field_img_x_bin_centers).cpu().numpy().squeeze()\n",
    "\n",
    "# Plot the flow field\n",
    "max_abs_flow_field = np.nanmax(np.abs(flow_field_img_bins))\n",
    "color_norm = matplotlib.colors.TwoSlopeNorm(vmin=-max_abs_flow_field, vcenter=0.0, vmax=max_abs_flow_field)\n",
    "im = ax2.imshow(\n",
    "    flow_field_img_bins.T,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    "    extent=[0, 1, x_min, x_max],\n",
    "    cmap=\"coolwarm\",\n",
    "    norm=color_norm,\n",
    ")\n",
    "ax2.set_ylim(x_min, x_max)\n",
    "cbar = fig.colorbar(\n",
    "    im,\n",
    "    ax=[ax1, ax2, ax3],\n",
    "    orientation=\"horizontal\",\n",
    "    fraction=0.08,\n",
    "    aspect=40,\n",
    "    pad=0.04,\n",
    ")\n",
    "cbar.set_label(\"Velocity field value (red pushes up, blue pulls down)\")\n",
    "\n",
    "\n",
    "# overlay original line on top of arrows for clarity\n",
    "step_draw = ax2.scatter(\n",
    "    [0],\n",
    "    [x_0.squeeze().cpu().numpy()],\n",
    "    color=\"tab:blue\",\n",
    "    zorder=4,\n",
    "    alpha=0.9,\n",
    ")\n",
    "(line_draw,) = ax2.plot(\n",
    "    [0],\n",
    "    [x_0.squeeze().cpu().numpy()],\n",
    "    linestyle=\":\",\n",
    "    color=\"tab:blue\",\n",
    "    linewidth=1,\n",
    "    alpha=0.5,\n",
    "    zorder=3,\n",
    ")\n",
    "text_draw = ax2.text(\n",
    "    0.77,\n",
    "    -1.58,\n",
    "    f\"   t = 0.00\\nstep = 0\\n   x = {path_x[0]:.2f}\",\n",
    "    fontsize=16,\n",
    "    color=\"black\",\n",
    "    fontfamily=\"monospace\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"center\",\n",
    "    bbox={\"facecolor\": \"white\", \"alpha\": 0.5, \"pad\": 8},\n",
    ")\n",
    "ax2.set_xlabel(\"$t$: flow step\")\n",
    "ax2.set_title(r\"Euler integration of the predicted velocity field ${FM}_{\\theta}(x_t, t)$\")\n",
    "ax2.grid(True)\n",
    "ax2.legend(\n",
    "    [step_draw],\n",
    "    [\"Euler Integration Step\"],\n",
    "    loc=\"lower left\",\n",
    ")\n",
    "\n",
    "\n",
    "def update_animation(frame: int, step_draw, line_draw, text_draw, nb_steps):\n",
    "    \"\"\"\n",
    "    Update the figure to show an animation of the integration of the velocity field.\n",
    "    \"\"\"\n",
    "    xs = path_x[: frame + 1]\n",
    "    ts = t_steps[: frame + 1]\n",
    "    step_draw.set_offsets(np.stack([ts, xs]).T)\n",
    "    line_draw.set_xdata(ts)\n",
    "    line_draw.set_ydata(xs)\n",
    "    text_draw.set_text(f\"   t = {ts[-1]:.2f}\\nstep = {min(frame, nb_steps):d}\\n   x = {xs[-1]:.2f}\")\n",
    "    return (step_draw, line_draw)\n",
    "\n",
    "\n",
    "# Create figure\n",
    "ani = FuncAnimation(\n",
    "    fig=fig,\n",
    "    func=functools.partial(\n",
    "        update_animation,\n",
    "        step_draw=step_draw,\n",
    "        line_draw=line_draw,\n",
    "        text_draw=text_draw,\n",
    "        nb_steps=nb_steps,\n",
    "    ),\n",
    "    frames=nb_steps + 2,\n",
    "    interval=1,\n",
    ")\n",
    "ani.save(str(ANIMATION_FILE), writer=\"ffmpeg\", fps=3)\n",
    "plt.close(fig)\n",
    "\n",
    "# Display the animation in the notebook\n",
    "# Embed the GIF directly in the notebook by encoding the bytes as base64, this way it should hopefully also be exported\n",
    "with ANIMATION_FILE.open(\"rb\") as f:\n",
    "    gif_data = f.read()\n",
    "\n",
    "display(\n",
    "    HTML(f\"\"\"\n",
    "<video alt=\"Flow matching path integration\" autoplay muted loop playsinline\n",
    "style=\"width:100%;height:auto;max-height:90vh;object-fit:contain;display:block;margin:0 auto;\">\n",
    "    <source type=\"video/mp4\" src=\"data:video/mp4;base64,{base64.b64encode(gif_data).decode()}\">\n",
    "</video>\n",
    "\"\"\")\n",
    ")\n",
    "del (fig, ax1, ax2, ax3, x_min, x_max, data_size, data_x_0, n_flow_steps, img_hist_size, # fmt: skip\n",
    "    flow_field_img_bins, flow_field_img_x_bin_edges, flow_field_img_x_bin_centers, max_abs_flow_field, # fmt: skip\n",
    "    color_norm, im, cbar, step_draw, line_draw, text_draw, update_animation, ani, gif_data)  # fmt: skip\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a large sample from the model $\\hat{X}_1$ and reconstruct the target distribution $\\pi_1$.\n",
    "We'll define a `sample` function that will generate samples by integrating the learned vector field using [Euler integration](https://en.wikipedia.org/wiki/Euler_method).\n",
    "We'll then plot the target distribution and the reconstructed samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def sample(\n",
    "    n_samples: int,  # Number of samples to generate\n",
    "    model: FlowMatchingModel,  # The flow matching model\n",
    "    nb_steps: int,  # Number of Euler integration steps\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Generates samples by integrating the learned vector field using Euler integration.\"\"\"\n",
    "    ts = torch.linspace(0, 1, nb_steps + 1, device=DEVICE)\n",
    "    x_t = torch.randn(n_samples, data_dim).to(DEVICE)  # Sample x_0 ~ N(0, I)\n",
    "    for i in range(nb_steps):  # Euler integration from t=0 to t=1 (last step happens just before t=1)\n",
    "        t = ts[i]  # Current step $t$\n",
    "        dt = ts[i + 1] - ts[i]  # Step size\n",
    "        t_batch = t.expand(n_samples).unsqueeze(-1)\n",
    "        # Move the sample a small step dt in the direction of the velocity field\n",
    "        x_t = x_t + model(t=t_batch, x_t=x_t) * dt\n",
    "    return x_t  # Final sample x_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data distribution. This is the TARGET distribution (π₁)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 3), constrained_layout=True, dpi=100)\n",
    "\n",
    "# Plot the target distribution π₁\n",
    "x_all_steps = np.linspace(-2.5, 2.5, 1000)\n",
    "pdf_target = mixture_pdf(x=x_all_steps)\n",
    "ax.plot(x_all_steps, pdf_target, label=r\"PDF Target $\\pi_1$\", color=\"tab:purple\")\n",
    "ax.fill_between(x_all_steps, pdf_target, alpha=0.4, color=\"tab:purple\")\n",
    "\n",
    "# Plot the samples\n",
    "x1_samples = sample(n_samples=100_000, model=flow_matching_model, nb_steps=50).cpu().numpy().flatten()\n",
    "sns.histplot(\n",
    "    x=x1_samples,\n",
    "    bins=100,\n",
    "    color=\"tab:blue\",\n",
    "    kde=False,\n",
    "    alpha=0.8,\n",
    "    ax=ax,\n",
    "    stat=\"density\",\n",
    "    label=r\"Reconstructed samples $\\hat{X}_1$\",\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(r\"Target Distribution ($\\pi_1$) vs Reconstructed Samples ($\\hat{X}_1$)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"density\")\n",
    "plt.show()\n",
    "del fig, ax, x_all_steps, pdf_target, x1_samples\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final illustration, let's illustrate the the path density between the starting noise samples $\\hat{X}_0$ and the final reconstructed samples $\\hat{X}_1$ by sampling a large number of paths from the noise distribution $\\pi_0$ to the target distribution $\\pi_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path density\n",
    "@torch.inference_mode()\n",
    "def sample_paths(\n",
    "    n_samples: int,\n",
    "    model: FlowMatchingModel,\n",
    "    nb_steps: int,  # Number of Euler integration steps\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Generates samples by integrating the learned vector field, keeping track of the intermediate steps.\"\"\"\n",
    "    x_all_steps = torch.zeros(n_samples, nb_steps + 1).to(DEVICE)\n",
    "    ts = torch.linspace(0, 1, nb_steps + 1, device=DEVICE)\n",
    "    x_t = torch.randn(n_samples, 1).to(DEVICE)  # Sample x_0 ~ N(0, I)\n",
    "    x_all_steps[:, 0] = x_t.squeeze()\n",
    "    for i in range(nb_steps):  # Euler integration from t=0 to t=1\n",
    "        t = ts[i]  # Current step $t$\n",
    "        dt = ts[i + 1] - ts[i]  # Step size\n",
    "        t_batch = t.expand(n_samples).unsqueeze(-1)  # Expand the step to a batch dimension\n",
    "        # Move the sample a small step dt in the direction of the velocity field\n",
    "        x_t = x_t + model(t=t_batch, x_t=x_t) * dt\n",
    "        x_all_steps[:, i + 1] = x_t.squeeze()\n",
    "    return x_all_steps\n",
    "\n",
    "\n",
    "nb_paths = 100_000\n",
    "nb_steps = 200\n",
    "paths = sample_paths(n_samples=nb_paths, model=flow_matching_model, nb_steps=nb_steps).cpu().numpy()\n",
    "\n",
    "\n",
    "# Illustration of the sampled reference paths\n",
    "# Set up the plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(\n",
    "    1,\n",
    "    3,\n",
    "    figsize=(12, 4),\n",
    "    gridspec_kw={\"width_ratios\": [1, 5, 1]},\n",
    "    sharey=True,\n",
    "    dpi=100,\n",
    ")\n",
    "fig.subplots_adjust(wspace=0)\n",
    "x_min, x_max = -2.5, 2.5\n",
    "\n",
    "\n",
    "# Sample set of noise and target points\n",
    "np.random.seed(1)  # Set random seeds for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "# Plot the full data distribution ##################################\n",
    "# Plot Noise samples X0\n",
    "ax1.set_title(\"Noise Samples X₀\")\n",
    "ax1.hist(\n",
    "    paths[:, 0],\n",
    "    bins=100,\n",
    "    alpha=0.5,\n",
    "    label=\"Noise π₀\",\n",
    "    color=\"tab:orange\",\n",
    "    density=True,\n",
    "    orientation=\"horizontal\",\n",
    ")\n",
    "ax1.invert_xaxis()\n",
    "ax1.set_ylabel(\"x\")\n",
    "ax1.set_xlabel(\"density\")\n",
    "\n",
    "# Plot target data distribution x1\n",
    "ax3.set_title(r\"Reconstructed samples $\\hat{X}_1$\")\n",
    "ax3.hist(\n",
    "    paths[:, -1],\n",
    "    bins=100,\n",
    "    alpha=0.5,\n",
    "    label=r\"Reconstructed samples $\\hat{X}_1$\",\n",
    "    color=\"tab:blue\",\n",
    "    density=True,\n",
    "    orientation=\"horizontal\",\n",
    ")\n",
    "ax3.set_xlabel(\"density\")\n",
    "ax3.yaxis.set_label_position(\"right\")\n",
    "ax3.set_ylabel(\"x\")\n",
    "# Also show y-axis values on the right side\n",
    "ax3.yaxis.set_visible(True)\n",
    "ax3.yaxis.set_tick_params(labelright=True)\n",
    "\n",
    "# Plot path density\n",
    "# Set up the path density histogram parameters\n",
    "img_hist_x_size = 480\n",
    "path_density_bins = np.zeros((nb_steps + 1, img_hist_x_size))\n",
    "flow_field_img_x_bin_edges = np.linspace(x_min, x_max, img_hist_x_size + 1)\n",
    "\n",
    "# Build up the histogram of the reference paths by going over the discretized t-bins\n",
    "for i in range(nb_steps + 1):\n",
    "    path_density_bins[i] = np.histogram(paths[:, i], bins=flow_field_img_x_bin_edges)[0]\n",
    "\n",
    "im = ax2.imshow(path_density_bins.T, aspect=\"auto\", origin=\"lower\", extent=[0, 1, x_min, x_max], cmap=\"viridis\")\n",
    "ax2.set_xlabel(\"$t$: flow step\")\n",
    "ax2.set_title(r\"Path density of paths sampled using Euler integration of ${FM}_{\\theta}(x_t, t)$\")\n",
    "ax2.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del (fig, ax1, ax2, ax3, x_min, x_max, nb_paths, nb_steps, paths, img_hist_x_size, # fmt: skip\n",
    "    path_density_bins, flow_field_img_x_bin_edges, i, im)  # fmt: skip\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "To conclude we've implemented a simple flow matching model and trained it on 1D toy data. The 1D toy data allowed us to easily visualize the flow matching model, the velocity field, and the sampled paths.\n",
    "\n",
    "In real-world applications the target distribution is not known and more complex, resulting in a more complex vector field, which typically requires using more complex models to learn the vector field and more sophisticated sampling strategies to sample from the model.\n",
    "\n",
    "\n",
    "## References and further reading\n",
    "\n",
    "- Flow matching paper: [Flow Matching for Generative Modeling](https://arxiv.org/abs/2210.02747)\n",
    "- Rectified flow paper: [Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow](https://arxiv.org/abs/2209.03003)\n",
    "- [Flow Matching Guide and Code](https://arxiv.org/abs/2412.06264)\n",
    "- [An introduction to flow matching](https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python package versions used\n",
    "%load_ext watermark\n",
    "%watermark --python\n",
    "%watermark --iversions\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post at <a rel=\"canonical\" href=\"https://peterroelants.github.io/posts/flow_matching_intro/\">peterroelants.github.io</a> is generated from an IPython notebook file. [Link to the full IPython notebook file](https://github.com/peterroelants/peterroelants.github.io/blob/main/notebooks/diffusion_flow_matching/flow_matching_intro.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement a neural network: Part 3\n",
    "\n",
    "This is the third part of a 4 (+2) parts tutorial on how to implement a simple neural network model. You can find the links to the rest of the tutorial here:\n",
    "\n",
    "* [Part 1: Linear regression]({% post_url 2015-06-10-neural_network_implementation_part01 %})\n",
    "* [Intermezzo 1: Logistic classification function]({% post_url 2015-06-10-neural_network_implementation_intermezzo01 %})\n",
    "* [Part 2: Logistic regression (classification)]({% post_url 2015-06-10-neural_network_implementation_part02 %})\n",
    "* [Part 3: Hidden layer]({% post_url 2015-06-10-neural_network_implementation_part03 %})\n",
    "* [Intermezzo 2: Softmax classification function]({% post_url 2015-06-10-neural_network_implementation_intermezzo02 %})\n",
    "* [Part 4: Vectorization]({% post_url 2015-06-10-neural_network_implementation_part04 %})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer\n",
    "\n",
    "While the previous tutorials described very simple single layer regression and classification models, this tutorial will describe a 2-class classification neural network with 1 input dimension, and a non-linear hidden layer with 1 neuron. This network can be represented graphically as:\n",
    "\n",
    "![Image of the logistic model](https://dl.dropboxusercontent.com/u/8938051/Blog_images/SimpleANN03.png)\n",
    "\n",
    "The notebook starts out with importing the libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D plots\n",
    "from matplotlib import cm # Colormaps\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset \n",
    "\n",
    "In this example the target classes $t$ corresponding to the inputs $x$ will be generated from 2 class distributions: blue ($t=1$) and red ($t=0$). Where the red class is a [multimodal distribution](http://en.wikipedia.org/wiki/Multimodal_distribution) that surrounds the distribution of the blue class. This results in a 1D dataset that is not linearly separable. These samples are plotted in the figure below. \n",
    "\n",
    "The model from part 2 won't be able to classify both classes correctly since it can learn only linear separators. By adding a hidden layer with a non-linear transfer function, the model will be able to train a non-linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and generate the samples\n",
    "nb_of_samples_per_class = 20  # The number of sample in each class\n",
    "blue_mean = [0]  # The mean of the blue class\n",
    "red_left_mean = [-2]  # The mean of the red class\n",
    "red_right_mean = [2]  # The mean of the red class\n",
    "\n",
    "std_dev = 0.5  # standard deviation of both classes\n",
    "# Generate samples from both classes\n",
    "x_blue = np.random.randn(nb_of_samples_per_class, 1) * std_dev + blue_mean\n",
    "x_red_left = np.random.randn(nb_of_samples_per_class/2, 1) * std_dev + red_left_mean\n",
    "x_red_right = np.random.randn(nb_of_samples_per_class/2, 1) * std_dev + red_right_mean\n",
    "\n",
    "# Merge samples in set of input variables x, and corresponding set of\n",
    "# output variables t\n",
    "x = np.vstack((x_blue, x_red_left, x_red_right))\n",
    "# print x\n",
    "t = np.vstack((np.ones((x_blue.shape[0],1)), \n",
    "               np.zeros((x_red_left.shape[0],1)), \n",
    "               np.zeros((x_red_right.shape[0], 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot samples from both classes as lines on a 1D space\n",
    "plt.figure(figsize=(8,0.5))\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-1,1)\n",
    "# Plot samples\n",
    "plt.plot(x_blue, np.zeros_like(x_blue), 'b|', ms = 30) \n",
    "plt.plot(x_red_left, np.zeros_like(x_red_left), 'r|', ms = 30) \n",
    "plt.plot(x_red_right, np.zeros_like(x_red_right), 'r|', ms = 30) \n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.title('Input samples from the blue and red class')\n",
    "plt.xlabel('$x$', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear transfer function \n",
    "\n",
    "The non-linear transfer function used in the hidden layer of this example is the [Gaussian](http://en.wikipedia.org/wiki/Gaussian_function) [radial basis function](http://en.wikipedia.org/wiki/Radial_basis_function) (RBF).  \n",
    "The RBF is a transfer function that is not usually used in neural networks, except for [radial basis function networks](http://en.wikipedia.org/wiki/Radial_basis_function_network). One of the most common transfer functions in neural networks is the [sigmoid](http://en.wikipedia.org/wiki/Sigmoid_function) transfer function.\n",
    "The RBF will allow to separate the blue samples from the red samples in this simple example by only activating for a certain region around the origin. The RBF is plotted in the figure below and is defined in this example as:\n",
    "\n",
    "$$ \\text{RBF} = \\phi(z) = e^{-z^2} $$\n",
    "\n",
    "The derivative of this RBF function is:\n",
    "\n",
    "$$ \\frac{d \\phi(z)}{dz} = -2 z e^{-z^2} = -2 z \\phi(z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the rbf function\n",
    "def rbf(z): return np.exp(-z**2)\n",
    "\n",
    "# Plot the rbf function\n",
    "z = np.linspace(-6,6,100)\n",
    "plt.plot(z, rbf(z), 'b-')\n",
    "plt.xlabel('$z$', fontsize=15)\n",
    "plt.ylabel('$e^{-z^2}$', fontsize=15)\n",
    "plt.title('RBF function')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization by backpropagation\n",
    "\n",
    "We will train this model by using the [backpropagation](http://en.wikipedia.org/wiki/Backpropagation) algorithm that is typically used to train neural networks. Each iteration of the backpropagation algorithm consists of two steps:\n",
    "\n",
    "1. A forward propagation step to compute the output of the network.\n",
    "2. A backward propagation step in which the error at the end of the network is propagated backward through all the neurons while updating their parameters.\n",
    "\n",
    "### 1. Forward step\n",
    "\n",
    "During the forward step, the input will be propagated layer by layer through the network to compute the final output of the network.\n",
    "\n",
    "#### Compute activations of hidden layer\n",
    "\n",
    "The activations $\\mathbf{h}$ of the hidden layer will be computed by:\n",
    "\n",
    "$$\\mathbf{h} = \\phi(\\mathbf{x}*w_h) = e^{-(\\mathbf{x}*w_h)^2} $$\n",
    "\n",
    "With $w_h$ the weight parameter that transforms the input before applying the RBF transfer function. This is implemented below by the `hidden_activations(x, wh)` method.\n",
    "\n",
    "#### Compute activations of output \n",
    "\n",
    "The output of the final layer and network will be computed by passing the hidden activations $\\mathbf{h}$ as input to the logistic output function:\n",
    "\n",
    "$$ \\mathbf{y} = \\sigma(\\mathbf{h} * w_o - 1) = \\frac{1}{1+e^{-\\mathbf{h} * w_o - 1}} $$\n",
    "\n",
    "With $w_o$ the weight parameter of the output layer. This is implemented below as the `output_activations(h , wo)` method.\n",
    "Note that we add a bias (intercept) term of $-1$ to the input of the logistic output neuron. Remember from part 2 that the logistic output neuron without bias can only learn a decision boundary that goes through the origin $(0)$. Since the RBF in the hidden layer projects all input variables to a range between $0$ and $+ \\infty$, the output layer without an intercept will not be able to learn any useful classifier, because none of the samples will be below $0$ and thus lie on the left side of the decision boundary. By adding a bias term the decision boundary is moved from the intercept. Normally the value of this bias termed is learned together with the rest of the weight parameters, but to keep this model simple we just make this bias constant in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def logistic(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Function to compute the hidden activations\n",
    "def hidden_activations(x, wh):\n",
    "    return rbf(x * wh)\n",
    "\n",
    "# Define output layer feedforward\n",
    "def output_activations(h , wo):\n",
    "    return logistic(h * wo - 1)\n",
    "\n",
    "# Define the neural network function\n",
    "def nn(x, wh, wo): \n",
    "    return output_activations(hidden_activations(x, wh), wo)\n",
    "\n",
    "# Define the neural network prediction function that only returns\n",
    "#  1 or 0 depending on the predicted class\n",
    "def nn_predict(x, wh, wo): \n",
    "    return np.around(nn(x, wh, wo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Backward step\n",
    "\n",
    "The backward step will begin with computing the cost at the output node. This cost will then be propagated backwards layer by layer through the network to update the parameters.\n",
    "\n",
    "The [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) algorithm is used in every layer to update the parameters in the direction of the negative [gradient](http://en.wikipedia.org/wiki/Gradient).\n",
    "\n",
    "The parameters $w_h$ and $w_o$ are updated by $w(k+1) = w(k) - \\Delta w(k+1)$. $\\Delta w$ is defined as: $\\Delta w = \\mu * {\\partial \\xi}/{\\partial w}$ with $\\mu$ the learning rate and ${\\partial \\xi}/{\\partial w}$ the gradient of the parameter $w$ with respect to the cost function $\\xi$.\n",
    "\n",
    "#### Compute the cost function\n",
    "\n",
    "The cost function $\\xi$ used in this model is the same cross-entropy cost function explained in [intermezzo 1]({% post_url 2015-06-10-neural_network_implementation_intermezzo01 %}):\n",
    "\n",
    "$$\\xi(t_i,y_i) = - \\left[ t_i log(y_i) + (1-t_i)log(1-y_i) \\right]$$\n",
    "\n",
    "This cost function is plotted for the $w_h$ and $w_o$ parameters in the next figure. Note that this error surface is not convex anymore and that the $w_h$ parameter mirrors the cost function along the $w_h = 0$ axis.  \n",
    "Also, notice that this cost function has a very sharp gradient around $w_h = 0$ starting from $w_o > 0$ and that the minima run along the lower edge of this peak. If the learning rate will be to big, the updates might jump over the minima gap, onto the sharp gradient. Because the gradient is sharp, the update will be large, and we might end up further from the minima than we started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "def cost(y, t):\n",
    "    return - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))\n",
    "\n",
    "# Define a function to calculate the cost for a given set of parameters\n",
    "def cost_for_param(x, wh, wo, t):\n",
    "    return cost(nn(x, wh, wo) , t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the cost in function of the weights\n",
    "# Define a vector of weights for which we want to plot the cost\n",
    "nb_of_ws = 200 # compute the cost nb_of_ws times in each dimension\n",
    "wsh = np.linspace(-10, 10, num=nb_of_ws) # hidden weights\n",
    "wso = np.linspace(-10, 10, num=nb_of_ws) # output weights\n",
    "ws_x, ws_y = np.meshgrid(wsh, wso) # generate grid\n",
    "cost_ws = np.zeros((nb_of_ws, nb_of_ws)) # initialize cost matrix\n",
    "# Fill the cost matrix for each combination of weights\n",
    "for i in xrange(nb_of_ws):\n",
    "    for j in xrange(nb_of_ws):\n",
    "        cost_ws[i,j] = cost(nn(x, ws_x[i,j], ws_y[i,j]) , t)\n",
    "# Plot the cost function surface\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "# plot the surface\n",
    "surf = ax.plot_surface(ws_x, ws_y, cost_ws, linewidth=0, cmap=cm.pink)\n",
    "ax.view_init(elev=60, azim=-30)\n",
    "cbar = fig.colorbar(surf)\n",
    "ax.set_xlabel('$w_h$', fontsize=15)\n",
    "ax.set_ylabel('$w_o$', fontsize=15)\n",
    "ax.set_zlabel('$\\\\xi$', fontsize=15)\n",
    "cbar.ax.set_ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Cost function surface')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the output layer\n",
    "\n",
    "At the output the gradient for sample $i$, ${\\partial \\xi_i}/{\\partial w_o}$, can be worked out the same way as we did in [part 2]({% post_url 2015-06-10-neural_network_implementation_part02 %}):\n",
    "\n",
    "$$\\frac{\\partial \\xi_i}{\\partial w_o} = \\frac{\\partial z_{oi}}{\\partial w_o} \\frac{\\partial y_i}{\\partial z_{oi}} \\frac{\\partial \\xi_i}{\\partial y_i} = h_i (y_i-t_i) = h_i * \\delta_{oi}$$\n",
    "\n",
    "With $z_{oi} = h_i * w_o$, $h_i$ the hidden layer activation of sample $i$ and ${\\partial \\xi_i}/{\\partial z_{oi}} = \\delta_{oi}$ the gradient of the error at the output layer of the neural network with respect to the input to this layer.\n",
    "\n",
    "\n",
    "$\\delta_{o}$ is defined below as the `gradient_output(y, t)` method and ${\\partial \\xi}/{\\partial w_o}$ as the `gradient_weight_out(h, grad_output)` method.\n",
    "\n",
    "\n",
    "#### Update the hidden layer\n",
    "\n",
    "At the hidden layer the gradient for sample $i$, ${\\partial \\xi_i}/{\\partial w_{h}}$, of the hidden neuron is computed the same way:\n",
    "\n",
    "$$\\frac{\\partial \\xi_i}{\\partial w_{h}} = \\frac{\\partial z_{hi}}{\\partial w_{h}} \\frac{\\partial h_i}{\\partial z_{hi}} \\frac{\\partial \\xi_i}{\\partial h_i}$$\n",
    "\n",
    "With $z_{hi} = x_i * w_{h} $. And with ${\\partial \\xi_i}/{\\partial z_{hi}} = \\delta_{hi}$ the gradient of the error at the input of the hidden layer with respect to the input to this layer. This error can be interpreted as the contribution of $z_{hi}$ to the final error.\n",
    "How do we define this error gradient $\\delta_{hi}$ at the input of the hidden neurons? It can be computed as the error gradient propagated back from the output layer through the hidden layer.\n",
    "\n",
    "$$\\delta_{hi} = \\frac{\\partial \\xi_i}{\\partial z_{hi}} = \\frac{\\partial h_i}{\\partial z_{hi}} \\frac{\\partial z_{oi}}{\\partial h_i} \\frac{\\partial \\xi_i}{\\partial z_{oi}} \n",
    "= (-2 * z_{hi} * h_i) * w_{o} * (y_i - t_i) = -2 * z_{hi} * h_i * w_{o} * \\delta_{oi} $$\n",
    "\n",
    "Because of this, and because ${\\partial z_{hi}}/{\\partial w_{h}} = x_i$ we can compute ${\\partial \\xi_i}/{\\partial w_{h}}$ as:\n",
    "\n",
    "$$\\frac{\\partial \\xi_i}{\\partial w_{h}} = x_i \\delta_{hi}  $$\n",
    "\n",
    "The gradients for each parameter can again be summed up to compute the update for a batch of input examples.\n",
    "\n",
    "$\\delta_{h}$ is defined below as the `gradient_hidden(wo, grad_output)` method and ${\\partial \\xi}/{\\partial w_h}$ as the `gradient_weight_hidden(x, zh, h, grad_hidden)` method.\n",
    "\n",
    "To start out the gradient descent algorithm, you typically start with picking the initial parameters at random and start updating these parameters in the direction of the negative gradient with help of the backpropagation algorithm. One backpropagation iteration is implemented below by the `backprop_update(x, t, wh, wo, learning_rate)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the error function\n",
    "def gradient_output(y, t):\n",
    "    return y - t\n",
    "\n",
    "# Define the gradient function for the weight parameter at the output layer\n",
    "def gradient_weight_out(h, grad_output): \n",
    "    return  h * grad_output\n",
    "\n",
    "# Define the gradient function for the hidden layer\n",
    "def gradient_hidden(wo, grad_output):\n",
    "    return wo * grad_output\n",
    "\n",
    "# Define the gradient function for the weight parameter at the hidden layer\n",
    "def gradient_weight_hidden(x, zh, h, grad_hidden):\n",
    "    return x * -2 * zh * h * grad_hidden\n",
    "\n",
    "# Define the update function to update the network parameters over 1 iteration\n",
    "def backprop_update(x, t, wh, wo, learning_rate):\n",
    "    # Compute the output of the network\n",
    "    # This can be done with y = nn(x, wh, wo), but we need the intermediate \n",
    "    #  h and zh for the weight updates.\n",
    "    zh = x * wh\n",
    "    h = rbf(zh)  # hidden_activations(x, wh)\n",
    "    y = output_activations(h, wo)\n",
    "    # Compute the gradient at the output\n",
    "    grad_output = gradient_output(y, t)\n",
    "    # Get the delta for wo\n",
    "    d_wo = learning_rate * gradient_weight_out(h, grad_output)\n",
    "    # Compute the gradient at the hidden layer\n",
    "    grad_hidden = gradient_hidden(wo, grad_output)\n",
    "    # Get the delta for wh\n",
    "    d_wh = learning_rate * gradient_weight_hidden(x, zh, h, grad_hidden)\n",
    "    # return the update parameters\n",
    "    return (wh-d_wh.sum(), wo-d_wo.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation updates\n",
    "\n",
    "An example run of backpropagation for 50 iterations on the example inputs $\\mathbf{x}$ and targets $\\mathbf{t}$ is shown in the figure below. The white dots represent the weight parameter values $w_h$ and $w_o$ at iteration $k$ and are plotted on the cost surface.\n",
    "\n",
    "Notice that we decrease the learning rate linearly with each step. This is to make sure that in the end the learning rate is 0 and the sharp gradient will not allow the weight paramaters to fluctuate much during the last few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run backpropagation\n",
    "# Set the initial weight parameter\n",
    "wh = 2\n",
    "wo = -5\n",
    "# Set the learning rate\n",
    "learning_rate = 0.2\n",
    "\n",
    "# Plot the error surface\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_surface(ws_x, ws_y, cost_ws, linewidth=0, cmap=cm.pink)\n",
    "ax.view_init(elev=60, azim=-30)\n",
    "cbar = fig.colorbar(surf)\n",
    "cbar.ax.set_ylabel('$\\\\xi$', fontsize=15)\n",
    "\n",
    "# Start the gradient descent updates and plot the iterations\n",
    "nb_of_iterations = 50  # number of gradient descent updates\n",
    "lr_update = learning_rate / nb_of_iterations # learning rate update rule\n",
    "for i in xrange(nb_of_iterations):\n",
    "    learning_rate -= lr_update # decrease the learning rate\n",
    "    current_cost = cost_for_param(x, wh, wo, t) # Get the current cost\n",
    "    # Plot the weight-cost value and the line that represents the update \n",
    "    ax.plot([wh], [wo], [current_cost], 'w+')  # Plot the weight cost value\n",
    "    # Update the weights via backpropagation\n",
    "    wh_new, wo_new = backprop_update(x, t, wh, wo, learning_rate) \n",
    "    new_cost = cost_for_param(x, wh_new, wo_new, t) # Get the new cost\n",
    "    ax.plot([wh, wh_new], [wo, wo_new], [current_cost, new_cost], 'w-')\n",
    "    wh, wo = wh_new, wo_new  # Set the weight to the updated weights\n",
    "    \n",
    "# Plot the last weight, axis, and show figure\n",
    "ax.plot([wh], [wo], new_cost, 'w+')\n",
    "ax.set_xlabel('$w_h$', fontsize=15)\n",
    "ax.set_ylabel('$w_o$', fontsize=15)\n",
    "ax.set_zlabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Gradient descent updates on cost surface')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print the final cost\n",
    "print 'final cost is {:.2f} for weights wh: {:.2f} and wo: {:.2f}'.format(new_cost, wh, wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the trained classifier\n",
    "\n",
    "\n",
    "The resulting decision boundary of running backpropagation on the example inputs $\\mathbf{x}$ and targets $\\mathbf{t}$ is shown in the figure below. The background color (blue, red) refers to the classification decision of the trained classifier at that position in the input space. Note that all examples are classified correctly by the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the resulting decision boundary\n",
    "# Generate a grid over the input space to plot the color of the\n",
    "#  classification at that grid point\n",
    "nb_of_xs = 100\n",
    "xs = np.linspace(-3, 3, num=nb_of_xs)\n",
    "ys = np.linspace(-1, 1, num=nb_of_xs)\n",
    "xx, yy = np.meshgrid(xs, ys) # create the grid\n",
    "# Initialize and fill the classification plane\n",
    "classification_plane = np.zeros((nb_of_xs, nb_of_xs))\n",
    "for i in xrange(nb_of_xs):\n",
    "    for j in xrange(nb_of_xs):\n",
    "        classification_plane[i,j] = nn_predict(xx[i,j], wh, wo)\n",
    "# Create a color map to show the classification colors of each grid point\n",
    "cmap = ListedColormap([\n",
    "        colorConverter.to_rgba('r', alpha=0.25),\n",
    "        colorConverter.to_rgba('b', alpha=0.25)])\n",
    "\n",
    "# Plot the classification plane with decision boundary and input samples\n",
    "plt.figure(figsize=(8,0.5))\n",
    "plt.contourf(xx, yy, classification_plane, cmap=cmap)\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-1,1)\n",
    "# Plot samples from both classes as lines on a 1D space\n",
    "plt.plot(x_blue, np.zeros_like(x_blue), 'b|', ms = 30) \n",
    "plt.plot(x_red_left, np.zeros_like(x_red_left), 'r|', ms = 30) \n",
    "plt.plot(x_red_right, np.zeros_like(x_red_right), 'r|', ms = 30) \n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.title('Input samples and their classification')\n",
    "plt.xlabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of the input domain\n",
    "\n",
    "How is the neural network able to separate the non-linearly seperable classes with a linear logistic classifier at the output?  The key is the hidden layer with the non-linear RBF transfer function. Note that the RBF transfer function is able to transform the samples near the origin (blue class) to a value larger than $0$, and the samples further from the origin (red samples) to a value near $0$. This projection is plotted in the following figure. Note that the red samples are located around $0$ to the left, and that the blue samples are located more to the right. This projection is linearly seperable by the logistic classifier in the output layer.\n",
    "\n",
    "Also, note that the offset of the peak of the Gaussian function we use is $0$. This means that the Gaussian function is centered around the origin, which can be noted in the symmetrical decision boundaries around the origin on the previous figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot projected samples from both classes as lines on a 1D space\n",
    "plt.figure(figsize=(8,0.5))\n",
    "plt.xlim(-0.01,1)\n",
    "plt.ylim(-1,1)\n",
    "# Plot projected samples\n",
    "plt.plot(hidden_activations(x_blue, wh), np.zeros_like(x_blue), 'b|', ms = 30) \n",
    "plt.plot(hidden_activations(x_red_left, wh), np.zeros_like(x_red_left), 'r|', ms = 30) \n",
    "plt.plot(hidden_activations(x_red_right, wh), np.zeros_like(x_red_right), 'r|', ms = 30) \n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "plt.title('Projection of the input samples by the hidden layer.')\n",
    "plt.xlabel('h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is generated from an IPython notebook file. [Link to the full IPython notebook file](https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/neural_net_implementation/neural_network_implementation_part03.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

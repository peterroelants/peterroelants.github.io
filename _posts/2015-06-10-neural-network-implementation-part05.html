---
layout: default
title: Neural network implemetation - Generalization to multiple layers
description: Generalization to multiple layers.
---

<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <p>
    This last part of our tutorial on how to implement a neural network, generalizes the single layer feedforward neural network into any number of layers. The concepts of a linear projection via matrix multiplication and non-linear transformation will be generalized. The usage of the generalization will be illustrated by building a small feedforward network that consists of two hidden layers to classify handwritten digits. This network will be trained by
    <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">
     stochastic gradient descent
    </a>
    , a popular variant of gradient descent that updates the parameters each step on only a subset of the parameters.
   </p>
   <p>
    This part will cover:
   </p>
   <ul>
    <li>
     <a href="http://peterroelants.github.io/posts/neural_network_implementation_part05/#Generalization-of-the-layers">
      Generalization to multiple layers
     </a>
    </li>
    <li>
     Minibatches with
     <a href="http://peterroelants.github.io/posts/neural_network_implementation_part05/#Stochastic-gradient-descent-backpropagation">
      stochastic gradient descent
     </a>
    </li>
   </ul>
   <p>
    This is the last part of a 5-part tutorial on how to implement neural networks from scratch in Python:
   </p>
   <ul>
    <li>
     <a href="{% post_url 2015-06-10-neural-network-implementation-part01 %}">
      Part 1: Gradient descent
     </a>
    </li>
    <li>
     <a href="{% post_url 2015-06-10-neural-network-implementation-part02 %}">
      Part 2: Classification
     </a>
    </li>
    <li>
     <a href="{% post_url 2015-06-10-neural-network-implementation-part03 %}">
      Part 3: Hidden layers trained by backpropagation
     </a>
    </li>
    <li>
     <a href="{% post_url 2015-06-10-neural-network-implementation-part04 %}">
      Part 4: Vectorization of the operations
     </a>
    </li>
    <li>
     <a href="{% post_url 2015-06-10-neural-network-implementation-part05 %}">
      Part 5: Generalization to multiple layers (this)
     </a>
    </li>
   </ul>
   <p>
    The notebook starts out with importing the libraries we need:
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [1]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Imports</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_formats = ['svg']

<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>  <span class="c1"># Matrix and vector computation package</span>
<span class="c1"># data and evaluation utils</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">model_selection</span><span class="p">,</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  <span class="c1"># Plotting library</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">colorConverter</span><span class="p">,</span> <span class="n">ListedColormap</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>  <span class="c1"># Fancier plots</span>

<span class="c1"># Set seaborn plotting style</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">'darkgrid'</span><span class="p">)</span>
<span class="c1"># Set the seed for reproducability</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Handwritten-digits-dataset">
    Handwritten digits dataset
    <a class="anchor-link" href="#Handwritten-digits-dataset">
     ¶
    </a>
   </h2>
   <p>
    The dataset used in this tutorial is the
    <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html">
     digits dataset
    </a>
    provided by
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">
     scikit-learn
    </a>
    . This dataset consists of 1797 8x8 images of handwritten digits between 0 and 9. Each 8x8 pixel image is provided as a flattened input vector of 64 variables. Example images for each digit are shown below. Note that this dataset is different from the larger and more famous
    <a href="https://en.wikipedia.org/wiki/MNIST_database">
     MNIST
    </a>
    dataset. The smaller dataset from scikit-learn was chosen to minimize training time for this tutorial. Feel free to experiment and adapt this tutorial to classify the MNIST digits.
   </p>
   <p>
    The dataset will be split into:
   </p>
   <ul>
    <li>
     A training set used to train the model. (inputs:
     <code>
      X_train
     </code>
     , targets:
     <code>
      T_train
     </code>
     )
    </li>
    <li>
     A validation set used to validate the model performance and to stop training if the model starts
     <a href="https://en.wikipedia.org/wiki/Overfitting">
      overfitting
     </a>
     on the training data. (inputs:
     <code>
      X_validation
     </code>
     , targets:
     <code>
      T_validation
     </code>
     )
    </li>
    <li>
     A final
     <a href="https://en.wikipedia.org/wiki/Test_set">
      test set
     </a>
     to evaluate the trained model on data is independent of the training and validation data. (inputs:
     <code>
      X_test
     </code>
     , targets:
     <code>
      T_test
     </code>
     )
    </li>
   </ul>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [2]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Load the data from scikit-learn.</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="c1"># Load the targets.</span>
<span class="c1"># Note that the targets are stored as digits, these need to be </span>
<span class="c1">#  converted to one-hot-encoding for the output sofmax layer.</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">T</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">T</span><span class="p">)),</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Divide the data into a train and test set.</span>
<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">T_train</span><span class="p">,</span> <span class="n">T_test</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="c1"># Divide the test set into a validation set and final test set.</span>
<span class="p">(</span><span class="n">X_validation</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">T_validation</span><span class="p">,</span> <span class="n">T_test</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">T_test</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [3]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Plot an example of each image.</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'binary'</span><span class="p">)</span> 
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_svg output_subarea">
     <?xml version="1.0" encoding="utf-8" standalone="no"?>
     <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
     <svg height="61.688136pt" version="1.1" viewbox="0 0 572.4 61.688136" width="572.4pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <metadata>
       <rdf:rdf xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
        <cc:work>
         <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage">
         </dc:type>
         <dc:date>
          2021-05-05T21:27:56.731843
         </dc:date>
         <dc:format>
          image/svg+xml
         </dc:format>
         <dc:creator>
          <cc:agent>
           <dc:title>
            Matplotlib v3.4.1, https://matplotlib.org/
           </dc:title>
          </cc:agent>
         </dc:creator>
        </cc:work>
       </rdf:rdf>
      </metadata>
      <defs>
       <style type="text/css">
        *{stroke-linecap:butt;stroke-linejoin:round;}
       </style>
      </defs>
      <g id="figure_1">
       <g id="patch_1">
        <path d="M 0 61.688136 
L 572.4 61.688136 
L 572.4 0 
L 0 0 
z
" style="fill:#ffffff;">
        </path>
       </g>
       <g id="axes_1">
        <g clip-path="url(#p78dc120075)">
         <image height="47.52" id="imageb42604dfc1" transform="scale(1 -1)translate(0 -47.52)" width="47.52" x="7.2" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEIAAABCCAYAAADjVADoAAABYUlEQVR4nO3aQXHCQBxG8dCpgUgAJICEWMACFrBALEQCSAALxEGQkEhIL++8HzT00Jn3u/4pu/NmZ7oTsprnea4W6LoufuZ8Phfn+/2+OL9cLm/t6Te+/nyFf8IQMAQMAUPAEDAEVuke8Xw+i1+w2+3iIumucb1ei/NhGIrzx+MR95B4ImAIGAKGgCFgCBgChsB3+kC6UKWHKlVVVYfDoThvmmbRvG3buIfT6VSceyJgCBgChoAhYAgYAvEeMU1Tcf7Kg5mkruviPN1V0h5f4YmAIWAIGAKGgCFgCMR7RPoff7vd4iLpRZGle/gETwQMAUPAEDAEDAFDYPE9ou/7uEj6bWSz2RTn6UWQTzwT8UTAEDAEDAFDwBAwBAyBeKFKl5X1eh0X2W63xXl6keR+vxfnx+Mx7iHxRMAQMAQMAUPAEDAE4j0i+cTLnunhzziOi/7+FZ4IGAKGgCFgCBgChsAPnotDHJVEw8wAAAAASUVORK5CYII=" y="-6.968136"/>
        </g>
       </g>
       <g id="axes_2">
        <g clip-path="url(#p6d68a3587d)">
         <image height="47.52" id="image0008bffd60" transform="scale(1 -1)translate(0 -47.52)" width="47.52" x="63.945763" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEIAAABCCAYAAADjVADoAAABJElEQVR4nO3by42DMBgA4STaAugESoAO6ABKoQQ6gBbohFKgAvYyt934J3LIIZrvah7WyAcrOPfjOI7bxdq2TY4vy5Ic77ouOT7P84sz+uuR/YQvYQgYAoaAIWAIGAI/uQ/Yti28JtonROq6zrr/DFcEDAFDwBAwBAwBQ8AQMAQMAUPAEDAEDAFDwBDI/mHmE6qquvwdrggYAoaAIWAIGAKGQLiPGMcxOT4Mw5um8lxRFJe/wxUBQ8AQMAQMAUPAELjnHjhd1zW8Jjrose97cnyapuR43/fhHCKuCBgChoAhYAgYAoaAIZD9gefMx5doQxWdzD2zacvlioAhYAgYAoaAIWAIfOSgSPQvn7Isk+NN07xxNv9zRcAQMAQMAUPAEDAEfgFw4isLIzDaxgAAAABJRU5ErkJggg==" y="-6.968136"/>
        </g>
       </g>
       <g id="axes_3">
        <g clip-path="url(#p008a6df539)">
         <image height="47.52" id="image2b968487cc" transform="scale(1 -1)translate(0 -47.52)" width="47.52" x="120.691525" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEIAAABCCAYAAADjVADoAAABZUlEQVR4nO3aQY3CYBBA4WGz9yIBCcUBKAAJOAAJxUElIIEqwAI4QAIOupeXPW1mmi172vddh5Tm5U+Y0C7GcRzjj93v93TedV06H4YhnR+Px/Ie+r5P5x/lFf4JQ8AQMAQMAUPAEPisPlDtAIfDofySx+Mx9X5+tNvt0vl2u511/QhPxDdDwBAwBAwBQ8AQMATKhepyuaTzuctSRMT1ek3n1UL1Dp4IGAKGgCFgCBgChsCiesDzer3SC1QPTiLqPeH5fM6aL5fL8h4qnggYAoaAIWAIGAKGQLlHVKo9I6LeI06nUzrfbDazrj+FJwKGgCFgCBgChoAhMHuPqF4Wjah/5/f7fTo/n8/p/B3vzHoiYAgYAoaAIWAIGAKGwOyFqnozNyJivV6n86Zp0nn1AKd6ADSFJwKGgCFgCBgChoAhUL5wWmnbtvzM7XZL59UfM6vVavoN/ZInAoaAIWAIGAKGgCHwBY+bVJLYrSOUAAAAAElFTkSuQmCC" y="-6.968136"/>
        </g>
       </g>
       <g id="axes_4">
        <g clip-path="url(#pe47dcff557)">
         <image height="47.52" id="image55f761e41d" transform="scale(1 -1)translate(0 -47.52)" width="47.52" x="177.437288" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEIAAABCCAYAAADjVADoAAABTElEQVR4nO3awW2DQBBA0XGUBigBUQItQCu4BUowJdgluAZasVugBHL5yiliHI1zif67ghb0tZJHi0/7vu9RsCxLes/1eq08IsZx/NP1IyI+yiv8E4aAIWAIGAKGgCHwWV1g27b0ntvtVlpjmqZfvdNPslnDHQFDwBAwBAwBQ8AQMATKA9Xlcim/RDZQZQczTdOU38EdAUPAEDAEDAFDwBA4VT/wvCKbE4ZhKK2/rmt6TzZruCNgCBgChoAhYAgYAuXziOfzmd7Tdd3h9WyOuN/vh9c9j3gjQ8AQMAQMAUPAEEjniGxOeOU3vG3bw+uPx6P8jCp3BAwBQ8AQMAQMAUPAEEgHquxfs9mhSUTEPM+H18/n8+H17ANO9QNRhDvimyFgCBgChoAhYAikc0T2h9JXDk2yOSE7uOn7Pn1GlTsChoAhYAgYAoaAIfAF0j09sgLLjpgAAAAASUVORK5CYII=" y="-6.968136"/>
        </g>
       </g>
       <g id="axes_5">
        <g clip-path="url(#pd9d336fb37)">
         <image height="47.52" id="image61e12c10a9" transform="scale(1 -1)translate(0 -47.52)" width="47.52" x="234.183051" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEIAAABCCAYAAADjVADoAAABYElEQVR4nO3bwXGCUBRA0U8mey1BKrAFO9AOsAMpgQ5sgRLoQFuwAzsQKiCbu0vmPTMaFpl7tm8E5s5f/IFvNc/zXP7Y/X4P53Vdh/PL5RLOd7vdL5/ou4+Xr/BPGAKGgCFgCBgChsDnEjcZxzGcr1arcN73fTh3H/FGhoAhYAgYAoaAIWAIVNmLmev1Gl7gcDikN5mmKZxvt9twfrvdwvnj8UifYb1eh3NXBAwBQ8AQMAQMAUMgfTGT7QGyeSmlNE0TzrMXL1VVhfNhGNJnOB6P4dwVAUPAEDAEDAFDwBBI9xH7/T6cn8/n9CZd1z39QD85nU7hPDuI8gxXBAwBQ8AQMAQMAUMg/a6RyQ6BlJIf5Gjb9qXfP2Oz2YRzVwQMAUPAEDAEDAFDwBB4+eRtdgCjlPzjSvZiJdsMvYMrAoaAIWAIGAKGgCGwyD94sn1ENl+CKwKGgCFgCBgChoAh8AVFKkSKyqzqYAAAAABJRU5ErkJggg==" y="-6.968136"/>
        </g>
       </g>
       <g id="axes_6">
        <g clip-path="url(#p3c7d234ddd)">
         <image height="47.52" id="imageb8a7817bb2" transform="scale(1 -1)translate(0 -47.52)" width="47.52" x="290.928814" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEIAAABCCAYAAADjVADoAAABU0lEQVR4nO3bwW3CQBBA0SXK3S6BUuwO6MAl4BLoALsDWqACQweUQAlQgXP5ynHH0qAcov+u49joaw+jYHbruq4lYRzH8Jp5njOPKMMwVOeXyyV1/1JK+Urf4Z8wBAwBQ8AQMAQMgV12j7her+E1TdNU56fTqTq/3+/V+bIs4Wfouq4690TAEDAEDAFDwBAwBAyB9EL1CbfbrTrv+746Px6P4TOmaarOPREwBAwBQ8AQMAQMge+/eEi0Jzwej9T99/t96u9L8UT8MgQMAUPAEDAEDIH0HnE4HMJrtnwJVHM+n6vzLS+rRDwRMAQMAUPAEDAEDIH0HvF8PtMfInqRJPp/xev1Cp/Rtm117omAIWAIGAKGgCFgCBgC6YVqyzITeb/f1Xn0BVG0LG3hiYAhYAgYAoaAIWAIpPeILS9pRL+e+cQvebM8ETAEDAFDwBAwBAyBH34OP+CzSF/wAAAAAElFTkSuQmCC" y="-6.968136"/>
        </g>
       </g>
       <g id="axes_7">
        <g clip-path="url(#pfdd4f128f7)">
         <image height="47.52" id="imagebedc84f2d2" transform="scale(1 -1)translate(0 -47.52)" width="47.52" x="347.674576" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEIAAABCCAYAAADjVADoAAABPUlEQVR4nO3bQY2EQBQA0Z7NCkACEpCAA5AEEnCABCQgARwgARSwlzr3J+kJl613/YSQSifToYfPfd93KnCeZ3jNMAzZ+TzP2XnbtkX3Tymlpmmy85/wDv+EIWAIGAKGgCFgCPyW3iDaA6SU0jRN2XnXddn5uq7Z+bIs4TO4j3jIEDAEDAFDwBAwBAyBVzZUkeM4svPrurLzvu+Ln8EVAUPAEDAEDAFDwBD4lB7wVFUVXhO9FIn2AdG8ruvwGSKuCBgChoAhYAgYAoZA8T7iyW94tNfYtq3kEb7CFQFDwBAwBAwBQ8AQKD7XePI+Yt/37DzaR0TvM77BFQFDwBAwBAwBQ8AQMASKN1TjOIbXRAc00T9n3VC9yBAwBAwBQ8AQMASKD3ieiL7kjb4mfuMAyBUBQ8AQMAQMAUPAEPgDapJB0pcblBkAAAAASUVORK5CYII=" y="-6.968136"/>
        </g>
       </g>
       <g id="axes_8">
        <g clip-path="url(#p68434393a8)">
         <image height="47.52" id="imagee82c979283" transform="scale(1 -1)translate(0 -47.52)" width="47.52" x="404.420339" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEIAAABCCAYAAADjVADoAAABRUlEQVR4nO3bzW2DMBxAcVP1DqMwAhvABjACK7ABozACK7ABI7ABvbxbJf9pnX4c3u/qJLaeLMWxSHXf950KtG0bvmZZlux43/clS3iJt79ewH9hCBgChoAhYAgYAu+lH9B1XfiacRyz4+d5Zsebpnm+oG9yR8AQMAQMAUPAEDAEDIGq9GLm0SRVlR3fti07/hsXN+4IGAKGgCFgCBgChkB4joguTeZ5DifZ9/0LS/osOmc8uRyKuCNgCBgChoAhYAgYAuE5IjoDlJ4RUkppXdcff/80TdlxdwQMAUPAEDAEDAFDIHxQJPqt/4q7gEh0HzEMQ/Ec7ggYAoaAIWAIGAKGgCFQ/KBI6aVKSvG/gKI5rusK54gukNwRMAQMAUPAEDAEDIHif/A8EX3PRw+bHMeRHa/rungN7ggYAoaAIWAIGAKGwAfEP0gvX/WHHQAAAABJRU5ErkJggg==" y="-6.968136"/>
        </g>
       </g>
       <g id="axes_9">
        <g clip-path="url(#p3b9bf4d89a)">
         <image height="47.52" id="imagea2559a2b52" transform="scale(1 -1)translate(0 -47.52)" width="47.52" x="461.166102" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEIAAABCCAYAAADjVADoAAABX0lEQVR4nO3aQZHCQBgF4bC1d5CAA0AB4AAHRAIOgoRIwEIcxAlIAAXZS5/nTTHH7e86qSTVNYe/Jlkty7J0DS6XS7xmmqbi+nq9Lq6fTqfi+uPxiO+w2WyK6z/xDv+EIWAIGAKGgCFgCKzSHPF+v4s32G638SGv16tp/XA4FNeHYYjvcL/fi+vuCBgChoAhYAgYAoaAIfCbLkjDzufziQ8Zx7HpGbvdrrje9318h8QdAUPAEDAEDAFDwBCIBzPJPM/xmvP53PKI7vl8FtdrDocSdwQMAUPAEDAEDAFDIJ5HJOkHjBrH47G4nn4UqZll0qzhjoAhYAgYAoaAIWAINM8RNT97Xq/XpnukWSV9F+k654hqhoAhYAgYAoaAIWAINH/gSYcmXVc38JSkYajmYCZxR8AQMAQMAUPAEDAEmg9m9vt9vCbNEemH0dvtVv0+33JHwBAwBAwBQ8AQMAT+AFBVS+Pv3zIgAAAAAElFTkSuQmCC" y="-6.968136"/>
        </g>
       </g>
       <g id="axes_10">
        <g clip-path="url(#pda2f8d376d)">
         <image height="47.52" id="image681b987777" transform="scale(1 -1)translate(0 -47.52)" width="47.52" x="517.911864" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAAEIAAABCCAYAAADjVADoAAABRUlEQVR4nO3awW2DMBxGcafqIGSCMEJGyCZkJEZgBEaADdgg2YBennqq/CEZ9RC939UNtZ58+Atz2fd9Lw2ez2f8m2VZquvv97u6Po5jdb3v+7iH5Kv5CR/CEDAEDAFDwBAwBC6tc8QZ0iyS5ohpmuL/uN/v1XVPBAwBQ8AQMAQMAUPAEPiXgSq9mEkvVtLAtW1b3EMaujwRMAQMAUPAEDAEDIHv1gccuVxZ17W6PgxDdT29mElzyhGeCBgChoAhYAgYAoZAnCPSRxxpRiillNfrVV1/PB7xGTVd1zX9vhRPxC9DwBAwBAwBQ8AQiHPEkTuDJM0JaQ44Yw+JJwKGgCFgCBgChoAhYAjED0XSMHO9Xs/cz59ut1t13QueExkChoAhYAgYAoZA8wenRy5n0iXRPM8tWziFJwKGgCFgCBgChoAh8AO7olECzxeH3QAAAABJRU5ErkJggg==" y="-6.968136"/>
        </g>
       </g>
      </g>
      <defs>
       <clippath id="p78dc120075">
        <rect height="47.288136" width="47.288136" x="7.2" y="7.2">
        </rect>
       </clippath>
       <clippath id="p6d68a3587d">
        <rect height="47.288136" width="47.288136" x="63.945763" y="7.2">
        </rect>
       </clippath>
       <clippath id="p008a6df539">
        <rect height="47.288136" width="47.288136" x="120.691525" y="7.2">
        </rect>
       </clippath>
       <clippath id="pe47dcff557">
        <rect height="47.288136" width="47.288136" x="177.437288" y="7.2">
        </rect>
       </clippath>
       <clippath id="pd9d336fb37">
        <rect height="47.288136" width="47.288136" x="234.183051" y="7.2">
        </rect>
       </clippath>
       <clippath id="p3c7d234ddd">
        <rect height="47.288136" width="47.288136" x="290.928814" y="7.2">
        </rect>
       </clippath>
       <clippath id="pfdd4f128f7">
        <rect height="47.288136" width="47.288136" x="347.674576" y="7.2">
        </rect>
       </clippath>
       <clippath id="p68434393a8">
        <rect height="47.288136" width="47.288136" x="404.420339" y="7.2">
        </rect>
       </clippath>
       <clippath id="p3b9bf4d89a">
        <rect height="47.288136" width="47.288136" x="461.166102" y="7.2">
        </rect>
       </clippath>
       <clippath id="pda2f8d376d">
        <rect height="47.288136" width="47.288136" x="517.911864" y="7.2">
        </rect>
       </clippath>
      </defs>
     </svg>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Generalization-of-the-layers">
    Generalization of the layers
    <a class="anchor-link" href="#Generalization-of-the-layers">
     ¶
    </a>
   </h2>
   <p>
    <a href="{% post_url 2015-06-10-neural-network-implementation-part04 %}">
     Part 4
    </a>
    of this tutorial series took on the classical view of neural networks where each layer consists of a
    <a href="https://en.wikipedia.org/wiki/Linear_map">
     linear transformation
    </a>
    by a
    <a href="https://en.wikipedia.org/wiki/Transformation_matrix">
     matrix multiplication
    </a>
    and vector addition followed by a non-linear function.
    <br/>
    Here the linear transformation is split from the non-linear function and each is abstracted into its own layer. This has the benefit that the forward and backward step of each layer can easily be calculated separately.
   </p>
   <p>
    This tutorial defines three example layers as
    <a href="https://docs.python.org/3/tutorial/classes.html">
     Python classes
    </a>
    :
   </p>
   <ul>
    <li>
     A layer to apply the linear transformation (
     <code>
      LinearLayer
     </code>
     ).
    </li>
    <li>
     A layer to apply the logistic function (
     <code>
      LogisticLayer
     </code>
     ).
    </li>
    <li>
     A layer to compute the softmax classification probabilities at the output (
     <code>
      SoftmaxOutputLayer
     </code>
     ).
    </li>
   </ul>
   <p>
    Each layer can compute its output in the forward step with
    <code>
     get_output
    </code>
    , which can then be used as the input for the next layer. The gradient at the input of each layer in the backpropagation step can be computed with
    <code>
     get_input_grad
    </code>
    . This function computes the gradient with the help of the targets if it's the last layer, or the gradients at its outputs (gradients at input of next layer) if it's an intermediate layer.
Each layer has the option to
    <a href="https://docs.python.org/3/library/stdtypes.html#iterator-types">
     iterate
    </a>
    over the parameters (if any) with
    <code>
     get_params_iter
    </code>
    , and get the gradients of these parameters in the same order with
    <code>
     get_params_grad
    </code>
    .
   </p>
   <p>
    Notice that the gradient and cost computed by the softmax layer are divided by the number of input samples. This is to make this gradient and cost independent of the number of input samples so that the size of the mini-batches can be changed without affecting other parameters. More on this later.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [4]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Non-linear functions used</span>
<span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">logistic_deriv</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>  <span class="c1"># Derivative of logistic function</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [5]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Layers used in this model</span>
<span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Base class for the different layers.</span>
<span class="sd">    Defines base methods and documentation of methods."""</span>
    
    <span class="k">def</span> <span class="nf">get_params_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return an iterator over the parameters (if any).</span>
<span class="sd">        The iterator has the same order as get_params_grad.</span>
<span class="sd">        The elements returned by the iterator are editable in-place."""</span>
        <span class="k">return</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="nf">get_params_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return a list of gradients over the parameters.</span>
<span class="sd">        The list has the same order as the get_params_iter iterator.</span>
<span class="sd">        X is the input.</span>
<span class="sd">        output_grad is the gradient at the output of this layer.</span>
<span class="sd">        """</span>
        <span class="k">return</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step linear transformation.</span>
<span class="sd">        X is the input."""</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer.</span>
<span class="sd">        Y is the pre-computed output of this layer (not needed in </span>
<span class="sd">        this case).</span>
<span class="sd">        output_grad is the gradient at the output of this layer </span>
<span class="sd">         (gradient at input of next layer).</span>
<span class="sd">        Output layer uses targets T to compute the gradient based on the </span>
<span class="sd">         output error instead of output_grad"""</span>
        <span class="k">pass</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [6]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""The linear layer performs a linear transformation to its input."""</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
        <span class="sd">"""Initialize hidden layer parameters.</span>
<span class="sd">        n_in is the number of input variables.</span>
<span class="sd">        n_out is the number of output variables."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_out</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">get_params_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return an iterator over the parameters."""</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]))</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step linear transformation."""</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        
    <span class="k">def</span> <span class="nf">get_params_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return a list of gradients over the parameters."""</span>
        <span class="n">JW</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">output_grad</span>
        <span class="n">Jb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">output_grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">JW</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">Jb</span><span class="p">))]</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="k">return</span> <span class="n">output_grad</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [7]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="k">class</span> <span class="nc">LogisticLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""The logistic layer applies the logistic function to its inputs."""</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="n">logistic</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">logistic_deriv</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="n">output_grad</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [8]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="k">class</span> <span class="nc">SoftmaxOutputLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""The softmax output layer computes the classification </span>
<span class="sd">    propabilities at the output."""</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">get_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the cost at the output of this output layer."""</span>
        <span class="k">return</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Sample-model">
    Sample model
    <a class="anchor-link" href="#Sample-model">
     ¶
    </a>
   </h3>
   <p>
    The following sections will refer to a layer as a layer defined above, and hidden-layer or output-layer as the classical neural network view of a linear transformation followed by a non-linear function.
   </p>
   <p>
    The sample model used to classify the handwritten digits in this tutorial consists of two hidden-layers with logistic functions and a softmax output-layer. The fist hidden-layer takes a vector of 64 pixel values and transforms them to a vector of 20 values. The second hidden-layer projects the previous 20 values to 20 new values. The output-layer outputs probabilities for the 10 possible classes. This architecture is illustrated in the following figure (biases are not shown to keep figure clean).
   </p>
   <p>
    <img alt="Image of the sample neural network model" src="https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/neural_net_implementation/img/SimpleANN05.png"/>
   </p>
   <p>
    The full network is represented as a sequential list where each next layer is added on top of the previous layer by putting it in the next position in the list. The first layer is at position 0 in this list, the last layer is at the last index of this list.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [9]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Sample model to be trained on the data</span>
<span class="n">hidden_neurons_1</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of neurons in the first hidden-layer</span>
<span class="n">hidden_neurons_2</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of neurons in the second hidden-layer</span>
<span class="c1"># Create the model</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># Define a list of layers</span>
<span class="c1"># Add first hidden layer</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_neurons_1</span><span class="p">))</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LogisticLayer</span><span class="p">())</span>
<span class="c1"># Add second hidden layer</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_neurons_1</span><span class="p">,</span> <span class="n">hidden_neurons_2</span><span class="p">))</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LogisticLayer</span><span class="p">())</span>
<span class="c1"># Add output layer</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_neurons_2</span><span class="p">,</span> <span class="n">T_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SoftmaxOutputLayer</span><span class="p">())</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Backpropagation">
    Backpropagation
    <a class="anchor-link" href="#Backpropagation">
     ¶
    </a>
   </h2>
   <p>
    The details of how backpropagation works in the forward and backward step of vectorized model are explained in
    <a href="{% post_url 2015-06-10-neural-network-implementation-part04 %}">
     part 4
    </a>
    of this tutorial. This section will only illustrate how to perform backpropagation over any number of layers with the generalized model described here.
   </p>
   <h3 id="Forward-step">
    Forward step
    <a class="anchor-link" href="#Forward-step">
     ¶
    </a>
   </h3>
   <p>
    The forward steps are computed by the
    <code>
     forward_step
    </code>
    method defined below. This method iteratively computes the outputs of each layer and feeds it as input to the next layer until the last layer. Each layer's output is computed by calling the
    <code>
     get_output
    </code>
    method. These output activations are stored in the
    <code>
     activations
    </code>
    list.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [10]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Forward propagation step as a method.</span>
<span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="n">input_samples</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Compute and return the forward activation of each layer in layers.</span>
<span class="sd">    Input:</span>
<span class="sd">        input_samples: A matrix of input samples (each row </span>
<span class="sd">                       is an input vector)</span>
<span class="sd">        layers: A list of Layers</span>
<span class="sd">    Output:</span>
<span class="sd">        A list of activations where the activation at each index </span>
<span class="sd">        i+1 corresponds to the activation of layer i in layers. </span>
<span class="sd">        activations[0] contains the input samples.  </span>
<span class="sd">    """</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_samples</span><span class="p">]</span> <span class="c1"># List of layer activations</span>
    <span class="c1"># Compute the forward activations for each layer starting </span>
    <span class="c1">#  from the first</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">input_samples</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="c1"># Get the output of the current layer</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Store the output for future processing</span>
        <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
        <span class="c1"># Set the current input as the activations of the previous layer</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">activations</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Backward-step">
    Backward step
    <a class="anchor-link" href="#Backward-step">
     ¶
    </a>
   </h3>
   <p>
    The gradients computed in the backward step are computed by the
    <code>
     backward_step
    </code>
    method defined below. The backward step goes over all the layers in the reversed order. It first gets the initial gradients from the output layer and uses these gradients to compute the gradients of the layers below by iteratively calling the
    <code>
     get_input_grad
    </code>
    method. During each step, it computes the gradients of the cost with respect to the parameters by calling the
    <code>
     get_params_grad
    </code>
    method and returns them in a list.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [11]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Define the backward propagation step as a method</span>
<span class="k">def</span> <span class="nf">backward_step</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Perform the backpropagation step over all the layers and return the parameter gradients.</span>
<span class="sd">    Input:</span>
<span class="sd">        activations: A list of forward step activations where the activation at </span>
<span class="sd">            each index i+1 corresponds to the activation of layer i in layers. </span>
<span class="sd">            activations[0] contains the input samples. </span>
<span class="sd">        targets: The output targets of the output layer.</span>
<span class="sd">        layers: A list of Layers corresponding that generated the outputs in activations.</span>
<span class="sd">    Output:</span>
<span class="sd">        A list of parameter gradients where the gradients at each index corresponds to</span>
<span class="sd">        the parameters gradients of the layer at the same index in layers. </span>
<span class="sd">    """</span>
    <span class="n">param_grads</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">()</span>  <span class="c1"># List of parameter gradients for each layer</span>
    <span class="n">output_grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># The error gradient at the output of the current layer</span>
    <span class="c1"># Propagate the error backwards through all the layers.</span>
    <span class="c1">#  Use reversed to iterate backwards over the list of layers.</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>   
        <span class="n">Y</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>  <span class="c1"># Get the activations of the last layer on the stack</span>
        <span class="c1"># Compute the error at the output layer.</span>
        <span class="c1"># The output layer error is calculated different then hidden layer error.</span>
        <span class="k">if</span> <span class="n">output_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_grad</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_input_grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># output_grad is not None (layer is not output layer)</span>
            <span class="n">input_grad</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_input_grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
        <span class="c1"># Get the input of this layer (activations of the previous layer)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Compute the layer parameter gradients used to update the parameters</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_params_grad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
        <span class="n">param_grads</span><span class="o">.</span><span class="n">appendleft</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="c1"># Compute gradient at output of previous layer (input of current layer):</span>
        <span class="n">output_grad</span> <span class="o">=</span> <span class="n">input_grad</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">param_grads</span><span class="p">)</span>  <span class="c1"># Return the parameter gradients</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Gradient-Checking">
    Gradient Checking
    <a class="anchor-link" href="#Gradient-Checking">
     ¶
    </a>
   </h2>
   <p>
    As in
    <a href="{% post_url 2015-06-10-neural-network-implementation-part04 %}">
     part 4
    </a>
    of this tutorial the gradient computed by backpropagation is compared with the
    <a href="https://en.wikipedia.org/wiki/Numerical_differentiation">
     numerical gradient
    </a>
    to assert that there are no bugs in the code to compute the gradients.
   </p>
   <p>
    The code below gets the parameters of each layer by the help of the
    <code>
     get_params_iter
    </code>
    method that returns an iterator over all the parameters in the layer. The order of parameters returned corresponds to the order of parameter gradients returned by
    <code>
     get_params_grad
    </code>
    during backpropagation.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [12]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Perform gradient checking</span>
<span class="c1"># Test the gradients on a subset of the data</span>
<span class="n">nb_samples_gradientcheck</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">X_temp</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">nb_samples_gradientcheck</span><span class="p">,:]</span>
<span class="n">T_temp</span> <span class="o">=</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">nb_samples_gradientcheck</span><span class="p">,:]</span>
<span class="c1"># Get the parameter gradients with backpropagation</span>
<span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
<span class="n">param_grads</span> <span class="o">=</span> <span class="n">backward_step</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">T_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>

<span class="c1"># Set the small change to compute the numerical gradient</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="c1"># Compute the numerical gradients of the parameters in all layers.</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)):</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">layer_backprop_grads</span> <span class="o">=</span> <span class="n">param_grads</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="c1"># Compute the numerical gradient for each parameter in the layer</span>
    <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
        <span class="n">grad_backprop</span> <span class="o">=</span> <span class="n">layer_backprop_grads</span><span class="p">[</span><span class="n">p_idx</span><span class="p">]</span>
        <span class="c1"># + eps</span>
        <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
        <span class="n">plus_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span>
            <span class="n">forward_step</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_temp</span><span class="p">)</span>
        <span class="c1"># - eps</span>
        <span class="n">param</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span>
        <span class="n">min_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span>
            <span class="n">forward_step</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_temp</span><span class="p">)</span>
        <span class="c1"># reset param value</span>
        <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
        <span class="c1"># calculate numerical gradient</span>
        <span class="n">grad_num</span> <span class="o">=</span> <span class="p">(</span><span class="n">plus_cost</span> <span class="o">-</span> <span class="n">min_cost</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps</span><span class="p">)</span>
        <span class="c1"># Raise error if the numerical grade is not close to the </span>
        <span class="c1">#  backprop gradient</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">grad_num</span><span class="p">,</span> <span class="n">grad_backprop</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span>
                <span class="sa">f</span><span class="s1">'Numerical gradient of </span><span class="si">{</span><span class="n">grad_num</span><span class="si">:</span><span class="s1">.6f</span><span class="si">}</span><span class="s1"> is '</span>
                <span class="s1">'not close to the backpropagation gradient '</span>
                <span class="sa">f</span><span class="s1">'of </span><span class="si">{</span><span class="n">grad_backprop</span><span class="si">:</span><span class="s1">.6f</span><span class="si">}</span><span class="s1">!'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'No gradient errors found'</span><span class="p">)</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>No gradient errors found
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Stochastic-gradient-descent-backpropagation">
    Stochastic gradient descent backpropagation
    <a class="anchor-link" href="#Stochastic-gradient-descent-backpropagation">
     ¶
    </a>
   </h2>
   <p>
    This tutorial uses a variant of gradient descent called
    <a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">
     Stochastic gradient descent
    </a>
    (SGD) to optimize the cost function. SGD follows the negative gradient of the cost function on subsets of the total training set. This has a few benefits: One of them is that the training time on large datasets can be reduced because the matrix of samples is much smaller during each sub-iteration and the gradients can be computed faster and with less memory. Another benefit is that computing the cost function on subsets results in noise, i.e. each subset will give a different cost depending on the samples. This will result in noisy (stochastic) gradient updates which might be able to push the gradient descent out of local minima. These, and other, benefits contribute to the popularity of SGD on training large scale machine learning methods such as neural networks.
   </p>
   <p>
    The cost function needs to be independent of the number of input samples because the size of the subsets used during SGD can vary. This is why the
    <a href="https://en.wikipedia.org/wiki/Mean_squared_error">
     mean squared error
    </a>
    (MSE) cost function is used instead of just the squared error. Using the mean instead of the sum is reflected in the gradient and cost computed by the softmax layer being divided by the number of input samples.
   </p>
   <h3 id="Minibatches">
    Minibatches
    <a class="anchor-link" href="#Minibatches">
     ¶
    </a>
   </h3>
   <p>
    The subsets of the training set are often called mini-batches. The following code will divide the training set into mini-batches of around 25 samples per batch. The inputs and targets are combined together in a list of (input, target) tuples.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [13]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Create the minibatches</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">25</span>  <span class="c1"># Approximately 25 samples per batch</span>
<span class="n">nb_of_batches</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span>  <span class="c1"># Number of batches</span>
<span class="c1"># Create batches (X,Y) from the training set</span>
<span class="n">XT_batches</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">nb_of_batches</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>   <span class="c1"># X samples</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">T_train</span><span class="p">,</span> <span class="n">nb_of_batches</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>  <span class="c1"># Y targets</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="SGD-updates">
    SGD updates
    <a class="anchor-link" href="#SGD-updates">
     ¶
    </a>
   </h3>
   <p>
    The parameters $\mathbf{\theta}$ of the network are updated by the
    <code>
     update_params
    </code>
    method that iterates over each parameter of each layer and applies the simple
    <a href="https://en.wikipedia.org/wiki/Gradient_descent">
     gradient descent
    </a>
    rule on each mini-batch: $\mathbf{\theta}(k+1) = \mathbf{\theta}(k) - \Delta \mathbf{\theta}(k+1)$. $\Delta \mathbf{\theta}$ is defined as: $\Delta \mathbf{\theta} = \mu \frac{\partial \xi}{\partial \mathbf{\theta}}$ with $\mu$ the learning rate.
   </p>
   <p>
    The update steps will be performed for a number of iterations (
    <code>
     nb_of_iterations
    </code>
    ) over the full training set, where each full iterations consists of multiple updates over the mini-batches. After each full iteration, the resulting network will be tested on the validation set. The training will stop if the cost on the validation set doesn't increase after three full iterations to prevent overfitting or after maximum 300 iterations. All costs will be stored in between for future analysis.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [14]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Define a method to update the parameters</span>
<span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">param_grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Function to update the parameters of the given layers with the given </span>
<span class="sd">    gradients by gradient descent with the given learning rate.</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">layer_backprop_grads</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">param_grads</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">(),</span> 
                               <span class="n">layer_backprop_grads</span><span class="p">):</span>
            <span class="c1"># The parameter returned by the iterator point to the </span>
            <span class="c1">#  memory space of the original layer and can thus be </span>
            <span class="c1">#  modified inplace.</span>
            <span class="n">param</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>  <span class="c1"># Update each parameter</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [15]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Perform backpropagation</span>
<span class="c1"># initalize some lists to store the cost for future analysis        </span>
<span class="n">batch_costs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_costs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_costs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">max_nb_of_iterations</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># Train for a maximum of 300 iterations</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Gradient descent learning rate</span>

<span class="c1"># Train for the maximum number of iterations</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_nb_of_iterations</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span> <span class="ow">in</span> <span class="n">XT_batches</span><span class="p">:</span>  <span class="c1"># For each minibatch sub-iteration</span>
        <span class="c1"># Get the activations</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
        <span class="c1"># Get cost</span>
        <span class="n">batch_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">batch_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_cost</span><span class="p">)</span>
        <span class="c1"># Get the gradients</span>
        <span class="n">param_grads</span> <span class="o">=</span> <span class="n">backward_step</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
        <span class="c1"># Update the parameters</span>
        <span class="n">update_params</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">param_grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
    <span class="c1"># Get full training cost for future analysis (plots)</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
    <span class="n">train_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_train</span><span class="p">)</span>
    <span class="n">train_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_cost</span><span class="p">)</span>
    <span class="c1"># Get full validation cost</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_validation</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
    <span class="n">validation_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span>
        <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_validation</span><span class="p">)</span>
    <span class="n">val_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">validation_cost</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_costs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Stop training if the cost on the validation set doesn't </span>
        <span class="c1"># decrease for 3 iterations</span>
        <span class="k">if</span> <span class="n">val_costs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">val_costs</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">val_costs</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]:</span>
            <span class="k">break</span>
    
<span class="c1"># The number of iterations that have been executed</span>
<span class="n">nb_of_iterations</span> <span class="o">=</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <p>
    The costs stored during training can be plotted to visualize the performance during training. The resulting plot is shown in the next figure. The cost on the training samples and validation samples goes down very quickly and flattens out after about 40 iterations on the full training set. Notice that the cost on the training set is lower than the cost on the validation set, this is because the network is optimized on the training set and is slightly overfitting. The training stops after around 90 iterations because the validation cost stops decreasing.
    <br/>
    Also, notice that the cost of the mini-batches fluctuates around the cost of the full training set. This is the stochastic effect of mini-batches in SGD.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [16]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Plot the minibatch, full training set, and validation costs</span>
<span class="n">batch_x_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
    <span class="mi">0</span><span class="p">,</span> <span class="n">nb_of_iterations</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">nb_of_iterations</span><span class="o">*</span><span class="n">nb_of_batches</span><span class="p">)</span>
<span class="n">iteration_x_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
    <span class="mi">1</span><span class="p">,</span> <span class="n">nb_of_iterations</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">nb_of_iterations</span><span class="p">)</span>
<span class="c1"># Plot the cost over the iterations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">batch_x_inds</span><span class="p">,</span> <span class="n">batch_costs</span><span class="p">,</span> 
         <span class="s1">'k-'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'cost minibatches'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iteration_x_inds</span><span class="p">,</span> <span class="n">train_costs</span><span class="p">,</span> 
         <span class="s1">'r-'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'cost full training set'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iteration_x_inds</span><span class="p">,</span> <span class="n">val_costs</span><span class="p">,</span> 
         <span class="s1">'b-'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'cost validation set'</span><span class="p">)</span>
<span class="c1"># Add labels to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'$</span><span class="se">\\</span><span class="s1">xi$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Decrease of cost over backprop iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># x1, x2, y1, y2 = plt.axis()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">nb_of_iterations</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_svg output_subarea">
     <?xml version="1.0" encoding="utf-8" standalone="no"?>
     <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
     <svg height="281.634375pt" version="1.1" viewbox="0 0 387.743125 281.634375" width="387.743125pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <metadata>
       <rdf:rdf xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
        <cc:work>
         <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage">
         </dc:type>
         <dc:date>
          2021-05-05T21:28:17.886024
         </dc:date>
         <dc:format>
          image/svg+xml
         </dc:format>
         <dc:creator>
          <cc:agent>
           <dc:title>
            Matplotlib v3.4.1, https://matplotlib.org/
           </dc:title>
          </cc:agent>
         </dc:creator>
        </cc:work>
       </rdf:rdf>
      </metadata>
      <defs>
       <style type="text/css">
        *{stroke-linecap:butt;stroke-linejoin:round;}
       </style>
      </defs>
      <g id="figure_1">
       <g id="patch_1">
        <path d="M 0 281.634375 
L 387.743125 281.634375 
L 387.743125 0 
L 0 0 
z
" style="fill:#ffffff;">
        </path>
       </g>
       <g id="axes_1">
        <g id="patch_2">
         <path d="M 45.743125 244.078125 
L 380.543125 244.078125 
L 380.543125 22.318125 
L 45.743125 22.318125 
z
" style="fill:#eaeaf2;">
         </path>
        </g>
        <g id="matplotlib.axis_1">
         <g id="xtick_1">
          <g id="line2d_1">
           <path clip-path="url(#p5125e43100)" d="M 45.743125 244.078125 
L 45.743125 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_1">
           <!-- 0 -->
           <g style="fill:#262626;" transform="translate(42.561875 258.676562)scale(0.1 -0.1)">
            <defs>
             <path d="M 2034 4250 
Q 1547 4250 1301 3770 
Q 1056 3291 1056 2328 
Q 1056 1369 1301 889 
Q 1547 409 2034 409 
Q 2525 409 2770 889 
Q 3016 1369 3016 2328 
Q 3016 3291 2770 3770 
Q 2525 4250 2034 4250 
z
M 2034 4750 
Q 2819 4750 3233 4129 
Q 3647 3509 3647 2328 
Q 3647 1150 3233 529 
Q 2819 -91 2034 -91 
Q 1250 -91 836 529 
Q 422 1150 422 2328 
Q 422 3509 836 4129 
Q 1250 4750 2034 4750 
z
" id="DejaVuSans-30" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_2">
          <g id="line2d_2">
           <path clip-path="url(#p5125e43100)" d="M 114.069656 244.078125 
L 114.069656 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_2">
           <!-- 20 -->
           <g style="fill:#262626;" transform="translate(107.707156 258.676562)scale(0.1 -0.1)">
            <defs>
             <path d="M 1228 531 
L 3431 531 
L 3431 0 
L 469 0 
L 469 531 
Q 828 903 1448 1529 
Q 2069 2156 2228 2338 
Q 2531 2678 2651 2914 
Q 2772 3150 2772 3378 
Q 2772 3750 2511 3984 
Q 2250 4219 1831 4219 
Q 1534 4219 1204 4116 
Q 875 4013 500 3803 
L 500 4441 
Q 881 4594 1212 4672 
Q 1544 4750 1819 4750 
Q 2544 4750 2975 4387 
Q 3406 4025 3406 3419 
Q 3406 3131 3298 2873 
Q 3191 2616 2906 2266 
Q 2828 2175 2409 1742 
Q 1991 1309 1228 531 
z
" id="DejaVuSans-32" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-32">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_3">
          <g id="line2d_3">
           <path clip-path="url(#p5125e43100)" d="M 182.396186 244.078125 
L 182.396186 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_3">
           <!-- 40 -->
           <g style="fill:#262626;" transform="translate(176.033686 258.676562)scale(0.1 -0.1)">
            <defs>
             <path d="M 2419 4116 
L 825 1625 
L 2419 1625 
L 2419 4116 
z
M 2253 4666 
L 3047 4666 
L 3047 1625 
L 3713 1625 
L 3713 1100 
L 3047 1100 
L 3047 0 
L 2419 0 
L 2419 1100 
L 313 1100 
L 313 1709 
L 2253 4666 
z
" id="DejaVuSans-34" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-34">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_4">
          <g id="line2d_4">
           <path clip-path="url(#p5125e43100)" d="M 250.722717 244.078125 
L 250.722717 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_4">
           <!-- 60 -->
           <g style="fill:#262626;" transform="translate(244.360217 258.676562)scale(0.1 -0.1)">
            <defs>
             <path d="M 2113 2584 
Q 1688 2584 1439 2293 
Q 1191 2003 1191 1497 
Q 1191 994 1439 701 
Q 1688 409 2113 409 
Q 2538 409 2786 701 
Q 3034 994 3034 1497 
Q 3034 2003 2786 2293 
Q 2538 2584 2113 2584 
z
M 3366 4563 
L 3366 3988 
Q 3128 4100 2886 4159 
Q 2644 4219 2406 4219 
Q 1781 4219 1451 3797 
Q 1122 3375 1075 2522 
Q 1259 2794 1537 2939 
Q 1816 3084 2150 3084 
Q 2853 3084 3261 2657 
Q 3669 2231 3669 1497 
Q 3669 778 3244 343 
Q 2819 -91 2113 -91 
Q 1303 -91 875 529 
Q 447 1150 447 2328 
Q 447 3434 972 4092 
Q 1497 4750 2381 4750 
Q 2619 4750 2861 4703 
Q 3103 4656 3366 4563 
z
" id="DejaVuSans-36" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-36">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_5">
          <g id="line2d_5">
           <path clip-path="url(#p5125e43100)" d="M 319.049247 244.078125 
L 319.049247 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_5">
           <!-- 80 -->
           <g style="fill:#262626;" transform="translate(312.686747 258.676562)scale(0.1 -0.1)">
            <defs>
             <path d="M 2034 2216 
Q 1584 2216 1326 1975 
Q 1069 1734 1069 1313 
Q 1069 891 1326 650 
Q 1584 409 2034 409 
Q 2484 409 2743 651 
Q 3003 894 3003 1313 
Q 3003 1734 2745 1975 
Q 2488 2216 2034 2216 
z
M 1403 2484 
Q 997 2584 770 2862 
Q 544 3141 544 3541 
Q 544 4100 942 4425 
Q 1341 4750 2034 4750 
Q 2731 4750 3128 4425 
Q 3525 4100 3525 3541 
Q 3525 3141 3298 2862 
Q 3072 2584 2669 2484 
Q 3125 2378 3379 2068 
Q 3634 1759 3634 1313 
Q 3634 634 3220 271 
Q 2806 -91 2034 -91 
Q 1263 -91 848 271 
Q 434 634 434 1313 
Q 434 1759 690 2068 
Q 947 2378 1403 2484 
z
M 1172 3481 
Q 1172 3119 1398 2916 
Q 1625 2713 2034 2713 
Q 2441 2713 2670 2916 
Q 2900 3119 2900 3481 
Q 2900 3844 2670 4047 
Q 2441 4250 2034 4250 
Q 1625 4250 1398 4047 
Q 1172 3844 1172 3481 
z
" id="DejaVuSans-38" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-38">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="text_6">
          <!-- iteration -->
          <g style="fill:#262626;" transform="translate(192.020469 272.354687)scale(0.1 -0.1)">
           <defs>
            <path d="M 603 3500 
L 1178 3500 
L 1178 0 
L 603 0 
L 603 3500 
z
M 603 4863 
L 1178 4863 
L 1178 4134 
L 603 4134 
L 603 4863 
z
" id="DejaVuSans-69" transform="scale(0.015625)">
            </path>
            <path d="M 1172 4494 
L 1172 3500 
L 2356 3500 
L 2356 3053 
L 1172 3053 
L 1172 1153 
Q 1172 725 1289 603 
Q 1406 481 1766 481 
L 2356 481 
L 2356 0 
L 1766 0 
Q 1100 0 847 248 
Q 594 497 594 1153 
L 594 3053 
L 172 3053 
L 172 3500 
L 594 3500 
L 594 4494 
L 1172 4494 
z
" id="DejaVuSans-74" transform="scale(0.015625)">
            </path>
            <path d="M 3597 1894 
L 3597 1613 
L 953 1613 
Q 991 1019 1311 708 
Q 1631 397 2203 397 
Q 2534 397 2845 478 
Q 3156 559 3463 722 
L 3463 178 
Q 3153 47 2828 -22 
Q 2503 -91 2169 -91 
Q 1331 -91 842 396 
Q 353 884 353 1716 
Q 353 2575 817 3079 
Q 1281 3584 2069 3584 
Q 2775 3584 3186 3129 
Q 3597 2675 3597 1894 
z
M 3022 2063 
Q 3016 2534 2758 2815 
Q 2500 3097 2075 3097 
Q 1594 3097 1305 2825 
Q 1016 2553 972 2059 
L 3022 2063 
z
" id="DejaVuSans-65" transform="scale(0.015625)">
            </path>
            <path d="M 2631 2963 
Q 2534 3019 2420 3045 
Q 2306 3072 2169 3072 
Q 1681 3072 1420 2755 
Q 1159 2438 1159 1844 
L 1159 0 
L 581 0 
L 581 3500 
L 1159 3500 
L 1159 2956 
Q 1341 3275 1631 3429 
Q 1922 3584 2338 3584 
Q 2397 3584 2469 3576 
Q 2541 3569 2628 3553 
L 2631 2963 
z
" id="DejaVuSans-72" transform="scale(0.015625)">
            </path>
            <path d="M 2194 1759 
Q 1497 1759 1228 1600 
Q 959 1441 959 1056 
Q 959 750 1161 570 
Q 1363 391 1709 391 
Q 2188 391 2477 730 
Q 2766 1069 2766 1631 
L 2766 1759 
L 2194 1759 
z
M 3341 1997 
L 3341 0 
L 2766 0 
L 2766 531 
Q 2569 213 2275 61 
Q 1981 -91 1556 -91 
Q 1019 -91 701 211 
Q 384 513 384 1019 
Q 384 1609 779 1909 
Q 1175 2209 1959 2209 
L 2766 2209 
L 2766 2266 
Q 2766 2663 2505 2880 
Q 2244 3097 1772 3097 
Q 1472 3097 1187 3025 
Q 903 2953 641 2809 
L 641 3341 
Q 956 3463 1253 3523 
Q 1550 3584 1831 3584 
Q 2591 3584 2966 3190 
Q 3341 2797 3341 1997 
z
" id="DejaVuSans-61" transform="scale(0.015625)">
            </path>
            <path d="M 1959 3097 
Q 1497 3097 1228 2736 
Q 959 2375 959 1747 
Q 959 1119 1226 758 
Q 1494 397 1959 397 
Q 2419 397 2687 759 
Q 2956 1122 2956 1747 
Q 2956 2369 2687 2733 
Q 2419 3097 1959 3097 
z
M 1959 3584 
Q 2709 3584 3137 3096 
Q 3566 2609 3566 1747 
Q 3566 888 3137 398 
Q 2709 -91 1959 -91 
Q 1206 -91 779 398 
Q 353 888 353 1747 
Q 353 2609 779 3096 
Q 1206 3584 1959 3584 
z
" id="DejaVuSans-6f" transform="scale(0.015625)">
            </path>
            <path d="M 3513 2113 
L 3513 0 
L 2938 0 
L 2938 2094 
Q 2938 2591 2744 2837 
Q 2550 3084 2163 3084 
Q 1697 3084 1428 2787 
Q 1159 2491 1159 1978 
L 1159 0 
L 581 0 
L 581 3500 
L 1159 3500 
L 1159 2956 
Q 1366 3272 1645 3428 
Q 1925 3584 2291 3584 
Q 2894 3584 3203 3211 
Q 3513 2838 3513 2113 
z
" id="DejaVuSans-6e" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-69">
           </use>
           <use x="27.783203" xlink:href="#DejaVuSans-74">
           </use>
           <use x="66.992188" xlink:href="#DejaVuSans-65">
           </use>
           <use x="128.515625" xlink:href="#DejaVuSans-72">
           </use>
           <use x="169.628906" xlink:href="#DejaVuSans-61">
           </use>
           <use x="230.908203" xlink:href="#DejaVuSans-74">
           </use>
           <use x="270.117188" xlink:href="#DejaVuSans-69">
           </use>
           <use x="297.900391" xlink:href="#DejaVuSans-6f">
           </use>
           <use x="359.082031" xlink:href="#DejaVuSans-6e">
           </use>
          </g>
         </g>
        </g>
        <g id="matplotlib.axis_2">
         <g id="ytick_1">
          <g id="line2d_6">
           <path clip-path="url(#p5125e43100)" d="M 45.743125 244.078125 
L 380.543125 244.078125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_7">
           <!-- 0.0 -->
           <g style="fill:#262626;" transform="translate(22.84 247.877344)scale(0.1 -0.1)">
            <defs>
             <path d="M 684 794 
L 1344 794 
L 1344 0 
L 684 0 
L 684 794 
z
" id="DejaVuSans-2e" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-30">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_2">
          <g id="line2d_7">
           <path clip-path="url(#p5125e43100)" d="M 45.743125 199.726125 
L 380.543125 199.726125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_8">
           <!-- 0.5 -->
           <g style="fill:#262626;" transform="translate(22.84 203.525344)scale(0.1 -0.1)">
            <defs>
             <path d="M 691 4666 
L 3169 4666 
L 3169 4134 
L 1269 4134 
L 1269 2991 
Q 1406 3038 1543 3061 
Q 1681 3084 1819 3084 
Q 2600 3084 3056 2656 
Q 3513 2228 3513 1497 
Q 3513 744 3044 326 
Q 2575 -91 1722 -91 
Q 1428 -91 1123 -41 
Q 819 9 494 109 
L 494 744 
Q 775 591 1075 516 
Q 1375 441 1709 441 
Q 2250 441 2565 725 
Q 2881 1009 2881 1497 
Q 2881 1984 2565 2268 
Q 2250 2553 1709 2553 
Q 1456 2553 1204 2497 
Q 953 2441 691 2322 
L 691 4666 
z
" id="DejaVuSans-35" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-30">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-35">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_3">
          <g id="line2d_8">
           <path clip-path="url(#p5125e43100)" d="M 45.743125 155.374125 
L 380.543125 155.374125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_9">
           <!-- 1.0 -->
           <g style="fill:#262626;" transform="translate(22.84 159.173344)scale(0.1 -0.1)">
            <defs>
             <path d="M 794 531 
L 1825 531 
L 1825 4091 
L 703 3866 
L 703 4441 
L 1819 4666 
L 2450 4666 
L 2450 531 
L 3481 531 
L 3481 0 
L 794 0 
L 794 531 
z
" id="DejaVuSans-31" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-31">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_4">
          <g id="line2d_9">
           <path clip-path="url(#p5125e43100)" d="M 45.743125 111.022125 
L 380.543125 111.022125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_10">
           <!-- 1.5 -->
           <g style="fill:#262626;" transform="translate(22.84 114.821344)scale(0.1 -0.1)">
            <use xlink:href="#DejaVuSans-31">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-35">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_5">
          <g id="line2d_10">
           <path clip-path="url(#p5125e43100)" d="M 45.743125 66.670125 
L 380.543125 66.670125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_11">
           <!-- 2.0 -->
           <g style="fill:#262626;" transform="translate(22.84 70.469344)scale(0.1 -0.1)">
            <use xlink:href="#DejaVuSans-32">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_6">
          <g id="line2d_11">
           <path clip-path="url(#p5125e43100)" d="M 45.743125 22.318125 
L 380.543125 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_12">
           <!-- 2.5 -->
           <g style="fill:#262626;" transform="translate(22.84 26.117344)scale(0.1 -0.1)">
            <use xlink:href="#DejaVuSans-32">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-35">
            </use>
           </g>
          </g>
         </g>
         <g id="text_13">
          <!-- $\xi$ -->
          <g style="fill:#262626;" transform="translate(16.32 136.558125)rotate(-90)scale(0.12 -0.12)">
           <defs>
            <path d="M 2016 397 
Q 2428 394 2628 159 
Q 2844 -88 2772 -463 
Q 2700 -822 2422 -1072 
Q 2119 -1344 1609 -1344 
Q 1656 -1109 1700 -872 
Q 1913 -888 2072 -750 
Q 2194 -641 2216 -525 
Q 2247 -359 2175 -222 
Q 2100 -91 1922 -91 
Q -25 -91 241 1275 
Q 422 2213 1516 2488 
Q 663 2600 822 3413 
Q 941 4028 1678 4284 
L 1028 4284 
L 1141 4863 
L 3606 4863 
L 3494 4284 
Q 1528 4284 1350 3375 
Q 1234 2778 2881 2750 
L 2778 2219 
Q 1009 2288 819 1275 
Q 659 428 2016 397 
z
" id="DejaVuSans-Oblique-3be" transform="scale(0.015625)">
            </path>
           </defs>
           <use transform="translate(0 0.015625)" xlink:href="#DejaVuSans-Oblique-3be">
           </use>
          </g>
         </g>
        </g>
        <g id="line2d_12">
         <path clip-path="url(#p5125e43100)" d="M 45.743125 39.586768 
L 45.902062 41.280173 
L 45.98153 37.462112 
L 46.060998 42.500023 
L 46.140467 39.837003 
L 46.219935 42.144527 
L 46.299403 41.099213 
L 46.378871 38.122366 
L 46.45834 38.571267 
L 46.617276 40.514282 
L 46.696745 40.858996 
L 46.776213 42.180779 
L 46.93515 36.893969 
L 47.014618 40.45894 
L 47.094086 38.369415 
L 47.173555 38.728328 
L 47.253023 39.018242 
L 47.332491 42.420881 
L 47.41196 41.261302 
L 47.491428 41.345327 
L 47.570896 43.203026 
L 47.650364 41.12919 
L 47.729833 42.32585 
L 47.809301 36.285489 
L 47.968238 40.889183 
L 48.047706 37.039777 
L 48.286111 42.771922 
L 48.365579 43.46824 
L 48.445048 42.988426 
L 48.524516 39.629378 
L 48.603984 42.505548 
L 48.683453 37.68563 
L 48.762921 41.587362 
L 48.842389 40.860409 
L 48.921857 38.38006 
L 49.001326 39.335456 
L 49.080794 39.669905 
L 49.319199 41.705073 
L 49.398667 40.938695 
L 49.478136 42.528905 
L 49.557604 39.69888 
L 49.637072 42.955158 
L 49.716541 42.766991 
L 49.796009 39.969746 
L 49.875477 41.371954 
L 49.954946 40.680175 
L 50.034414 42.738165 
L 50.113882 41.39748 
L 50.19335 43.148549 
L 50.352287 39.353586 
L 50.431755 42.593712 
L 50.511224 39.903527 
L 50.749629 43.770743 
L 50.908565 42.426993 
L 50.988034 44.207477 
L 51.067502 42.789832 
L 51.14697 43.409372 
L 51.226439 37.784464 
L 51.385375 42.659868 
L 51.464843 38.85659 
L 51.703248 45.093335 
L 51.782717 45.3971 
L 51.862185 44.809036 
L 51.941653 41.298736 
L 52.021122 44.281557 
L 52.10059 39.209665 
L 52.180058 43.509073 
L 52.259527 42.327568 
L 52.338995 39.653643 
L 52.418463 41.294629 
L 52.497932 41.242084 
L 52.656868 43.304291 
L 52.736336 43.062236 
L 52.815805 42.372317 
L 52.895273 43.893842 
L 52.974741 41.59993 
L 53.133678 44.570267 
L 53.213146 42.184717 
L 53.292615 42.875512 
L 53.372083 42.212918 
L 53.451551 44.732042 
L 53.53102 43.004172 
L 53.610488 45.12663 
L 53.769425 41.236909 
L 53.848893 44.12872 
L 53.928361 41.747795 
L 54.166766 45.730165 
L 54.325703 44.170704 
L 54.405171 46.236782 
L 54.643576 40.027818 
L 54.802513 44.944841 
L 54.881981 40.663992 
L 55.120386 47.402598 
L 55.199854 47.672678 
L 55.279322 46.6922 
L 55.358791 43.694793 
L 55.438259 46.665629 
L 55.517727 41.20596 
L 55.597196 45.950687 
L 55.676664 44.634899 
L 55.756132 41.623756 
L 55.835601 44.284017 
L 55.915069 43.607165 
L 55.994537 43.951463 
L 56.074006 45.913322 
L 56.153474 45.135508 
L 56.232942 44.308129 
L 56.312411 45.906414 
L 56.391879 44.43653 
L 56.550815 47.118661 
L 56.78922 44.611035 
L 56.868689 47.520974 
L 56.948157 45.522153 
L 57.027625 48.101956 
L 57.186562 44.102349 
L 57.26603 46.258119 
L 57.345499 44.498694 
L 57.583904 48.600849 
L 57.663372 48.329831 
L 57.74284 46.790154 
L 57.822308 49.388199 
L 58.060713 43.360456 
L 58.21965 48.287027 
L 58.299118 43.334212 
L 58.537523 50.837372 
L 58.616992 51.171413 
L 58.775928 47.332797 
L 58.855397 50.302939 
L 58.934865 44.254836 
L 59.014333 49.720557 
L 59.093801 48.058435 
L 59.17327 44.592229 
L 59.252738 49.005296 
L 59.411675 46.963732 
L 59.491143 50.012005 
L 59.65008 47.271251 
L 59.888485 50.397037 
L 59.967953 51.060489 
L 60.047421 50.965031 
L 60.12689 48.328333 
L 60.206358 48.340919 
L 60.285826 51.755034 
L 60.365294 49.471688 
L 60.444763 52.71935 
L 60.524231 48.502099 
L 60.603699 48.65267 
L 60.683168 49.465753 
L 60.762636 48.575428 
L 61.001041 52.979426 
L 61.080509 53.064732 
L 61.159978 50.849715 
L 61.239446 54.191428 
L 61.477851 48.425123 
L 61.636787 53.511676 
L 61.716256 47.514126 
L 61.954661 56.109569 
L 62.034129 56.660852 
L 62.193066 52.815073 
L 62.272534 55.837374 
L 62.352002 49.008976 
L 62.431471 55.595278 
L 62.510939 53.134795 
L 62.590407 49.201736 
L 62.669876 56.343898 
L 62.828812 51.585432 
L 62.90828 56.391845 
L 63.067217 51.727764 
L 63.464559 59.130686 
L 63.544027 53.301845 
L 63.623495 54.019468 
L 63.702964 57.948982 
L 63.782432 55.602397 
L 63.8619 59.686502 
L 63.941369 54.316711 
L 64.020837 55.44891 
L 64.100305 54.300246 
L 64.179773 54.450253 
L 64.33871 59.058352 
L 64.497647 60.191346 
L 64.577115 57.005558 
L 64.656583 61.188826 
L 64.894988 55.59896 
L 65.053925 61.46069 
L 65.133393 53.87676 
L 65.371798 63.808804 
L 65.451266 64.48288 
L 65.530735 60.187807 
L 65.610203 60.578297 
L 65.689671 63.647227 
L 65.76914 55.789215 
L 65.848608 63.823345 
L 66.007545 55.825078 
L 66.087013 66.509977 
L 66.24595 58.071841 
L 66.325418 65.161231 
L 66.484355 58.216197 
L 66.881696 70.051888 
L 66.961164 60.261426 
L 67.040633 61.743633 
L 67.120101 66.215356 
L 67.199569 64.198094 
L 67.279038 69.364997 
L 67.358506 62.553023 
L 67.437974 64.436446 
L 67.517443 61.132244 
L 67.596911 62.239528 
L 67.755848 68.62944 
L 67.835316 67.742763 
L 67.914784 69.833018 
L 67.994252 65.217137 
L 68.073721 70.148375 
L 68.312126 64.77571 
L 68.391594 66.095547 
L 68.471062 71.928968 
L 68.550531 62.542427 
L 68.788936 73.969284 
L 68.868404 74.417255 
L 68.947872 68.495467 
L 69.027341 70.457832 
L 69.106809 73.507196 
L 69.186277 64.382164 
L 69.265745 73.950679 
L 69.424682 64.480615 
L 69.50415 78.602314 
L 69.663087 66.225962 
L 69.742555 76.024535 
L 69.901492 66.701244 
L 69.98096 68.117299 
L 70.060429 75.3486 
L 70.139897 72.960253 
L 70.219365 75.697872 
L 70.298834 82.634669 
L 70.378302 68.984696 
L 70.45777 71.315803 
L 70.696175 81.037884 
L 70.775643 72.884341 
L 70.855112 74.750006 
L 70.93458 69.876138 
L 71.014048 71.528784 
L 71.172985 79.819627 
L 71.252453 77.467032 
L 71.331922 81.384243 
L 71.41139 74.962746 
L 71.490858 80.406169 
L 71.570327 78.673728 
L 71.729263 75.12053 
L 71.808731 76.738821 
L 71.8882 84.045547 
L 71.967668 73.227562 
L 72.206073 85.867294 
L 72.285541 85.748498 
L 72.36501 78.223543 
L 72.523946 84.647825 
L 72.603415 73.953549 
L 72.682883 85.199458 
L 72.84182 74.608919 
L 72.921288 91.178831 
L 73.080224 75.599012 
L 73.159693 88.004103 
L 73.318629 76.707374 
L 73.398098 77.102677 
L 73.477566 88.014903 
L 73.557034 84.518564 
L 73.636503 87.13135 
L 73.715971 95.708835 
L 73.795439 78.839445 
L 73.874908 82.182153 
L 74.113313 93.531042 
L 74.192781 84.488717 
L 74.272249 85.562789 
L 74.351717 80.139644 
L 74.431186 81.856916 
L 74.590122 91.747114 
L 74.669591 88.230291 
L 74.749059 93.957936 
L 74.828527 85.682413 
L 74.907996 91.392295 
L 74.987464 90.490331 
L 75.146401 85.881515 
L 75.225869 89.1113 
L 75.305337 96.917719 
L 75.384806 85.435627 
L 75.62321 98.633601 
L 75.702679 97.705882 
L 75.782147 88.91276 
L 75.941084 96.488942 
L 76.020552 83.894115 
L 76.10002 97.065779 
L 76.258957 85.667154 
L 76.338425 103.646439 
L 76.497362 85.712875 
L 76.57683 100.351609 
L 76.735767 87.664665 
L 76.815235 86.974145 
L 76.894703 101.585178 
L 76.974172 97.3312 
L 77.05364 99.248707 
L 77.133108 108.875013 
L 77.212577 89.503049 
L 77.292045 93.875021 
L 77.371513 99.607172 
L 77.450982 96.758426 
L 77.53045 106.139836 
L 77.609918 96.501177 
L 77.689387 96.581427 
L 77.768855 91.610394 
L 77.848323 93.068066 
L 78.00726 104.050436 
L 78.086728 99.9013 
L 78.166196 106.991272 
L 78.245665 97.132674 
L 78.325133 102.93956 
L 78.404601 102.918914 
L 78.563538 96.98159 
L 78.722475 110.115985 
L 78.801943 98.670857 
L 78.881411 102.798274 
L 79.040348 111.97668 
L 79.119816 109.903157 
L 79.199285 100.133321 
L 79.358221 108.851227 
L 79.437689 94.229587 
L 79.517158 109.426824 
L 79.676094 97.440778 
L 79.755563 115.909246 
L 79.914499 96.294148 
L 79.993968 112.831773 
L 80.152904 99.231549 
L 80.232373 97.590998 
L 80.311841 115.703961 
L 80.391309 110.90495 
L 80.470778 111.74638 
L 80.550246 121.678639 
L 80.629714 101.063737 
L 80.709182 106.376118 
L 80.788651 112.616183 
L 80.868119 107.918238 
L 80.947587 118.592019 
L 81.106524 107.655929 
L 81.185992 103.97063 
L 81.265461 105.008462 
L 81.424397 116.625449 
L 81.503866 112.156051 
L 81.583334 120.037169 
L 81.662802 109.085505 
L 81.821739 115.71843 
L 81.980675 108.37573 
L 82.139612 123.282698 
L 82.21908 112.250189 
L 82.298549 115.216979 
L 82.457485 125.542466 
L 82.536954 122.038209 
L 82.616422 111.229066 
L 82.775359 121.655316 
L 82.854827 104.594401 
L 82.934295 121.70149 
L 83.013763 109.434631 
L 83.093232 109.69202 
L 83.1727 127.833083 
L 83.331637 107.478477 
L 83.411105 125.364055 
L 83.570042 111.165716 
L 83.64951 108.75442 
L 83.728978 129.653388 
L 83.808447 124.723083 
L 83.887915 124.5187 
L 83.967383 133.790136 
L 84.046852 112.879708 
L 84.12632 119.47853 
L 84.205788 125.493567 
L 84.285256 119.328 
L 84.364725 130.571305 
L 84.523661 118.716981 
L 84.60313 116.746132 
L 84.682598 117.317758 
L 84.841535 129.271718 
L 84.921003 124.47549 
L 85.000471 132.776664 
L 85.07994 121.50529 
L 85.238876 128.367475 
L 85.397813 119.734057 
L 85.556749 136.122193 
L 85.636218 125.587562 
L 85.715686 127.231511 
L 85.874623 138.623303 
L 85.954091 133.638763 
L 86.033559 121.647132 
L 86.192496 134.451567 
L 86.271964 114.623482 
L 86.351433 133.529793 
L 86.430901 121.272897 
L 86.510369 121.995631 
L 86.589838 139.371162 
L 86.748774 119.164698 
L 86.828242 137.600346 
L 86.907711 123.161667 
L 86.987179 123.204621 
L 87.066647 120.191232 
L 87.146116 142.582027 
L 87.225584 138.336997 
L 87.305052 137.108082 
L 87.384521 144.981687 
L 87.463989 124.406316 
L 87.543457 132.340276 
L 87.622926 137.958585 
L 87.702394 130.923709 
L 87.781862 142.165909 
L 87.940799 129.504472 
L 88.020267 129.484531 
L 88.099735 129.342695 
L 88.258672 141.476967 
L 88.33814 136.285893 
L 88.417609 144.900598 
L 88.497077 134.086212 
L 88.576545 137.643394 
L 88.656014 140.533208 
L 88.81495 130.508918 
L 88.973887 148.336511 
L 89.053355 138.125035 
L 89.132824 138.355328 
L 89.29176 150.661823 
L 89.450697 131.270002 
L 89.609633 146.710444 
L 89.689102 124.263683 
L 89.76857 144.717493 
L 89.848038 132.487725 
L 89.927507 133.908383 
L 90.006975 150.271756 
L 90.086443 130.580553 
L 90.165912 130.925308 
L 90.24538 149.081177 
L 90.324848 134.683058 
L 90.404317 134.997395 
L 90.483785 131.415633 
L 90.563253 154.225302 
L 90.642721 150.979433 
L 90.72219 149.113282 
L 90.801658 155.180429 
L 90.881126 135.282743 
L 91.040063 149.40117 
L 91.119531 141.941746 
L 91.199 153.106646 
L 91.357936 139.651353 
L 91.437405 141.684497 
L 91.516873 140.86327 
L 91.67581 152.61137 
L 91.755278 147.132741 
L 91.834746 156.038549 
L 91.914214 146.12424 
L 91.993683 148.468493 
L 92.073151 151.781957 
L 92.232088 140.383677 
L 92.391024 159.746465 
L 92.549961 147.985365 
L 92.708898 161.501922 
L 92.867834 140.023758 
L 93.026771 157.979023 
L 93.106239 133.035019 
L 93.185707 155.077687 
L 93.265176 142.801584 
L 93.344644 145.094386 
L 93.424112 160.050953 
L 93.503581 141.362696 
L 93.583049 141.495767 
L 93.662517 159.375376 
L 93.741986 145.517547 
L 93.821454 145.938974 
L 93.900922 141.896939 
L 93.980391 164.628679 
L 94.059859 161.925873 
L 94.139327 160.10457 
L 94.218796 164.182237 
L 94.298264 145.310823 
L 94.4572 159.462257 
L 94.536669 151.543808 
L 94.616137 163.200274 
L 94.775074 149.254829 
L 95.092947 162.326962 
L 95.172415 156.766919 
L 95.251884 166.089644 
L 95.331352 157.250474 
L 95.41082 158.408996 
L 95.490289 161.784503 
L 95.649225 149.259436 
L 95.808162 169.922906 
L 95.967098 156.340781 
L 96.126035 171.29063 
L 96.284972 148.123989 
L 96.443908 167.908605 
L 96.523377 140.813539 
L 96.602845 164.296216 
L 96.682313 152.121726 
L 96.761782 155.349592 
L 96.84125 168.802718 
L 97.000186 151.017809 
L 97.079655 168.386531 
L 97.159123 155.350346 
L 97.238591 155.776666 
L 97.31806 151.845756 
L 97.397528 173.681024 
L 97.476996 171.578379 
L 97.556465 169.845118 
L 97.635933 171.870548 
L 97.715401 154.029421 
L 97.874338 168.645331 
L 97.953806 159.902048 
L 98.033275 171.954759 
L 98.192211 158.345755 
L 98.271679 163.595031 
L 98.351148 161.413539 
L 98.430616 171.496661 
L 98.510084 170.616029 
L 98.589553 165.180561 
L 98.669021 175.082107 
L 98.748489 167.029963 
L 98.827958 167.436696 
L 98.907426 170.523401 
L 99.066363 157.000882 
L 99.145831 178.867903 
L 99.225299 178.450783 
L 99.384236 164.009956 
L 99.543172 179.949276 
L 99.702109 155.327118 
L 99.861046 176.564192 
L 99.940514 147.812709 
L 100.019982 172.393476 
L 100.099451 160.526438 
L 100.178919 164.176969 
L 100.258387 176.727634 
L 100.417324 159.879475 
L 100.496792 176.290396 
L 100.576261 163.874065 
L 100.655729 164.969039 
L 100.735197 160.795511 
L 100.814665 181.415317 
L 100.894134 180.239065 
L 100.973602 178.357729 
L 101.05307 178.431971 
L 101.132539 161.344614 
L 101.291475 176.934605 
L 101.370944 167.32181 
L 101.450412 179.439966 
L 101.609349 166.858934 
L 101.688817 173.226478 
L 101.768285 170.526041 
L 101.847754 179.602059 
L 101.927222 177.866596 
L 102.00669 172.619312 
L 102.086158 182.930044 
L 102.165627 175.187348 
L 102.245095 175.578498 
L 102.324563 178.151736 
L 102.4835 164.176358 
L 102.562968 186.77125 
L 102.642437 185.550093 
L 102.801373 171.505145 
L 102.96031 187.466317 
L 103.039778 180.484509 
L 103.119247 161.89141 
L 103.278183 184.121308 
L 103.357651 154.572122 
L 103.43712 179.518407 
L 103.516588 168.134186 
L 103.596056 171.715965 
L 103.675525 183.798035 
L 103.754993 167.731424 
L 103.834461 168.227481 
L 103.91393 183.310411 
L 103.993398 171.443978 
L 104.072866 173.320141 
L 104.152335 168.692963 
L 104.231803 188.023566 
L 104.311271 187.672779 
L 104.470208 184.224137 
L 104.549676 167.501476 
L 104.708613 184.147593 
L 104.788081 174.057172 
L 104.867549 185.808708 
L 105.026486 174.691833 
L 105.105954 181.676925 
L 105.185423 178.819006 
L 105.264891 186.749858 
L 105.344359 184.336708 
L 105.423828 179.062414 
L 105.503296 189.707575 
L 105.582764 182.174429 
L 105.662233 183.014485 
L 105.741701 184.973099 
L 105.900637 171.102914 
L 105.980106 193.320238 
L 106.059574 191.59671 
L 106.218511 178.456242 
L 106.377447 193.813317 
L 106.456916 187.681795 
L 106.536384 167.78519 
L 106.695321 190.666366 
L 106.774789 161.213062 
L 106.854257 185.784367 
L 106.933726 174.978311 
L 107.013194 178.293194 
L 107.092662 190.241623 
L 107.17213 174.488654 
L 107.251599 176.114283 
L 107.331067 189.684275 
L 107.410535 178.315489 
L 107.490004 180.565497 
L 107.569472 175.779887 
L 107.728409 194.057336 
L 107.887345 189.265837 
L 107.966814 172.717059 
L 108.12575 190.335756 
L 108.205219 180.075599 
L 108.284687 191.27942 
L 108.364155 181.303733 
L 108.443623 181.624451 
L 108.523092 189.152167 
L 108.60256 186.169377 
L 108.682028 193.149785 
L 108.840965 184.717683 
L 108.920433 195.538116 
L 108.999902 188.393731 
L 109.07937 189.74244 
L 109.158838 191.000641 
L 109.317775 178.03237 
L 109.397243 198.608723 
L 109.476712 196.831759 
L 109.635648 184.625028 
L 109.794585 199.166496 
L 109.874053 193.754317 
L 109.953521 172.861657 
L 110.112458 196.226537 
L 110.191926 167.359646 
L 110.271395 191.416869 
L 110.350863 181.216236 
L 110.430331 184.038236 
L 110.5098 196.285894 
L 110.589268 180.85062 
L 110.668736 183.153276 
L 110.748205 195.282283 
L 110.827673 184.542684 
L 110.907141 186.875611 
L 110.986609 182.112356 
L 111.145546 199.521977 
L 111.225014 197.868409 
L 111.304483 193.78239 
L 111.383951 177.0041 
L 111.542888 196.014598 
L 111.622356 185.678535 
L 111.701824 195.987373 
L 111.781293 185.830983 
L 111.860761 188.12119 
L 111.940229 195.533555 
L 112.019698 192.453755 
L 112.099166 198.785595 
L 112.258102 189.826779 
L 112.337571 200.581779 
L 112.417039 193.989465 
L 112.496507 195.629686 
L 112.575976 196.354307 
L 112.734912 185.25209 
L 112.814381 203.140482 
L 112.893849 201.236448 
L 113.052786 190.041845 
L 113.211722 203.55177 
L 113.291191 198.955395 
L 113.370659 177.678099 
L 113.529595 200.957171 
L 113.609064 173.099092 
L 113.688532 196.660613 
L 113.768 186.816568 
L 113.847469 189.131926 
L 113.926937 201.783851 
L 114.006405 187.03665 
L 114.085874 189.295308 
L 114.165342 200.334905 
L 114.24481 190.094793 
L 114.324279 192.278538 
L 114.403747 187.56887 
L 114.562684 204.268552 
L 114.642152 202.885131 
L 114.72162 197.929132 
L 114.801088 181.221385 
L 114.960025 201.461748 
L 115.039493 190.690247 
L 115.118962 200.242724 
L 115.19843 190.675815 
L 115.277898 193.412891 
L 115.357367 200.920333 
L 115.436835 198.135011 
L 115.516303 203.652463 
L 115.67524 194.95629 
L 115.754708 204.867422 
L 115.834177 198.92701 
L 115.913645 200.676111 
L 115.993113 201.189431 
L 116.15205 191.354333 
L 116.231518 207.425591 
L 116.310986 204.961688 
L 116.469923 195.070662 
L 116.549391 207.764718 
L 116.62886 207.050653 
L 116.708328 203.57472 
L 116.787796 182.628365 
L 116.867265 205.100123 
L 116.946733 205.066719 
L 117.026201 178.574981 
L 117.10567 201.491679 
L 117.185138 191.501866 
L 117.264606 193.647017 
L 117.344074 206.642831 
L 117.423543 192.609886 
L 117.503011 194.930999 
L 117.582479 205.052958 
L 117.661948 194.380614 
L 117.741416 196.621192 
L 117.820884 192.27468 
L 117.979821 208.24075 
L 118.059289 207.337801 
L 118.138758 201.587679 
L 118.218226 185.799183 
L 118.377163 206.186832 
L 118.456631 195.263431 
L 118.536099 204.127548 
L 118.615567 195.161503 
L 118.695036 197.838988 
L 118.774504 205.703061 
L 118.853972 202.848011 
L 118.933441 207.790909 
L 119.092377 199.660405 
L 119.171846 208.874678 
L 119.251314 203.3172 
L 119.330782 205.084748 
L 119.410251 205.496687 
L 119.569187 196.142918 
L 119.648656 210.994267 
L 119.728124 208.382691 
L 119.88706 199.708061 
L 119.966529 211.585409 
L 120.045997 210.176754 
L 120.125465 207.429553 
L 120.204934 187.07552 
L 120.284402 209.294216 
L 120.36387 208.66275 
L 120.443339 183.56853 
L 120.522807 205.767051 
L 120.602275 195.367448 
L 120.681744 197.594861 
L 120.761212 210.873917 
L 120.84068 197.240806 
L 120.920148 199.695603 
L 120.999617 209.144222 
L 121.079085 198.153298 
L 121.158553 200.266473 
L 121.238022 196.223268 
L 121.396958 211.523048 
L 121.476427 211.174793 
L 121.635363 190.691803 
L 121.7943 209.928096 
L 121.873768 199.618703 
L 121.953237 207.628111 
L 122.032705 198.75333 
L 122.112173 202.050661 
L 122.191641 209.80273 
L 122.27111 206.726951 
L 122.350578 211.337054 
L 122.509515 203.574203 
L 122.588983 212.400122 
L 122.668451 206.981069 
L 122.74792 208.973411 
L 122.827388 209.316191 
L 122.986325 200.219082 
L 123.065793 213.988363 
L 123.145261 211.518639 
L 123.304198 203.647488 
L 123.383666 214.916651 
L 123.463134 213.245755 
L 123.542603 210.639588 
L 123.622071 191.147274 
L 123.701539 213.060852 
L 123.781008 211.736464 
L 123.860476 187.809515 
L 123.939944 209.513473 
L 124.019413 198.830024 
L 124.098881 201.138769 
L 124.178349 214.453906 
L 124.257818 201.000672 
L 124.337286 203.673607 
L 124.416754 212.628483 
L 124.496223 201.448664 
L 124.575691 203.442441 
L 124.655159 199.451053 
L 124.814096 214.223366 
L 124.893564 214.407705 
L 125.052501 195.690709 
L 125.211437 212.983009 
L 125.290906 203.369732 
L 125.370374 210.528737 
L 125.449842 201.291266 
L 125.767716 214.390853 
L 125.926652 206.614743 
L 126.00612 215.188105 
L 126.085589 209.933054 
L 126.244525 212.676291 
L 126.403462 203.810259 
L 126.48293 216.612529 
L 126.562399 214.313682 
L 126.721335 206.960883 
L 126.800804 217.722013 
L 126.880272 216.129873 
L 126.95974 213.419456 
L 127.039209 194.758995 
L 127.118677 216.449072 
L 127.198145 214.385021 
L 127.277613 191.668986 
L 127.357082 212.483639 
L 127.43655 202.185214 
L 127.516018 204.544512 
L 127.595487 217.474796 
L 127.674955 204.397416 
L 127.754423 207.316023 
L 127.833892 215.627014 
L 127.91336 204.513739 
L 127.992828 206.163758 
L 128.072297 202.434385 
L 128.231233 216.579687 
L 128.310702 216.959076 
L 128.469638 200.657889 
L 128.628575 215.466172 
L 128.708043 206.549509 
L 128.787511 212.804572 
L 128.86698 203.69622 
L 129.025916 216.369344 
L 129.105385 212.903704 
L 129.184853 217.107443 
L 129.34379 209.113222 
L 129.423258 217.360809 
L 129.502726 212.487993 
L 129.661663 215.684714 
L 129.820599 206.815854 
L 129.900068 219.006605 
L 129.979536 216.796761 
L 130.138473 209.929608 
L 130.217941 220.024233 
L 130.297409 218.72056 
L 130.376878 215.918683 
L 130.456346 198.09306 
L 130.535814 219.263009 
L 130.615283 216.749444 
L 130.694751 195.309966 
L 130.774219 214.736278 
L 130.853688 205.457757 
L 130.933156 207.792788 
L 131.012624 219.989815 
L 131.092092 207.558681 
L 131.171561 211.028757 
L 131.251029 218.300305 
L 131.330497 207.773838 
L 131.409966 208.46647 
L 131.489434 205.344579 
L 131.648371 218.857088 
L 131.727839 218.966841 
L 131.886776 205.377431 
L 132.045712 217.649178 
L 132.125181 209.572794 
L 132.204649 214.873364 
L 132.284117 206.000074 
L 132.443054 219.187293 
L 132.522522 215.72844 
L 132.60199 219.39737 
L 132.760927 211.395191 
L 132.840395 219.004484 
L 132.919864 214.701515 
L 133.0788 218.347979 
L 133.237737 209.19147 
L 133.317205 221.090311 
L 133.396674 218.967624 
L 133.55561 212.423034 
L 133.635078 222.078629 
L 133.714547 220.722775 
L 133.794015 218.225623 
L 133.873483 201.113508 
L 133.952952 221.542086 
L 134.03242 218.904366 
L 134.111888 198.762008 
L 134.191357 216.540558 
L 134.270825 208.27434 
L 134.350293 210.949493 
L 134.429762 222.170764 
L 134.50923 210.020179 
L 134.588698 214.124092 
L 134.668167 220.593649 
L 134.827103 210.82984 
L 134.906571 208.27443 
L 134.98604 220.930463 
L 135.065508 220.876433 
L 135.144976 220.751528 
L 135.303913 209.459731 
L 135.383381 220.401973 
L 135.46285 219.898536 
L 135.542318 212.147711 
L 135.621786 217.809405 
L 135.701255 208.18773 
L 135.860191 221.830437 
L 135.93966 218.34924 
L 136.019128 221.228207 
L 136.178064 213.722705 
L 136.257533 220.42222 
L 136.337001 216.597993 
L 136.495938 220.605556 
L 136.575406 217.980926 
L 136.654874 211.162075 
L 136.734343 222.952138 
L 136.813811 220.864122 
L 136.972748 214.418401 
L 137.052216 224.051804 
L 137.131684 222.381125 
L 137.211153 220.270359 
L 137.290621 203.747602 
L 137.370089 223.489407 
L 137.449557 220.738703 
L 137.529026 202.016272 
L 137.608494 218.155836 
L 137.687962 210.54232 
L 137.767431 213.814377 
L 137.846899 224.065297 
L 137.926367 211.966935 
L 138.005836 216.719072 
L 138.085304 222.606456 
L 138.244241 213.089026 
L 138.323709 211.167037 
L 138.403177 222.815474 
L 138.482646 222.598206 
L 138.562114 222.431342 
L 138.72105 212.479103 
L 138.800519 222.714107 
L 138.879987 221.620418 
L 138.959455 214.394663 
L 139.038924 220.177488 
L 139.118392 210.004931 
L 139.277329 224.136003 
L 139.356797 220.302459 
L 139.436265 222.711224 
L 139.595202 215.844751 
L 139.67467 221.924004 
L 139.754139 218.244609 
L 139.913075 222.462889 
L 139.992543 220.123628 
L 140.072012 212.930989 
L 140.15148 224.663599 
L 140.230948 222.602504 
L 140.389885 216.428324 
L 140.469353 225.915997 
L 140.548822 223.760372 
L 140.62829 222.067182 
L 140.707758 206.063378 
L 140.787227 225.176729 
L 140.866695 222.236889 
L 140.946163 205.000996 
L 141.025632 219.767523 
L 141.1051 212.526323 
L 141.184568 216.1564 
L 141.264036 225.74258 
L 141.343505 213.578706 
L 141.502441 224.250223 
L 141.661378 215.089598 
L 141.740846 213.615143 
L 141.820315 224.563525 
L 141.899783 224.200022 
L 141.979251 224.025867 
L 142.138188 214.606656 
L 142.217656 224.399522 
L 142.297125 223.039023 
L 142.376593 216.287586 
L 142.456061 221.689276 
L 142.535529 211.852 
L 142.694466 225.882982 
L 142.773934 221.542192 
L 142.853403 224.097973 
L 143.012339 217.648135 
L 143.091808 223.496238 
L 143.171276 219.64812 
L 143.330213 224.082393 
L 143.409681 221.76717 
L 143.489149 214.472893 
L 143.568618 226.218889 
L 143.648086 224.401258 
L 143.807022 218.242575 
L 143.886491 227.592136 
L 143.965959 225.0396 
L 144.045427 223.55268 
L 144.124896 208.159095 
L 144.204364 226.599709 
L 144.283832 223.51256 
L 144.363301 207.617045 
L 144.442769 221.331529 
L 144.522237 214.442041 
L 144.601706 218.119198 
L 144.681174 227.242873 
L 144.760642 215.098433 
L 144.919579 225.608443 
L 145.078515 216.998091 
L 145.157984 215.675209 
L 145.237452 226.122229 
L 145.31692 225.638876 
L 145.396389 225.399366 
L 145.555325 216.216889 
L 145.634794 225.793124 
L 145.714262 224.453741 
L 145.79373 217.920689 
L 145.873199 222.658401 
L 145.952667 214.031613 
L 146.032135 216.584143 
L 146.111604 227.449346 
L 146.191072 223.971371 
L 146.27054 225.558494 
L 146.429477 219.574587 
L 146.508945 225.223636 
L 146.588413 220.788121 
L 146.74735 225.526196 
L 146.826818 223.181999 
L 146.906287 215.691815 
L 146.985755 227.569984 
L 147.065223 225.934345 
L 147.22416 219.821092 
L 147.303628 229.021822 
L 147.383097 226.183946 
L 147.462565 224.844786 
L 147.542033 209.988664 
L 147.621501 227.853971 
L 147.70097 224.650472 
L 147.780438 209.93849 
L 147.859906 222.620735 
L 147.939375 216.212471 
L 148.018843 219.787564 
L 148.098311 228.558216 
L 148.17778 216.624606 
L 148.336716 226.680416 
L 148.416185 218.639927 
L 148.495653 218.725233 
L 148.575121 217.608397 
L 148.65459 227.506471 
L 148.734058 226.918688 
L 148.813526 226.560396 
L 148.972463 217.553945 
L 149.051931 226.939953 
L 149.131399 225.798287 
L 149.210868 219.368608 
L 149.290336 223.483002 
L 149.369804 216.662447 
L 149.449273 218.869395 
L 149.528741 228.374219 
L 149.608209 225.702755 
L 149.687678 226.746052 
L 149.846614 221.593776 
L 149.926083 227.014972 
L 150.005551 221.925922 
L 150.164487 226.925467 
L 150.243956 224.741257 
L 150.323424 216.847441 
L 150.402892 228.790663 
L 150.482361 227.134634 
L 150.641297 221.367955 
L 150.720766 230.187497 
L 150.800234 227.081588 
L 150.879702 225.997166 
L 150.959171 211.646692 
L 151.038639 228.946989 
L 151.118107 225.671849 
L 151.197576 212.051506 
L 151.277044 223.611247 
L 151.356512 217.84439 
L 151.43598 221.258827 
L 151.515449 229.692737 
L 151.594917 218.102909 
L 151.753854 227.458352 
L 151.833322 220.085821 
L 151.91279 220.276317 
L 151.992259 219.495221 
L 152.071727 228.675601 
L 152.151195 228.048663 
L 152.230664 227.590957 
L 152.3896 218.684622 
L 152.469069 227.915796 
L 152.548537 227.01465 
L 152.628005 220.750566 
L 152.707473 224.235908 
L 152.786942 219.096287 
L 152.945878 229.086804 
L 153.025347 227.066761 
L 153.104815 227.764526 
L 153.263752 223.24175 
L 153.34322 228.739114 
L 153.422688 223.256864 
L 153.581625 228.160144 
L 153.661093 226.191888 
L 153.740562 217.898599 
L 153.82003 229.931308 
L 153.899498 228.158031 
L 154.058435 222.949648 
L 154.137903 231.144648 
L 154.217371 227.896053 
L 154.29684 227.009349 
L 154.376308 213.158311 
L 154.455776 229.895904 
L 154.535245 226.564238 
L 154.614713 213.942305 
L 154.694181 224.421985 
L 154.77365 219.518634 
L 154.853118 222.660818 
L 154.932586 230.691751 
L 155.012055 219.510328 
L 155.170991 228.177294 
L 155.250459 221.418177 
L 155.329928 221.585408 
L 155.409396 221.112708 
L 155.488864 229.640902 
L 155.568333 229.014735 
L 155.647801 228.586858 
L 155.806738 219.599178 
L 155.886206 228.77194 
L 155.965674 228.074954 
L 156.045143 222.142301 
L 156.124611 224.995665 
L 156.204079 220.774675 
L 156.363016 229.68858 
L 156.680889 224.490391 
L 156.760357 230.092819 
L 156.839826 224.53439 
L 156.998762 229.177532 
L 157.078231 227.435618 
L 157.157699 218.882304 
L 157.237167 230.928857 
L 157.316636 229.216558 
L 157.475572 224.446025 
L 157.55504 231.978106 
L 157.713977 227.889369 
L 157.793445 214.732642 
L 157.872914 230.780722 
L 157.952382 227.403264 
L 158.03185 215.529261 
L 158.111319 225.06182 
L 158.190787 221.201258 
L 158.270255 223.936043 
L 158.349724 231.594537 
L 158.429192 220.821562 
L 158.588129 229.106782 
L 158.667597 222.379987 
L 158.747065 222.637458 
L 158.826533 222.398081 
L 158.906002 230.453418 
L 158.98547 229.80783 
L 159.064938 229.56634 
L 159.223875 220.369681 
L 159.303343 229.541383 
L 159.382812 229.107021 
L 159.46228 223.537805 
L 159.541748 225.760554 
L 159.621217 222.239745 
L 159.780153 230.078561 
L 159.859622 228.592578 
L 159.93909 229.19511 
L 160.098026 225.478539 
L 160.177495 231.117949 
L 160.256963 225.622446 
L 160.4159 230.064237 
L 160.495368 228.427608 
L 160.574836 219.836834 
L 160.654305 231.817502 
L 160.733773 230.288342 
L 160.813241 225.7149 
L 160.89271 225.720547 
L 160.972178 232.746117 
L 161.131115 228.704689 
L 161.210583 216.274284 
L 161.290051 231.581133 
L 161.369519 228.245884 
L 161.448988 216.872477 
L 161.528456 225.586433 
L 161.607924 222.928379 
L 161.687393 224.819613 
L 161.766861 232.391031 
L 161.846329 222.064766 
L 162.005266 230.114455 
L 162.084734 223.123824 
L 162.164203 223.516458 
L 162.243671 223.541055 
L 162.323139 231.165004 
L 162.402608 230.500243 
L 162.482076 230.479919 
L 162.641012 221.081584 
L 162.720481 230.357944 
L 162.799949 230.086112 
L 162.879417 224.743623 
L 162.958886 226.484787 
L 163.038354 223.796518 
L 163.197291 230.590652 
L 163.515164 226.594405 
L 163.594632 232.090217 
L 163.674101 226.643877 
L 163.833037 230.822783 
L 163.912505 229.275212 
L 163.991974 220.992926 
L 164.071442 232.633494 
L 164.15091 231.444503 
L 164.230379 226.557683 
L 164.309847 226.820024 
L 164.389315 233.495276 
L 164.548252 229.47939 
L 164.62772 217.650846 
L 164.707189 232.312023 
L 164.786657 229.16275 
L 164.866125 218.188552 
L 164.945594 225.920309 
L 165.025062 224.999912 
L 165.10453 225.383641 
L 165.183998 233.079458 
L 165.263467 223.067469 
L 165.422403 231.119755 
L 165.501872 223.815305 
L 165.58134 224.219672 
L 165.660808 224.578152 
L 165.740277 231.848876 
L 165.819745 231.193257 
L 165.899213 231.373519 
L 166.05815 221.659494 
L 166.137618 231.10605 
L 166.217087 231.028343 
L 166.296555 225.844539 
L 166.376023 227.227824 
L 166.455491 225.291141 
L 166.614428 231.258666 
L 166.932301 228.061312 
L 167.01177 233.108058 
L 167.091238 227.859424 
L 167.250175 231.443153 
L 167.329643 229.812267 
L 167.409111 221.927893 
L 167.48858 233.35902 
L 167.568048 232.712614 
L 167.647516 227.353545 
L 167.726984 227.799096 
L 167.806453 234.250935 
L 167.965389 230.17693 
L 168.044858 218.709344 
L 168.124326 232.976489 
L 168.203794 230.444045 
L 168.283263 220.333917 
L 168.362731 226.262714 
L 168.442199 226.233746 
L 168.521668 226.949741 
L 168.601136 233.769579 
L 168.680604 223.61408 
L 168.839541 232.182988 
L 168.919009 224.312297 
L 168.998477 224.903382 
L 169.077946 225.461821 
L 169.157414 232.525397 
L 169.236882 231.862041 
L 169.316351 232.162514 
L 169.475287 222.34556 
L 169.634224 231.919591 
L 169.713692 226.899466 
L 169.793161 227.937742 
L 169.872629 226.456942 
L 170.031566 232.016688 
L 170.111034 231.287889 
L 170.190502 231.298917 
L 170.26997 230.986563 
L 170.349439 229.017192 
L 170.428907 233.832566 
L 170.508375 229.40795 
L 170.587844 231.026358 
L 170.667312 231.984115 
L 170.74678 230.005865 
L 170.826249 222.573474 
L 170.905717 234.000263 
L 170.985185 233.596411 
L 171.064654 228.174348 
L 171.144122 228.763724 
L 171.22359 234.896222 
L 171.382527 230.836982 
L 171.461995 219.591771 
L 171.541463 233.579146 
L 171.620932 231.725685 
L 171.7004 221.984419 
L 171.859337 227.238182 
L 171.938805 227.623664 
L 172.018273 234.374455 
L 172.097742 224.216741 
L 172.256678 232.968007 
L 172.336147 225.172403 
L 172.415615 225.811634 
L 172.495083 226.894017 
L 172.574552 233.15128 
L 172.65402 232.328876 
L 172.733488 232.88089 
L 172.892425 222.936282 
L 173.051361 232.699843 
L 173.13083 227.869818 
L 173.210298 228.619724 
L 173.289766 227.454177 
L 173.448703 232.71761 
L 173.60764 231.750055 
L 173.687108 232.657461 
L 173.766576 229.170954 
L 173.846045 234.506759 
L 173.925513 231.43496 
L 174.004981 231.695554 
L 174.084449 232.631853 
L 174.163918 230.869638 
L 174.243386 224.298384 
L 174.322854 234.609645 
L 174.402323 234.1897 
L 174.481791 228.938765 
L 174.561259 229.842864 
L 174.640728 235.369701 
L 174.799664 231.39143 
L 174.879133 220.569024 
L 174.958601 234.134312 
L 175.038069 232.455321 
L 175.117538 223.283811 
L 175.276474 228.17509 
L 175.355942 228.270404 
L 175.435411 234.972132 
L 175.514879 224.992786 
L 175.673816 233.558193 
L 175.753284 226.487474 
L 175.832752 226.938336 
L 176.150626 233.619952 
L 176.309562 223.526334 
L 176.468499 233.395703 
L 176.547967 228.792856 
L 176.627435 229.436015 
L 176.706904 228.530273 
L 176.86584 233.507391 
L 177.024777 232.11881 
L 177.104245 233.731307 
L 177.183714 229.100868 
L 177.263182 235.079624 
L 177.34265 232.529259 
L 177.422119 232.740786 
L 177.501587 233.300626 
L 177.581055 231.790377 
L 177.660524 225.724517 
L 177.739992 235.118683 
L 177.81946 234.720152 
L 177.898928 229.761257 
L 177.978397 230.640166 
L 178.057865 235.807305 
L 178.216802 231.916623 
L 178.29627 221.459982 
L 178.375738 234.621788 
L 178.455207 232.945995 
L 178.534675 224.174239 
L 178.693612 229.084881 
L 178.77308 228.938053 
L 178.852548 235.544236 
L 178.932017 225.848381 
L 179.090953 234.056865 
L 179.170421 227.793454 
L 179.24989 228.866565 
L 179.329358 230.176875 
L 179.408826 233.909313 
L 179.488295 232.952072 
L 179.567763 234.242668 
L 179.7267 224.139735 
L 179.885636 233.968026 
L 179.965105 229.539788 
L 180.044573 230.152426 
L 180.124041 229.38399 
L 180.282978 234.182747 
L 180.441914 232.761433 
L 180.521383 234.446883 
L 180.600851 229.567795 
L 180.680319 235.603166 
L 180.759788 233.124539 
L 180.839256 233.352605 
L 180.918724 233.785314 
L 180.998193 232.360323 
L 181.077661 226.650312 
L 181.157129 235.574832 
L 181.236598 235.159936 
L 181.316066 230.443478 
L 181.395534 231.337302 
L 181.475003 236.204124 
L 181.633939 232.442769 
L 181.713407 222.248891 
L 181.792876 235.074985 
L 181.872344 233.414923 
L 181.951812 224.913008 
L 182.031281 228.27682 
L 182.110749 229.924615 
L 182.190217 229.566095 
L 182.269686 236.04936 
L 182.349154 226.597703 
L 182.508091 234.562872 
L 182.587559 228.740808 
L 182.667027 229.866592 
L 182.746496 230.808678 
L 182.825964 234.493951 
L 182.905432 233.599411 
L 182.9849 234.67442 
L 183.143837 224.696365 
L 183.302774 234.458873 
L 183.382242 230.264182 
L 183.46171 230.755245 
L 183.541179 230.066399 
L 183.700115 234.844737 
L 183.859052 233.45187 
L 183.93852 235.049125 
L 184.017989 229.997845 
L 184.097457 236.036288 
L 184.176925 233.652016 
L 184.335862 234.221114 
L 184.41533 232.98767 
L 184.494798 227.618192 
L 184.574267 236.003908 
L 184.653735 235.56753 
L 184.733203 231.061477 
L 184.812672 232.048316 
L 184.89214 236.547284 
L 185.051077 232.98752 
L 185.130545 222.994965 
L 185.210013 235.54861 
L 185.289482 233.894242 
L 185.36895 225.752575 
L 185.448418 228.265822 
L 185.686823 236.506098 
L 185.766291 227.33089 
L 185.925228 235.00736 
L 186.004696 229.47145 
L 186.084165 230.605025 
L 186.163633 231.199926 
L 186.243101 235.012399 
L 186.32257 234.193932 
L 186.402038 235.029815 
L 186.560975 225.256805 
L 186.719911 234.890885 
L 186.799379 230.988331 
L 186.878848 231.248258 
L 186.958316 230.646447 
L 187.117253 235.519565 
L 187.276189 234.115659 
L 187.355658 235.569105 
L 187.435126 230.489222 
L 187.514594 236.417695 
L 187.594063 234.113246 
L 187.752999 234.626141 
L 187.832468 233.647041 
L 187.911936 228.525866 
L 187.991404 236.40035 
L 188.070872 235.930949 
L 188.150341 231.642037 
L 188.229809 232.692841 
L 188.309277 236.854627 
L 188.468214 233.587268 
L 188.547682 223.694564 
L 188.627151 235.997076 
L 188.706619 234.378646 
L 188.786087 226.800327 
L 188.865556 228.480541 
L 189.103961 236.929736 
L 189.183429 228.080106 
L 189.342365 235.430319 
L 189.421834 230.043369 
L 189.501302 231.219644 
L 189.58077 231.619075 
L 189.660239 235.468845 
L 189.739707 234.685238 
L 189.819175 235.321115 
L 189.978112 225.792559 
L 190.137049 235.290978 
L 190.295985 231.760806 
L 190.375454 231.101482 
L 190.53439 236.116951 
L 190.613858 234.670409 
L 190.693327 234.71969 
L 190.772795 236.025532 
L 190.852263 231.02676 
L 190.931732 236.748541 
L 191.0112 234.493687 
L 191.170137 235.028331 
L 191.249605 234.307444 
L 191.329073 229.324341 
L 191.408542 236.768843 
L 191.48801 236.26337 
L 191.567478 232.206284 
L 191.646947 233.271268 
L 191.726415 237.130779 
L 191.885351 234.195756 
L 191.96482 224.414815 
L 192.044288 236.395007 
L 192.123756 234.78305 
L 192.203225 227.690359 
L 192.282693 229.245113 
L 192.362161 233.179739 
L 192.44163 231.03392 
L 192.521098 237.315804 
L 192.600566 228.759881 
L 192.759503 235.893032 
L 192.838971 230.479233 
L 192.91844 231.678615 
L 192.997908 232.075884 
L 193.077376 235.847561 
L 193.156844 235.054736 
L 193.236313 235.598862 
L 193.395249 226.268342 
L 193.554186 235.638952 
L 193.713123 232.311342 
L 193.792591 231.461495 
L 193.951528 236.620644 
L 194.030996 235.072172 
L 194.110464 235.275449 
L 194.189933 236.449037 
L 194.269401 231.585238 
L 194.348869 237.030205 
L 194.428337 234.839236 
L 194.507806 235.454487 
L 194.587274 235.424253 
L 194.666742 234.876816 
L 194.746211 230.014581 
L 194.825679 237.094542 
L 194.905147 236.570766 
L 194.984616 232.746108 
L 195.064084 233.831708 
L 195.143552 237.378638 
L 195.302489 234.68524 
L 195.381957 225.221485 
L 195.461425 236.729667 
L 195.540894 235.071483 
L 195.620362 228.401281 
L 195.69983 229.714604 
L 195.779299 234.348464 
L 195.858767 231.394008 
L 195.938235 237.669168 
L 196.017704 229.413287 
L 196.17664 236.281239 
L 196.256109 230.819846 
L 196.335577 232.127126 
L 196.415045 232.631729 
L 196.494514 236.211488 
L 196.573982 235.348249 
L 196.65345 235.899351 
L 196.812387 226.654456 
L 196.971323 235.962414 
L 197.13026 232.851102 
L 197.209728 231.872826 
L 197.368665 236.892792 
L 197.448133 235.515151 
L 197.527602 235.788347 
L 197.60707 236.841136 
L 197.686538 232.158592 
L 197.766007 237.31815 
L 197.845475 235.179341 
L 197.924943 235.89847 
L 198.004411 235.795027 
L 198.08388 235.368806 
L 198.163348 230.612201 
L 198.242816 237.396593 
L 198.322285 236.870109 
L 198.401753 233.295818 
L 198.481221 234.371025 
L 198.56069 237.607038 
L 198.719626 235.084708 
L 198.799095 225.893326 
L 198.878563 237.052659 
L 198.958031 235.37708 
L 199.0375 229.064148 
L 199.116968 230.094506 
L 199.196436 234.860767 
L 199.275904 231.826943 
L 199.355373 237.983799 
L 199.434841 230.043865 
L 199.593778 236.645403 
L 199.673246 231.1436 
L 199.752714 232.536095 
L 199.832183 233.096877 
L 199.911651 236.530075 
L 199.991119 235.581876 
L 200.070588 236.191237 
L 200.229524 226.982996 
L 200.388461 236.254695 
L 200.547397 233.331173 
L 200.626866 232.29866 
L 200.785802 237.093095 
L 200.865271 235.918428 
L 200.944739 236.25028 
L 201.024207 237.192219 
L 201.103676 232.778896 
L 201.183144 237.590159 
L 201.262612 235.529726 
L 201.342081 236.304798 
L 201.501017 235.794006 
L 201.580486 231.145062 
L 201.659954 237.670043 
L 201.739422 237.159295 
L 201.81889 233.807495 
L 201.898359 234.858112 
L 201.977827 237.826996 
L 202.136764 235.441913 
L 202.216232 226.512411 
L 202.2957 237.366822 
L 202.375169 235.693564 
L 202.454637 229.677594 
L 202.534105 230.436988 
L 202.613574 235.238806 
L 202.693042 232.256314 
L 202.77251 238.265049 
L 202.851979 230.700567 
L 203.010915 237.005499 
L 203.090383 231.451459 
L 203.169852 232.910931 
L 203.24932 233.525813 
L 203.328788 236.819057 
L 203.408257 235.778138 
L 203.487725 236.461655 
L 203.646662 227.265628 
L 203.805598 236.52133 
L 203.964535 233.786629 
L 204.044003 232.696148 
L 204.20294 237.324794 
L 204.282408 236.279126 
L 204.361876 236.656334 
L 204.441345 237.504661 
L 204.520813 233.412201 
L 204.600281 237.841961 
L 204.67975 235.879137 
L 204.759218 236.668576 
L 204.838686 236.414249 
L 204.918155 236.16865 
L 204.997623 231.623545 
L 205.077091 237.922965 
L 205.15656 237.438958 
L 205.236028 234.273959 
L 205.315496 235.300002 
L 205.394965 238.036951 
L 205.553901 235.784305 
L 205.633369 227.106183 
L 205.712838 237.670085 
L 205.792306 236.020779 
L 205.871774 230.227779 
L 205.951243 230.756137 
L 206.030711 235.558943 
L 206.110179 232.6559 
L 206.189648 238.520171 
L 206.269116 231.376936 
L 206.428053 237.388972 
L 206.507521 231.75171 
L 206.586989 233.256489 
L 206.666458 233.964961 
L 206.745926 237.1013 
L 206.825394 235.972966 
L 206.904862 236.7183 
L 207.063799 227.510542 
L 207.222736 236.765013 
L 207.381672 234.235483 
L 207.461141 233.070381 
L 207.620077 237.564144 
L 207.699546 236.607792 
L 207.858482 237.78934 
L 207.937951 234.022451 
L 208.017419 238.080189 
L 208.096887 236.218982 
L 208.176355 237.003767 
L 208.255824 236.700711 
L 208.335292 236.501039 
L 208.41476 232.046954 
L 208.494229 238.157182 
L 208.573697 237.710454 
L 208.653165 234.696813 
L 208.732634 235.703476 
L 208.812102 238.236405 
L 208.971039 236.120021 
L 209.050507 227.659383 
L 209.129975 237.957793 
L 209.209444 236.352116 
L 209.288912 230.716277 
L 209.36838 231.06147 
L 209.447848 235.849134 
L 209.527317 233.031121 
L 209.606785 238.754286 
L 209.686253 232.016309 
L 209.84519 237.776149 
L 209.924658 232.066929 
L 210.004127 233.585274 
L 210.083595 234.421122 
L 210.163063 237.390624 
L 210.242532 236.219063 
L 210.322 236.968246 
L 210.480937 227.731663 
L 210.639873 236.98393 
L 210.79881 234.668344 
L 210.878278 233.436412 
L 211.037215 237.798289 
L 211.116683 236.910068 
L 211.27562 238.048 
L 211.355088 234.601072 
L 211.434556 238.312045 
L 211.514025 236.542997 
L 211.593493 237.320522 
L 211.75243 236.802537 
L 211.831898 232.423625 
L 211.911366 238.373166 
L 211.990834 237.971886 
L 212.070303 235.079351 
L 212.149771 236.073627 
L 212.229239 238.426642 
L 212.388176 236.444634 
L 212.467644 228.164141 
L 212.547113 238.221974 
L 212.626581 236.671253 
L 212.706049 231.161179 
L 212.785518 231.354071 
L 212.864986 236.126382 
L 212.944454 233.387871 
L 213.023923 238.969417 
L 213.103391 232.585023 
L 213.262327 238.121235 
L 213.341796 232.402762 
L 213.421264 233.907407 
L 213.500732 234.873742 
L 213.580201 237.67886 
L 213.659669 236.519953 
L 213.739137 237.216855 
L 213.898074 227.942197 
L 214.057011 237.179678 
L 214.136479 234.980426 
L 214.215947 235.06798 
L 214.295416 233.79637 
L 214.454352 238.035451 
L 214.53382 237.190567 
L 214.692757 238.280724 
L 214.772225 235.14109 
L 214.851694 238.534497 
L 214.931162 236.846213 
L 215.01063 237.620635 
L 215.169567 237.087084 
L 215.249035 232.768159 
L 215.328504 238.571985 
L 215.407972 238.219975 
L 215.48744 235.425619 
L 215.566909 236.413785 
L 215.646377 238.608624 
L 215.725845 237.737405 
L 215.805313 236.750354 
L 215.884782 228.625386 
L 215.96425 238.459449 
L 216.043718 236.968975 
L 216.123187 231.578352 
L 216.202655 231.632222 
L 216.282123 236.394493 
L 216.361592 233.728387 
L 216.44106 239.16713 
L 216.520528 233.094426 
L 216.679465 238.419303 
L 216.758933 232.738191 
L 216.838402 234.222158 
L 216.91787 235.309828 
L 216.997338 237.950687 
L 217.076806 236.832988 
L 217.156275 237.466325 
L 217.315211 228.146203 
L 217.474148 237.361867 
L 217.553616 235.267198 
L 217.633085 235.431469 
L 217.712553 234.147186 
L 217.87149 238.280569 
L 217.950958 237.455793 
L 218.109895 238.492123 
L 218.189363 235.635698 
L 218.268831 238.742886 
L 218.348299 237.127305 
L 218.427768 237.902614 
L 218.586704 237.359517 
L 218.666173 233.086906 
L 218.745641 238.755128 
L 218.825109 238.455296 
L 218.904578 235.73917 
L 218.984046 236.726739 
L 219.063514 238.781681 
L 219.142983 237.997904 
L 219.222451 237.033974 
L 219.301919 229.05722 
L 219.381388 238.673758 
L 219.460856 237.245722 
L 219.619792 231.895706 
L 219.699261 236.653849 
L 219.778729 234.053955 
L 219.858197 239.349573 
L 219.937666 233.558537 
L 220.096602 238.681626 
L 220.176071 233.061939 
L 220.255539 234.528754 
L 220.414476 238.203058 
L 220.573412 237.716543 
L 220.732349 228.341505 
L 220.891285 237.540659 
L 220.970754 235.533308 
L 221.050222 235.763128 
L 221.12969 234.487949 
L 221.288627 238.527696 
L 221.368095 237.709572 
L 221.527032 238.687667 
L 221.6065 236.080557 
L 221.685969 238.936293 
L 221.765437 237.387717 
L 221.844905 238.166941 
L 222.003842 237.618625 
L 222.08331 233.382068 
L 222.162778 238.924677 
L 222.242247 238.681041 
L 222.321715 236.024025 
L 222.401183 237.014977 
L 222.480652 238.94516 
L 222.56012 238.242312 
L 222.639588 237.296028 
L 222.719057 229.474754 
L 222.798525 238.870778 
L 222.877993 237.504729 
L 223.03693 232.145973 
L 223.116398 236.904436 
L 223.195867 234.365821 
L 223.275335 239.518628 
L 223.354803 233.980822 
L 223.51374 238.916205 
L 223.593208 233.371718 
L 223.672676 234.82869 
L 223.831613 238.440528 
L 223.911081 237.391958 
L 223.99055 237.966888 
L 224.149486 228.52537 
L 224.308423 237.721449 
L 224.387891 235.778071 
L 224.46736 236.06751 
L 224.546828 234.816932 
L 224.705764 238.769567 
L 224.785233 237.953368 
L 224.944169 238.871557 
L 225.023638 236.476071 
L 225.103106 239.116224 
L 225.182574 237.631236 
L 225.262043 238.415374 
L 225.341511 237.838319 
L 225.420979 237.862412 
L 225.500448 233.655948 
L 225.579916 239.082659 
L 225.659384 238.900868 
L 225.738853 236.284333 
L 225.818321 237.281071 
L 225.897789 239.098941 
L 225.977257 238.470337 
L 226.056726 237.537968 
L 226.136194 229.893373 
L 226.215662 239.056322 
L 226.295131 237.749188 
L 226.454067 232.385026 
L 226.533536 237.145944 
L 226.613004 234.664728 
L 226.692472 239.675791 
L 226.771941 234.360928 
L 226.930877 239.128148 
L 227.010346 233.667103 
L 227.089814 235.123491 
L 227.24875 238.668842 
L 227.328219 237.626554 
L 227.407687 238.216726 
L 227.566624 228.696104 
L 227.72556 237.904247 
L 227.805029 236.002889 
L 227.884497 236.349806 
L 227.963965 235.131807 
L 228.122902 239.000476 
L 228.20237 238.188018 
L 228.361307 239.046701 
L 228.440775 236.826246 
L 228.520243 239.284774 
L 228.599712 237.863029 
L 228.67918 238.649713 
L 228.758648 238.021601 
L 228.838117 238.089983 
L 228.917585 233.911169 
L 228.997053 239.230787 
L 229.076522 239.116927 
L 229.15599 236.523983 
L 229.235458 237.527813 
L 229.314927 239.242975 
L 229.394395 238.680703 
L 229.473863 237.760965 
L 229.553332 230.331989 
L 229.6328 239.235483 
L 229.712268 237.983616 
L 229.871205 232.614181 
L 229.950673 237.377573 
L 230.030141 234.949968 
L 230.10961 239.822303 
L 230.189078 234.698854 
L 230.348015 239.321266 
L 230.427483 233.948152 
L 230.506951 235.411493 
L 230.665888 238.891175 
L 230.745356 237.834968 
L 230.824825 238.465304 
L 230.983761 228.852757 
L 231.142698 238.086784 
L 231.222166 236.211749 
L 231.301634 236.616173 
L 231.381103 235.430048 
L 231.540039 239.216755 
L 231.619508 238.413923 
L 231.778444 239.214807 
L 231.857913 237.136643 
L 231.937381 239.443888 
L 232.016849 238.087378 
L 232.096317 238.871474 
L 232.175786 238.194845 
L 232.255254 238.301992 
L 232.334722 234.150294 
L 232.414191 239.370606 
L 232.493659 239.326369 
L 232.573127 236.747268 
L 232.652596 237.757878 
L 232.732064 239.376962 
L 232.811532 238.870998 
L 232.891001 237.965992 
L 232.970469 230.811267 
L 233.049937 239.413188 
L 233.129406 238.21557 
L 233.288342 232.832266 
L 233.36781 237.596384 
L 233.447279 235.218671 
L 233.526747 239.959359 
L 233.606215 234.997376 
L 233.765152 239.498644 
L 233.84462 234.216847 
L 233.924089 235.685272 
L 234.083025 239.10449 
L 234.162494 238.021618 
L 234.241962 238.710384 
L 234.400899 228.99594 
L 234.559835 238.26734 
L 234.639303 236.41078 
L 234.718772 236.871996 
L 234.79824 235.70883 
L 234.957177 239.416967 
L 235.036645 238.629027 
L 235.195582 239.376012 
L 235.27505 237.413357 
L 235.354518 239.595298 
L 235.433987 238.304662 
L 235.513455 239.081835 
L 235.592923 238.358437 
L 235.672392 238.500577 
L 235.75186 234.37513 
L 235.910796 239.520066 
L 235.990265 236.959552 
L 236.069733 237.973432 
L 236.149201 239.500886 
L 236.22867 239.040474 
L 236.308138 238.15442 
L 236.387606 231.331107 
L 236.467075 239.596606 
L 236.546543 238.452568 
L 236.70548 233.03744 
L 236.784948 237.79936 
L 236.864416 235.470657 
L 236.943885 240.088686 
L 237.023353 235.269278 
L 237.182289 239.663068 
L 237.261758 234.48267 
L 237.341226 235.936306 
L 237.500163 239.298184 
L 237.579631 238.190575 
L 237.659099 238.944451 
L 237.818036 229.130358 
L 237.897504 238.455419 
L 237.976973 238.445638 
L 238.056441 236.604501 
L 238.135909 237.116649 
L 238.215378 235.966626 
L 238.374314 239.602007 
L 238.453782 238.826944 
L 238.533251 239.556792 
L 238.612719 239.52881 
L 238.692187 237.663333 
L 238.771656 239.740236 
L 238.851124 238.511864 
L 238.930592 239.281462 
L 239.010061 238.514838 
L 239.089529 238.688858 
L 239.168997 234.586337 
L 239.327934 239.691488 
L 239.407402 237.164666 
L 239.566339 239.615795 
L 239.645807 239.193196 
L 239.725275 238.328414 
L 239.804744 231.83051 
L 239.884212 239.78704 
L 239.96368 238.694251 
L 240.122617 233.231242 
L 240.202085 237.990825 
L 240.281554 235.711498 
L 240.361022 240.212737 
L 240.44049 235.533025 
L 240.599427 239.816551 
L 240.678895 234.759278 
L 240.758364 236.162741 
L 240.9173 239.465651 
L 240.996768 238.343648 
L 241.076237 239.155189 
L 241.235173 229.262701 
L 241.314642 238.682368 
L 241.39411 238.619776 
L 241.473578 236.790592 
L 241.553047 237.345861 
L 241.632515 236.204604 
L 241.791452 239.774232 
L 241.87092 239.001238 
L 241.950388 239.799906 
L 242.029857 239.671599 
L 242.109325 237.892724 
L 242.188793 239.878126 
L 242.268261 238.706964 
L 242.34773 239.469426 
L 242.427198 238.669613 
L 242.506666 238.869421 
L 242.586135 234.784043 
L 242.745071 239.844227 
L 242.82454 237.362571 
L 242.983476 239.723663 
L 243.062945 239.33552 
L 243.142413 238.490396 
L 243.221881 232.234451 
L 243.30135 239.969781 
L 243.380818 238.93089 
L 243.539754 233.415017 
L 243.619223 238.17894 
L 243.698691 235.939954 
L 243.778159 240.332314 
L 243.857628 235.787408 
L 244.016564 239.959456 
L 244.096033 235.045313 
L 244.175501 236.366094 
L 244.334438 239.613351 
L 244.413906 238.481079 
L 244.493374 239.338491 
L 244.652311 229.394314 
L 244.731779 238.899761 
L 244.811247 238.78331 
L 244.890716 236.964389 
L 244.970184 237.566822 
L 245.049652 236.425765 
L 245.208589 239.935262 
L 245.288057 239.15369 
L 245.367526 240.017804 
L 245.446994 239.803828 
L 245.526462 238.103974 
L 245.605931 240.006596 
L 245.685399 238.889956 
L 245.764867 239.64359 
L 245.844336 238.822655 
L 245.923804 239.042906 
L 246.003272 234.968698 
L 246.162209 239.984278 
L 246.241677 237.553287 
L 246.400614 239.826713 
L 246.480082 239.470628 
L 246.55955 238.641885 
L 246.639019 232.545308 
L 246.718487 240.136423 
L 246.797955 239.154516 
L 246.956892 233.588703 
L 247.03636 238.365405 
L 247.115829 236.154641 
L 247.195297 240.447189 
L 247.274765 236.023119 
L 247.433702 240.093153 
L 247.51317 235.33602 
L 247.592638 236.549564 
L 247.751575 239.74892 
L 247.831043 238.604327 
L 247.910512 239.499922 
L 248.069448 229.523752 
L 248.148917 239.101271 
L 248.228385 238.932797 
L 248.307853 237.126157 
L 248.387322 237.792742 
L 248.46679 236.63469 
L 248.625726 240.085537 
L 248.705195 239.29145 
L 248.784663 240.208838 
L 248.864131 239.925912 
L 248.9436 238.297919 
L 249.023068 240.124334 
L 249.102536 239.062059 
L 249.182005 239.804145 
L 249.261473 238.969873 
L 249.340941 239.20888 
L 249.42041 235.141582 
L 249.579346 240.114838 
L 249.658815 237.738136 
L 249.817751 239.926324 
L 249.897219 239.598498 
L 249.976688 238.783375 
L 250.056156 232.79832 
L 250.135624 240.288418 
L 250.215093 239.363394 
L 250.374029 233.753202 
L 250.453498 238.542239 
L 250.532966 236.357708 
L 250.612434 240.557817 
L 250.691903 236.239528 
L 250.850839 240.219636 
L 250.930308 235.632689 
L 251.009776 236.717099 
L 251.168712 239.875139 
L 251.248181 238.71526 
L 251.327649 239.645213 
L 251.407117 236.717229 
L 251.486586 229.650308 
L 251.566054 239.287384 
L 251.645522 239.068355 
L 251.724991 237.280097 
L 251.804459 238.031628 
L 251.883927 236.835326 
L 252.042864 240.224923 
L 252.122332 239.419401 
L 252.201801 240.376013 
L 252.281269 240.038716 
L 252.360737 238.476051 
L 252.440205 240.232069 
L 252.519674 239.225023 
L 252.599142 239.95322 
L 252.67861 239.109914 
L 252.758079 239.366988 
L 252.837547 235.304107 
L 252.996484 240.237416 
L 253.075952 237.918134 
L 253.234889 240.022999 
L 253.314357 239.718996 
L 253.393825 238.915055 
L 253.473294 233.016458 
L 253.552762 240.42778 
L 253.63223 239.558264 
L 253.791167 233.910683 
L 253.870635 238.704835 
L 253.950103 236.550909 
L 254.029572 240.664423 
L 254.10904 236.439757 
L 254.267977 240.340023 
L 254.347445 235.937603 
L 254.426913 236.871898 
L 254.58585 239.992808 
L 254.665318 238.815856 
L 254.744787 239.778018 
L 254.824255 236.878213 
L 254.903723 229.773682 
L 254.983191 239.460674 
L 255.06266 239.190973 
L 255.142128 237.430003 
L 255.221596 238.285738 
L 255.301065 237.031423 
L 255.460001 240.35329 
L 255.53947 239.539366 
L 255.618938 240.523391 
L 255.698406 240.143016 
L 255.777875 238.640368 
L 255.857343 240.332265 
L 255.936811 239.380401 
L 256.01628 240.092484 
L 256.095748 239.243841 
L 256.175216 239.517122 
L 256.254684 235.457355 
L 256.413621 240.352581 
L 256.493089 238.094039 
L 256.652026 240.116926 
L 256.731494 239.832675 
L 256.810963 239.037431 
L 256.890431 233.211463 
L 256.969899 240.555758 
L 257.049368 239.740102 
L 257.208304 234.06308 
L 257.287773 238.852969 
L 257.367241 236.735523 
L 257.446709 240.766912 
L 257.526177 236.627464 
L 257.685114 240.454889 
L 257.764582 236.252803 
L 257.844051 237.016462 
L 258.002987 240.102498 
L 258.082456 238.908309 
L 258.161924 239.900802 
L 258.241392 237.031287 
L 258.320861 229.893641 
L 258.400329 239.623398 
L 258.479797 239.302383 
L 258.559266 237.577634 
L 258.638734 238.550848 
L 258.718202 237.226405 
L 258.877139 240.470769 
L 258.956607 239.651954 
L 259.036075 240.654797 
L 259.115544 240.239645 
L 259.195012 238.792671 
L 259.27448 240.429286 
L 259.353949 239.528653 
L 259.433417 240.222331 
L 259.512885 239.374512 
L 259.592354 239.659204 
L 259.671822 235.60249 
L 259.830759 240.460394 
L 259.910227 238.266649 
L 260.069163 240.208275 
L 260.148632 239.940476 
L 260.2281 239.151531 
L 260.307568 233.390383 
L 260.387037 240.673428 
L 260.466505 239.90893 
L 260.625442 234.211224 
L 260.70491 238.988125 
L 260.784378 236.912498 
L 260.863847 240.865196 
L 260.943315 236.806031 
L 261.102252 240.564649 
L 261.18172 236.57933 
L 261.261188 237.153006 
L 261.420125 240.205017 
L 261.499593 238.995259 
L 261.579061 240.015415 
L 261.65853 237.18116 
L 261.737998 230.009908 
L 261.817466 239.77709 
L 261.896935 239.404864 
L 261.976403 237.723247 
L 262.055871 238.814778 
L 262.13534 237.420118 
L 262.294276 240.577753 
L 262.373745 239.757903 
L 262.453213 240.773531 
L 262.532681 240.329858 
L 262.612149 238.934575 
L 262.691618 240.52798 
L 262.771086 239.668273 
L 262.850554 240.342451 
L 262.930023 239.505566 
L 263.009491 239.79307 
L 263.088959 235.741143 
L 263.247896 240.560896 
L 263.327364 238.436468 
L 263.486301 240.297288 
L 263.565769 240.043283 
L 263.645238 239.258775 
L 263.724706 233.558651 
L 263.804174 240.781965 
L 263.883642 240.063618 
L 264.042579 234.354763 
L 264.122047 239.112155 
L 264.201516 237.082197 
L 264.280984 240.959328 
L 264.360452 236.978292 
L 264.519389 240.669625 
L 264.598857 236.915962 
L 264.678326 237.283648 
L 264.837262 240.301505 
L 264.916731 239.080112 
L 264.996199 240.123305 
L 265.075667 237.329886 
L 265.155135 230.122333 
L 265.234604 239.922567 
L 265.314072 239.500647 
L 265.39354 237.866519 
L 265.473009 239.060791 
L 265.552477 237.607426 
L 265.711414 240.675033 
L 265.790882 239.858353 
L 265.87035 240.882136 
L 265.949819 240.415335 
L 266.029287 239.067706 
L 266.108755 240.630235 
L 266.188224 239.796223 
L 266.267692 240.453141 
L 266.34716 239.637926 
L 266.426628 239.918514 
L 266.506097 235.875119 
L 266.665033 240.654486 
L 266.744502 238.602987 
L 266.903438 240.38423 
L 266.982907 240.141622 
L 267.062375 239.360548 
L 267.141843 233.72045 
L 267.221312 240.882569 
L 267.30078 240.203357 
L 267.459717 234.492867 
L 267.539185 239.226535 
L 267.618653 237.244214 
L 267.698121 241.049433 
L 267.77759 237.14669 
L 267.936526 240.769987 
L 268.015995 237.259401 
L 268.095463 237.410156 
L 268.2544 240.393065 
L 268.333868 239.166579 
L 268.413336 240.225554 
L 268.492805 237.475985 
L 268.572273 230.230953 
L 268.651741 240.060068 
L 268.73121 239.591298 
L 268.810678 238.007303 
L 268.890146 239.276589 
L 268.969614 237.782876 
L 269.128551 240.764022 
L 269.208019 239.954442 
L 269.287488 240.982505 
L 269.366956 240.497944 
L 269.446424 239.193557 
L 269.525893 240.733704 
L 269.605361 239.910237 
L 269.684829 240.555879 
L 269.764298 239.768278 
L 269.843766 240.035495 
L 269.923234 236.005715 
L 270.082171 240.741892 
L 270.161639 238.76491 
L 270.320576 240.469274 
L 270.400044 240.235659 
L 270.479512 239.457783 
L 270.558981 233.878399 
L 270.638449 240.97625 
L 270.717917 240.329018 
L 270.876854 234.625083 
L 270.956322 239.332052 
L 271.035791 237.39779 
L 271.115259 241.135609 
L 271.194727 237.313645 
L 271.353664 240.865767 
L 271.5126 237.533584 
L 271.671537 240.480311 
L 271.751005 239.257184 
L 271.830474 240.322956 
L 271.909942 237.616861 
L 271.98941 230.335808 
L 272.068879 240.189581 
L 272.148347 239.677516 
L 272.227815 238.14593 
L 272.307284 239.459701 
L 272.386752 237.945156 
L 272.545688 240.846635 
L 272.625157 240.047028 
L 272.704625 241.076329 
L 272.784093 240.579513 
L 272.863562 239.31332 
L 272.94303 240.834667 
L 273.022498 240.010572 
L 273.101967 240.652516 
L 273.181435 239.892899 
L 273.260903 240.144392 
L 273.340372 236.133501 
L 273.499308 240.823812 
L 273.578777 238.921346 
L 273.737713 240.55247 
L 273.817181 240.325393 
L 273.89665 239.550969 
L 273.976118 234.034038 
L 274.055586 241.063716 
L 274.135055 240.442681 
L 274.293991 234.751477 
L 274.37346 239.429039 
L 274.452928 237.542556 
L 274.532396 241.217914 
L 274.611865 237.481467 
L 274.770801 240.956967 
L 274.929738 237.654212 
L 275.088674 240.563542 
L 275.168143 239.351872 
L 275.247611 240.416137 
L 275.327079 237.75108 
L 275.406548 230.436883 
L 275.486016 240.311124 
L 275.565484 239.759283 
L 275.644953 238.282973 
L 275.724421 239.614433 
L 275.803889 238.096086 
L 275.962826 240.924718 
L 276.042294 240.13671 
L 276.121763 241.165295 
L 276.201231 240.6615 
L 276.280699 239.427898 
L 276.360167 240.930771 
L 276.439636 240.099351 
L 276.519104 240.744369 
L 276.598572 240.010479 
L 276.678041 240.245924 
L 276.757509 236.258639 
L 276.916446 240.900727 
L 276.995914 239.07227 
L 277.154851 240.63382 
L 277.234319 240.410779 
L 277.313787 239.640401 
L 277.393256 234.188373 
L 277.472724 241.145467 
L 277.552192 240.546564 
L 277.711129 234.872291 
L 277.790597 239.517863 
L 277.870065 237.678797 
L 277.949534 241.296413 
L 278.029002 237.651686 
L 278.187939 241.043624 
L 278.346875 237.77169 
L 278.505812 240.643246 
L 278.58528 239.447881 
L 278.664749 240.505622 
L 278.744217 237.878641 
L 278.823685 230.534234 
L 278.903153 240.424759 
L 278.982622 239.836098 
L 279.06209 238.418859 
L 279.141558 239.746669 
L 279.221027 238.238093 
L 279.379963 240.999566 
L 279.459432 240.223956 
L 279.5389 241.250832 
L 279.697837 239.537925 
L 279.777305 241.021259 
L 279.856773 240.179044 
L 279.936242 240.832118 
L 280.01571 240.121339 
L 280.095178 240.340918 
L 280.174646 236.381221 
L 280.333583 240.973006 
L 280.413051 239.218046 
L 280.571988 240.713318 
L 280.651456 240.491762 
L 280.730925 239.72635 
L 280.810393 234.341819 
L 280.889861 241.22195 
L 280.96933 240.642525 
L 281.128266 234.987719 
L 281.207735 239.599298 
L 281.287203 237.807229 
L 281.366671 241.371194 
L 281.446139 237.824468 
L 281.605076 241.125804 
L 281.764013 237.885317 
L 281.922949 240.720221 
L 282.002418 239.541283 
L 282.081886 240.591846 
L 282.161354 238.000292 
L 282.240823 230.628086 
L 282.320291 240.53055 
L 282.399759 239.90723 
L 282.479228 238.553602 
L 282.558696 239.861329 
L 282.638164 238.373276 
L 282.797101 241.071767 
L 282.876569 240.30921 
L 282.956037 241.333697 
L 283.114974 239.643831 
L 283.194442 241.106246 
L 283.273911 240.251654 
L 283.353379 240.916085 
L 283.432847 240.226122 
L 283.512316 240.430196 
L 283.591784 236.501398 
L 283.750721 241.041079 
L 283.830189 239.358762 
L 283.989125 240.790886 
L 284.068594 240.568266 
L 284.148062 239.809093 
L 284.22753 234.494136 
L 284.306999 241.293667 
L 284.386467 240.732036 
L 284.545404 235.09794 
L 284.624872 239.67466 
L 284.70434 237.928716 
L 284.783809 241.442337 
L 284.863277 237.998752 
L 285.022214 241.203584 
L 285.18115 237.994508 
L 285.340087 240.795186 
L 285.419555 239.62943 
L 285.499023 240.67516 
L 285.578492 238.116938 
L 285.65796 230.718819 
L 285.737428 240.628687 
L 285.816897 239.972139 
L 285.896365 238.686776 
L 285.975833 239.962177 
L 286.055302 238.503436 
L 286.214238 241.141317 
L 286.293707 240.392849 
L 286.373175 241.413844 
L 286.532111 239.745939 
L 286.61158 241.186223 
L 286.691048 240.318579 
L 286.770516 240.996459 
L 286.849985 240.325314 
L 286.929453 240.514553 
L 287.008921 236.619366 
L 287.167858 241.105488 
L 287.247326 239.493793 
L 287.406263 240.866306 
L 287.485731 240.64027 
L 287.5652 239.888889 
L 287.644668 234.644838 
L 287.724136 241.3612 
L 287.803604 240.816295 
L 287.962541 235.203226 
L 288.042009 239.745648 
L 288.121478 238.044085 
L 288.200946 241.509878 
L 288.280414 238.173166 
L 288.439351 241.27703 
L 288.598288 238.09916 
L 288.757224 240.868438 
L 288.836693 239.712187 
L 288.916161 240.755833 
L 288.995629 238.229362 
L 289.075097 230.806934 
L 289.154566 240.719633 
L 289.234034 240.030983 
L 289.313502 238.81774 
L 289.392971 240.052401 
L 289.472439 238.62997 
L 289.631376 241.207964 
L 289.710844 240.475003 
L 289.790312 241.490821 
L 289.949249 239.844467 
L 290.028717 241.261794 
L 290.108186 240.380748 
L 290.187654 241.073399 
L 290.267122 240.419323 
L 290.34659 240.594717 
L 290.426059 236.73532 
L 290.584995 241.166815 
L 290.664464 239.621829 
L 290.8234 240.939253 
L 290.902869 240.707962 
L 290.982337 239.965966 
L 291.061805 234.793706 
L 291.141274 241.425169 
L 291.220742 240.896287 
L 291.379679 235.30395 
L 291.459147 239.814017 
L 291.538615 238.154074 
L 291.618083 241.57383 
L 291.697552 238.346916 
L 291.856488 241.346187 
L 292.015425 238.199589 
L 292.174362 240.939928 
L 292.25383 239.791096 
L 292.333298 240.834056 
L 292.412767 238.338143 
L 292.492235 230.892986 
L 292.571703 240.804102 
L 292.651172 240.084804 
L 292.73064 238.945939 
L 292.810108 240.134726 
L 292.889576 238.753625 
L 293.048513 241.271606 
L 293.127981 240.555507 
L 293.20745 241.5643 
L 293.366386 239.939505 
L 293.445855 241.333508 
L 293.525323 240.438846 
L 293.604791 241.147069 
L 293.68426 240.508591 
L 293.763728 240.67128 
L 293.843196 236.849442 
L 294.002133 241.225626 
L 294.081601 239.741417 
L 294.240538 241.009401 
L 294.320006 240.771802 
L 294.399474 240.040536 
L 294.478943 234.940839 
L 294.558411 241.486144 
L 294.637879 240.972749 
L 294.796816 235.400487 
L 294.876284 239.881275 
L 294.955753 238.259318 
L 295.035221 241.634238 
L 295.114689 238.519791 
L 295.273626 241.411092 
L 295.432562 238.296193 
L 295.591499 241.00951 
L 295.670967 239.86782 
L 295.750436 240.909914 
L 295.829904 238.44349 
L 295.909372 230.977511 
L 295.988841 240.882956 
L 296.068309 240.135341 
L 296.147777 239.071009 
L 296.227246 240.211171 
L 296.306714 238.874631 
L 296.465651 241.332488 
L 296.545119 240.634075 
L 296.624587 241.634236 
L 296.704055 241.166581 
L 296.783524 240.031079 
L 296.862992 241.401776 
L 296.94246 240.49356 
L 297.021929 241.217669 
L 297.101397 240.593583 
L 297.180865 240.744663 
L 297.260334 236.961935 
L 297.41927 241.282433 
L 297.498739 239.851659 
L 297.657675 241.076531 
L 297.737144 240.832414 
L 297.816612 240.112801 
L 297.89608 235.086266 
L 297.975548 241.544574 
L 298.055017 241.046114 
L 298.213953 235.493152 
L 298.293422 239.948529 
L 298.37289 238.360354 
L 298.452358 241.691193 
L 298.531827 238.691322 
L 298.690763 241.471846 
L 298.8497 238.389184 
L 299.008637 241.077071 
L 299.088105 239.943064 
L 299.167573 240.983339 
L 299.247041 238.545079 
L 299.32651 231.060965 
L 299.405978 240.957109 
L 299.485446 240.184806 
L 299.564915 239.192696 
L 299.644383 240.283026 
L 299.723851 238.993014 
L 299.882788 241.391165 
L 299.962256 240.710433 
L 300.041725 241.700732 
L 300.121193 241.248465 
L 300.200661 240.119199 
L 300.28013 241.466903 
L 300.359598 240.545756 
L 300.439066 241.285436 
L 300.518534 240.674742 
L 300.598003 240.815138 
L 300.677471 237.073057 
L 300.756939 241.353946 
L 300.836408 241.337656 
L 300.915876 239.95264 
L 301.074813 241.140579 
L 301.154281 240.890401 
L 301.233749 240.182931 
L 301.313218 235.229471 
L 301.392686 241.600739 
L 301.472154 241.116497 
L 301.631091 235.582245 
L 301.710559 240.016385 
L 301.790027 238.457616 
L 301.869496 241.744837 
L 301.948964 238.859896 
L 302.107901 241.528702 
L 302.266837 238.478621 
L 302.425774 241.142546 
L 302.505242 240.016448 
L 302.584711 241.05413 
L 302.664179 238.642206 
L 302.743647 231.143684 
L 302.823116 241.027404 
L 302.902584 240.23551 
L 302.982052 239.310768 
L 303.06152 240.351069 
L 303.140989 239.108765 
L 303.299925 241.448337 
L 303.379394 240.784353 
L 303.458862 241.763899 
L 303.53833 241.328513 
L 303.617799 240.203855 
L 303.697267 241.529118 
L 303.776735 240.596499 
L 303.856204 241.350615 
L 303.935672 240.752454 
L 304.01514 240.882871 
L 304.094609 237.183044 
L 304.174077 241.42849 
L 304.253545 241.391564 
L 304.333013 240.045355 
L 304.49195 241.20162 
L 304.571418 240.946237 
L 304.650887 240.251057 
L 304.730355 235.36924 
L 304.809823 241.654736 
L 304.889292 241.183781 
L 305.048228 235.668138 
L 305.127697 240.084895 
L 305.207165 238.551443 
L 305.286633 241.795349 
L 305.366102 239.022635 
L 305.525038 241.58206 
L 305.683975 238.564602 
L 305.842911 241.205921 
L 305.92238 240.087175 
L 306.001848 241.122078 
L 306.081316 238.734351 
L 306.160785 231.22585 
L 306.240253 241.094494 
L 306.319721 240.289162 
L 306.39919 239.425037 
L 306.478658 240.41582 
L 306.558126 239.221829 
L 306.717063 241.504625 
L 306.796531 240.855661 
L 306.875999 241.823836 
L 306.955468 241.406191 
L 307.034936 240.285015 
L 307.114404 241.588585 
L 307.193873 240.646906 
L 307.273341 241.413416 
L 307.352809 240.827037 
L 307.432278 240.947965 
L 307.511746 237.291947 
L 307.591214 241.49755 
L 307.670683 241.444255 
L 307.750151 240.131257 
L 307.909087 241.259827 
L 307.988556 241.000245 
L 308.068024 240.317258 
L 308.147492 235.504039 
L 308.226961 241.706538 
L 308.306429 241.247833 
L 308.465366 235.751319 
L 308.544834 240.153643 
L 308.624302 238.642086 
L 308.703771 241.842949 
L 308.783239 239.176307 
L 308.942176 241.632401 
L 309.101112 238.647384 
L 309.260049 241.267221 
L 309.339517 240.154764 
L 309.418985 241.18712 
L 309.498454 238.82162 
L 309.577922 231.307502 
L 309.65739 241.158814 
L 309.736859 240.346263 
L 309.816327 239.535447 
L 309.895795 240.477716 
L 309.975264 239.332094 
L 310.1342 241.56037 
L 310.213669 240.924278 
L 310.293137 241.880658 
L 310.372605 241.480912 
L 310.452073 240.362659 
L 310.531542 241.645413 
L 310.61101 240.697857 
L 310.690478 241.473978 
L 310.769947 240.898741 
L 310.849415 241.010485 
L 310.928883 237.399522 
L 311.008352 241.561957 
L 311.08782 241.495697 
L 311.167288 240.211752 
L 311.326225 241.315421 
L 311.405693 241.052649 
L 311.485162 240.381576 
L 311.56463 235.632663 
L 311.644098 241.756095 
L 311.723566 241.308696 
L 311.882503 235.832342 
L 311.961971 240.221984 
L 312.04144 238.729719 
L 312.120908 241.887893 
L 312.200376 239.318583 
L 312.359313 241.680174 
L 312.51825 238.727313 
L 312.677186 241.326467 
L 312.756655 240.219232 
L 312.836123 241.249385 
L 312.915591 238.904653 
L 312.995059 231.38862 
L 313.074528 241.220645 
L 313.153996 240.406186 
L 313.233464 239.642066 
L 313.312933 240.537164 
L 313.392401 239.439429 
L 313.551338 241.615589 
L 313.630806 240.990223 
L 313.710274 241.934528 
L 313.789743 241.552149 
L 313.869211 240.436813 
L 313.948679 241.699673 
L 314.028148 240.749722 
L 314.107616 241.532369 
L 314.187084 240.967764 
L 314.266552 241.070495 
L 314.346021 237.505284 
L 314.425489 241.622644 
L 314.504957 241.5458 
L 314.584426 240.287921 
L 314.743362 241.368627 
L 314.822831 241.10363 
L 314.902299 240.444035 
L 314.981767 235.754623 
L 315.061236 241.803408 
L 315.140704 241.366618 
L 315.299641 235.911723 
L 315.379109 240.289303 
L 315.458577 238.814461 
L 315.538045 241.930455 
L 315.617514 239.448671 
L 315.77645 241.725744 
L 315.935387 238.804716 
L 316.094324 241.383665 
L 316.173792 240.280849 
L 316.25326 241.309114 
L 316.332729 238.984227 
L 316.412197 231.469207 
L 316.491665 241.280181 
L 316.571134 240.467789 
L 316.650602 239.745025 
L 316.73007 240.594503 
L 316.809538 239.543741 
L 316.968475 241.67007 
L 317.047943 241.053587 
L 317.127412 241.985641 
L 317.20688 241.619536 
L 317.286348 240.507565 
L 317.365817 241.751437 
L 317.445285 240.802318 
L 317.524753 241.588608 
L 317.604222 241.034267 
L 317.68369 241.128073 
L 317.763158 237.608676 
L 317.842627 241.680337 
L 317.922095 241.59448 
L 318.001563 240.360498 
L 318.1605 241.419642 
L 318.319436 240.504656 
L 318.398905 235.870088 
L 318.478373 241.848548 
L 318.557841 241.421938 
L 318.716778 235.989862 
L 318.796246 240.355145 
L 318.875715 238.896395 
L 318.955183 241.970891 
L 319.034651 239.567107 
L 319.193588 241.769388 
L 319.352524 238.879845 
L 319.511461 241.438821 
L 319.590929 240.339925 
L 319.670398 241.366566 
L 319.749866 239.060969 
L 319.829334 231.549312 
L 319.908803 241.337574 
L 319.988271 240.529967 
L 320.067739 239.844464 
L 320.147208 240.649969 
L 320.226676 239.645004 
L 320.385613 241.723518 
L 320.465081 241.114488 
L 320.544549 242.034205 
L 320.624017 241.682922 
L 320.703486 240.575042 
L 320.782954 241.800804 
L 320.862422 240.855106 
L 320.941891 241.642704 
L 321.021359 241.098389 
L 321.100827 241.183319 
L 321.180296 237.709239 
L 321.259764 241.735497 
L 321.339232 241.641688 
L 321.418701 240.429962 
L 321.577637 241.468627 
L 321.736574 240.563472 
L 321.816042 235.97961 
L 321.89551 241.891635 
L 321.974979 241.474979 
L 322.133915 236.067021 
L 322.213384 240.419232 
L 322.292852 238.975591 
L 322.37232 242.009426 
L 322.451789 239.675156 
L 322.610725 241.811314 
L 322.769662 238.952875 
L 322.928599 241.491972 
L 323.008067 240.396719 
L 323.087535 241.421968 
L 323.167003 239.135285 
L 323.246472 231.629024 
L 323.32594 241.392954 
L 323.405408 240.591894 
L 323.484877 239.940522 
L 323.564345 240.703701 
L 323.643813 239.743259 
L 323.80275 241.775658 
L 323.882218 241.173046 
L 323.961687 242.080416 
L 324.041155 241.742359 
L 324.120623 240.639384 
L 324.200092 241.8479 
L 324.27956 240.907462 
L 324.359028 241.694693 
L 324.438496 241.160258 
L 324.517965 241.236343 
L 324.597433 237.806697 
L 324.676901 241.788398 
L 324.75637 241.687405 
L 324.835838 240.496651 
L 324.994775 241.515713 
L 325.153711 240.620531 
L 325.23318 236.083869 
L 325.312648 241.932812 
L 325.392116 241.525993 
L 325.551053 236.143345 
L 325.630521 240.481416 
L 325.709989 239.052116 
L 325.789458 242.046246 
L 325.868926 239.774298 
L 326.027863 241.851688 
L 326.186799 239.023927 
L 326.345736 241.543178 
L 326.425204 240.451431 
L 326.504673 241.475495 
L 326.584141 239.207405 
L 326.663609 231.708443 
L 326.743078 241.446432 
L 326.822546 240.653032 
L 326.902014 240.033342 
L 326.981482 240.755784 
L 327.060951 239.838596 
L 327.219887 241.826271 
L 327.299356 241.229375 
L 327.378824 242.124455 
L 327.458292 241.798046 
L 327.537761 240.70073 
L 327.617229 241.892869 
L 327.696697 240.958866 
L 327.776166 241.744637 
L 327.855634 241.21999 
L 327.935102 241.287258 
L 328.014571 237.900947 
L 328.094039 241.839206 
L 328.173507 241.731638 
L 328.252975 240.56082 
L 328.411912 241.561011 
L 328.570849 240.675892 
L 328.650317 236.183522 
L 328.729785 241.972221 
L 328.809254 241.575162 
L 328.96819 236.218892 
L 329.047659 240.541637 
L 329.127127 239.126046 
L 329.206595 242.0815 
L 329.286064 239.865938 
L 329.445 241.890646 
L 329.603937 239.093086 
L 329.762873 241.592522 
L 329.842342 240.50422 
L 329.92181 241.527279 
L 330.001278 239.277447 
L 330.080747 231.787673 
L 330.160215 241.498114 
L 330.239683 240.713062 
L 330.319152 240.123082 
L 330.39862 240.806272 
L 330.478088 239.931141 
L 330.637025 241.875215 
L 330.716493 241.283575 
L 330.795961 242.166484 
L 330.87543 241.85027 
L 330.954898 240.759207 
L 331.034366 241.93586 
L 331.113835 241.008966 
L 331.193303 241.792622 
L 331.272771 241.277693 
L 331.35224 241.336171 
L 331.431708 237.992018 
L 331.511176 241.888031 
L 331.590645 241.774412 
L 331.670113 240.62268 
L 331.82905 241.604619 
L 331.987986 240.729617 
L 332.067454 236.279145 
L 332.146923 242.010001 
L 332.226391 241.622612 
L 332.385328 236.293676 
L 332.464796 240.599895 
L 332.544264 239.197458 
L 332.623733 242.115315 
L 332.703201 239.951288 
L 332.862138 241.928304 
L 333.021074 239.160418 
L 333.180011 241.640096 
L 333.259479 240.555214 
L 333.338947 241.577418 
L 333.418416 239.345476 
L 333.497884 231.866817 
L 333.577352 241.5481 
L 333.656821 240.771811 
L 333.736289 240.209907 
L 333.815757 240.85521 
L 333.895226 240.021038 
L 334.054162 241.922404 
L 334.133631 241.335739 
L 334.213099 242.206651 
L 334.292567 241.899345 
L 334.372036 240.814928 
L 334.451504 241.977017 
L 334.530972 241.057568 
L 334.61044 241.838745 
L 334.689909 241.33347 
L 334.769377 241.383182 
L 334.848845 238.080023 
L 334.928314 241.934959 
L 335.007782 241.815759 
L 335.08725 240.682419 
L 335.246187 241.646627 
L 335.405124 240.781771 
L 335.484592 236.37122 
L 335.56406 242.046276 
L 335.643529 241.668435 
L 335.802465 236.367685 
L 335.881933 240.656224 
L 335.961402 239.266439 
L 336.04087 242.147797 
L 336.120338 240.031345 
L 336.279275 241.964758 
L 336.438212 239.225982 
L 336.597148 241.68599 
L 336.676617 240.604517 
L 336.756085 241.625988 
L 336.835553 239.411532 
L 336.915022 231.945981 
L 336.99449 241.596479 
L 337.073958 240.829201 
L 337.153426 240.293986 
L 337.232895 240.902643 
L 337.312363 240.108437 
L 337.4713 241.967808 
L 337.550768 241.385953 
L 337.630236 242.245091 
L 337.709705 241.945575 
L 337.789173 240.867993 
L 337.868641 242.016472 
L 337.94811 241.104591 
L 338.027578 241.883102 
L 338.107046 241.387413 
L 338.186515 241.428383 
L 338.265983 238.16512 
L 338.345451 241.980068 
L 338.424919 241.855722 
L 338.504388 240.740204 
L 338.663324 241.68712 
L 338.822261 240.832416 
L 338.901729 236.460148 
L 338.981198 242.081157 
L 339.060666 241.712706 
L 339.219603 236.440898 
L 339.299071 240.710681 
L 339.378539 239.333077 
L 339.458008 242.179039 
L 339.537476 240.106913 
L 339.696412 242.00009 
L 339.855349 239.289831 
L 340.014286 241.730294 
L 340.093754 240.652214 
L 340.173222 241.673051 
L 340.252691 239.475647 
L 340.332159 232.025287 
L 340.411627 241.643335 
L 340.491096 240.885214 
L 340.570564 240.375485 
L 340.650032 240.948617 
L 340.729501 240.193478 
L 340.888437 242.011432 
L 340.967905 241.434295 
L 341.047374 242.281928 
L 341.126842 241.989239 
L 341.20631 240.918486 
L 341.285779 242.054346 
L 341.365247 241.15003 
L 341.444715 241.925782 
L 341.524184 241.439609 
L 341.603652 241.471858 
L 341.68312 238.247487 
L 341.762589 242.02343 
L 341.842057 241.894344 
L 341.921525 240.796186 
L 342.080462 241.72618 
L 342.239398 240.881611 
L 342.318867 236.54626 
L 342.398335 242.114744 
L 342.477803 241.75549 
L 342.63674 236.513299 
L 342.716208 240.763335 
L 342.795677 239.397462 
L 342.875145 242.209124 
L 342.954613 240.178634 
L 343.11355 242.034369 
L 343.272487 239.352023 
L 343.431423 241.773091 
L 343.510891 240.698381 
L 343.59036 241.71866 
L 343.669828 239.537853 
L 343.749296 232.104879 
L 343.828765 241.688743 
L 343.908233 240.939873 
L 343.987701 240.454567 
L 344.06717 240.993181 
L 344.146638 240.276284 
L 344.305575 242.053308 
L 344.385043 241.480841 
L 344.464511 242.317276 
L 344.543979 242.030579 
L 344.623448 240.966474 
L 344.702916 242.090749 
L 344.782384 241.193919 
L 344.861853 241.966867 
L 344.941321 241.490135 
L 345.020789 241.513683 
L 345.100258 238.327309 
L 345.179726 242.065117 
L 345.259194 241.931671 
L 345.338663 240.850503 
L 345.497599 241.763886 
L 345.656536 240.92941 
L 345.736004 236.62983 
L 345.815472 242.147126 
L 345.894941 241.796848 
L 346.053877 236.584872 
L 346.133346 240.814266 
L 346.212814 239.459682 
L 346.292282 242.238129 
L 346.371751 240.247021 
L 346.530687 242.067653 
L 346.689624 239.412616 
L 346.848561 241.814458 
L 346.928029 240.74308 
L 347.007497 241.762864 
L 347.086965 239.598184 
L 347.166434 232.184941 
L 347.245902 241.732767 
L 347.32537 240.99323 
L 347.404839 240.531378 
L 347.484307 241.036381 
L 347.563775 240.356948 
L 347.722712 242.093487 
L 347.80218 241.525664 
L 347.881649 242.35124 
L 347.961117 242.069802 
L 348.040585 241.012002 
L 348.120054 242.125776 
L 348.199522 241.236316 
L 348.27899 242.006428 
L 348.358458 241.539062 
L 348.437927 241.553929 
L 348.517395 238.404768 
L 348.596863 242.105199 
L 348.676332 241.967749 
L 348.7558 240.903276 
L 348.914737 241.800313 
L 349.073673 240.975862 
L 349.153142 236.711093 
L 349.23261 242.178383 
L 349.312078 241.836838 
L 349.471015 236.655612 
L 349.550483 240.863551 
L 349.629951 239.519823 
L 349.70942 242.266119 
L 349.788888 240.312489 
L 349.947825 242.099993 
L 350.106761 239.47167 
L 350.265698 241.854465 
L 350.345166 240.786364 
L 350.424635 241.805705 
L 350.504103 239.656671 
L 350.583571 232.265712 
L 350.66304 241.775463 
L 350.742508 241.045359 
L 350.821976 240.606054 
L 350.901444 241.078264 
L 350.980913 240.435513 
L 351.139849 242.132032 
L 351.219318 241.568834 
L 351.298786 242.383919 
L 351.378254 242.107077 
L 351.457723 241.055084 
L 351.537191 242.159518 
L 351.616659 241.277289 
L 351.696128 242.044524 
L 351.855064 241.592659 
L 351.934533 238.480043 
L 352.014001 242.143742 
L 352.093469 242.002623 
L 352.172937 240.954617 
L 352.331874 241.835533 
L 352.490811 241.021012 
L 352.570279 236.79025 
L 352.649747 242.208587 
L 352.729216 241.87552 
L 352.888152 236.725516 
L 352.967621 240.911271 
L 353.047089 239.577967 
L 353.126557 242.293158 
L 353.206026 240.375374 
L 353.364962 242.131433 
L 353.44443 241.372633 
L 353.523899 239.529249 
L 353.682835 241.893177 
L 353.762304 240.828277 
L 353.841772 241.847227 
L 353.92124 239.713341 
L 354.000709 232.347508 
L 354.080177 241.816877 
L 354.159645 241.096353 
L 354.239114 240.678711 
L 354.318582 241.118874 
L 354.39805 240.511959 
L 354.556987 242.16901 
L 354.636455 241.610421 
L 354.715923 242.415408 
L 354.795392 242.142547 
L 354.87486 241.095696 
L 354.954328 242.192056 
L 355.033797 241.316908 
L 355.113265 242.081208 
L 355.272202 241.629931 
L 355.35167 238.553305 
L 355.431138 242.180808 
L 355.510607 242.036336 
L 355.590075 241.004623 
L 355.749012 241.869617 
L 355.907948 241.064896 
L 355.987416 236.86748 
L 356.066885 242.237804 
L 356.146353 241.912952 
L 356.30529 236.79459 
L 356.384758 240.957499 
L 356.464226 239.634187 
L 356.543695 242.319299 
L 356.623163 240.435954 
L 356.7821 242.162011 
L 356.861568 241.422238 
L 356.941036 239.585422 
L 357.099973 241.930651 
L 357.179441 240.868854 
L 357.258909 241.887469 
L 357.338378 239.768215 
L 357.417846 232.430753 
L 357.497314 241.857046 
L 357.576783 241.146327 
L 357.656251 240.74944 
L 357.735719 241.158252 
L 357.815188 240.586177 
L 357.974124 242.204492 
L 358.053593 241.65049 
L 358.133061 242.445792 
L 358.212529 242.176322 
L 358.291998 241.133759 
L 358.371466 242.223465 
L 358.450934 241.355242 
L 358.530402 242.11652 
L 358.689339 241.665795 
L 358.768807 238.62472 
L 358.848276 242.216457 
L 358.927744 242.068926 
L 359.007212 241.053385 
L 359.166149 241.902632 
L 359.325086 241.107546 
L 359.404554 236.942942 
L 359.484022 242.266094 
L 359.563491 241.949193 
L 359.722427 236.862845 
L 359.801895 241.002302 
L 359.881364 239.688552 
L 359.960832 242.344591 
L 360.0403 240.494463 
L 360.199237 242.191759 
L 360.278705 241.470081 
L 360.358174 239.640261 
L 360.51711 241.966942 
L 360.596579 240.908122 
L 360.676047 241.926467 
L 360.755515 239.821297 
L 360.834984 232.516027 
L 360.914452 241.895994 
L 360.99392 241.195422 
L 361.073388 240.818307 
L 361.152857 241.196435 
L 361.232325 240.657934 
L 361.391262 242.238545 
L 361.47073 241.689107 
L 361.550198 242.475158 
L 361.629667 242.208486 
L 361.709135 241.169115 
L 361.788603 242.25382 
L 361.868072 241.392354 
L 361.94754 242.150494 
L 362.106477 241.700298 
L 362.185945 238.694452 
L 362.265413 242.25074 
L 362.344881 242.10043 
L 362.42435 241.100983 
L 362.583286 241.934646 
L 362.742223 241.148986 
L 362.821691 237.016784 
L 362.90116 242.293511 
L 362.980628 241.984301 
L 363.139565 236.9303 
L 363.219033 241.045737 
L 363.298501 239.741119 
L 363.37797 242.369075 
L 363.457438 240.5511 
L 363.616374 242.220709 
L 363.695843 241.516306 
L 363.775311 239.693845 
L 363.934248 242.002095 
L 364.013716 240.946092 
L 364.093184 241.964255 
L 364.172653 239.872572 
L 364.252121 232.604129 
L 364.331589 241.933735 
L 364.411058 241.243806 
L 364.490526 240.885335 
L 364.569994 241.233458 
L 364.649463 240.726825 
L 364.808399 242.271234 
L 364.887867 241.726329 
L 364.967336 242.503585 
L 365.046804 242.239093 
L 365.126272 241.201489 
L 365.205741 242.283194 
L 365.285209 241.428306 
L 365.364677 242.183154 
L 365.523614 241.73348 
L 365.603082 238.762665 
L 365.682551 242.283702 
L 365.762019 242.130875 
L 365.841487 241.147492 
L 366.000424 241.965724 
L 366.15936 241.189234 
L 366.238829 237.089149 
L 366.318297 242.320106 
L 366.397765 242.018334 
L 366.556702 236.99698 
L 366.63617 241.087841 
L 366.715639 239.791931 
L 366.795107 242.392784 
L 366.874575 240.606038 
L 367.033512 242.248885 
L 367.11298 241.561052 
L 367.192449 239.746262 
L 367.351385 242.036156 
L 367.430853 240.982758 
L 367.510322 242.000864 
L 367.58979 239.921991 
L 367.669258 232.696187 
L 367.748727 241.970264 
L 367.828195 241.291693 
L 367.907663 240.950488 
L 367.987132 241.269349 
L 368.0666 240.792193 
L 368.225537 242.302616 
L 368.305005 241.76221 
L 368.384473 242.531154 
L 368.463942 242.268166 
L 368.54341 241.230431 
L 368.622878 242.311665 
L 368.702346 241.463154 
L 368.781815 242.214522 
L 368.940751 241.76538 
L 369.02022 238.829528 
L 369.099688 242.315379 
L 369.179156 242.160282 
L 369.258625 241.192983 
L 369.417561 241.995928 
L 369.576498 241.228293 
L 369.655966 237.160173 
L 369.735435 242.345924 
L 369.814903 242.051349 
L 369.973839 237.062922 
L 370.053308 241.128626 
L 370.132776 239.841016 
L 370.212244 242.415736 
L 370.291713 240.659432 
L 370.450649 242.27631 
L 370.530118 241.604458 
L 370.609586 239.797609 
L 370.768523 242.069162 
L 370.847991 241.018089 
L 370.927459 242.036321 
L 371.006928 239.969459 
L 371.086396 232.793829 
L 371.165864 242.005554 
L 371.245332 241.339342 
L 371.324801 241.013643 
L 371.404269 241.304136 
L 371.483737 240.853008 
L 371.642674 242.332739 
L 371.722142 241.796788 
L 371.801611 242.557946 
L 371.881079 242.295689 
L 371.960547 241.255217 
L 372.040016 242.339318 
L 372.119484 241.496949 
L 372.198952 242.244617 
L 372.357889 241.796039 
L 372.437357 238.895217 
L 372.516825 242.345791 
L 372.596294 242.18866 
L 372.675762 241.237527 
L 372.834699 242.025322 
L 372.993635 241.266161 
L 373.073104 237.229999 
L 373.152572 242.371003 
L 373.23204 242.0834 
L 373.390977 237.128176 
L 373.470445 241.168058 
L 373.549914 239.888375 
L 373.629382 242.437934 
L 373.70885 240.711419 
L 373.867787 242.303004 
L 373.947255 241.646679 
L 374.026723 239.847999 
L 374.18566 242.101146 
L 374.265128 241.052017 
L 374.344597 242.070646 
L 374.424065 240.014808 
L 374.503533 232.899482 
L 374.583002 242.039543 
L 374.66247 241.387061 
L 374.741938 241.074546 
L 374.821407 241.337843 
L 374.900875 240.907652 
L 375.059811 242.36164 
L 375.13928 241.830081 
L 375.218748 242.584042 
L 375.298216 242.321596 
L 375.377685 241.274711 
L 375.457153 242.366252 
L 375.536621 241.529739 
L 375.61609 242.273464 
L 375.775026 241.825512 
L 375.854495 238.959923 
L 375.933963 242.374942 
L 376.013431 242.216003 
L 376.0929 241.281195 
L 376.251836 242.053963 
L 376.331304 241.839704 
L 376.410773 241.30282 
L 376.490241 237.298771 
L 376.569709 242.395378 
L 376.649178 242.114539 
L 376.808114 237.192816 
L 376.887583 241.206034 
L 376.967051 239.933974 
L 377.046519 242.459348 
L 377.125988 240.762123 
L 377.284924 242.328981 
L 377.364393 241.6879 
L 377.443861 239.897565 
L 377.602797 242.132137 
L 377.682266 241.084417 
L 377.761734 242.103854 
L 377.841202 240.057762 
L 377.920671 233.01688 
L 378.000139 242.072109 
L 378.079607 241.435157 
L 378.159076 241.132751 
L 378.238544 241.370496 
L 378.318012 240.953559 
L 378.476949 242.389338 
L 378.556417 241.862079 
L 378.635886 242.609533 
L 378.715354 242.345764 
L 378.794822 241.287162 
L 378.87429 242.392582 
L 378.953759 241.561562 
L 379.033227 242.301094 
L 379.192164 241.853888 
L 379.271632 239.023862 
L 379.3511 242.402802 
L 379.430569 242.242285 
L 379.510037 241.324065 
L 379.668974 242.081909 
L 379.748442 241.870903 
L 379.82791 241.338237 
L 379.907379 237.366648 
L 379.986847 242.419073 
L 380.066315 242.144812 
L 380.225252 237.256959 
L 380.30472 241.242318 
L 380.384188 239.977717 
L 380.463657 242.479894 
L 380.543125 240.811659 
L 380.543125 240.811659 
" style="fill:none;stroke:#000000;stroke-linecap:round;stroke-width:0.5;">
         </path>
        </g>
        <g id="line2d_13">
         <path clip-path="url(#p5125e43100)" d="M 49.159452 41.764891 
L 52.575778 43.417776 
L 55.992105 45.775029 
L 59.408431 49.372344 
L 62.824758 54.872293 
L 66.241084 62.597887 
L 69.657411 72.268023 
L 73.073737 83.163313 
L 76.490064 94.754866 
L 79.90639 106.802321 
L 83.322717 119.07924 
L 86.739043 131.154448 
L 90.15537 142.477896 
L 93.571696 152.772704 
L 96.988023 161.977652 
L 100.404349 170.11768 
L 103.820676 177.323263 
L 107.237003 183.840303 
L 110.653329 189.652863 
L 114.069656 194.780197 
L 117.485982 199.410413 
L 120.902309 203.254713 
L 124.318635 206.488322 
L 127.734962 209.403346 
L 131.151288 212.204002 
L 134.567615 214.457343 
L 137.983941 216.309756 
L 141.400268 217.937852 
L 144.816594 219.372283 
L 148.232921 220.668975 
L 151.649247 221.908605 
L 155.065574 223.079525 
L 158.481901 224.196673 
L 161.898227 225.234707 
L 165.314554 226.164024 
L 168.73088 227.151629 
L 172.147207 227.937626 
L 175.563533 228.689197 
L 178.97986 229.369972 
L 182.396186 229.980852 
L 185.812513 230.540299 
L 189.228839 230.980753 
L 192.645166 231.294251 
L 196.061492 231.616158 
L 199.477819 231.888289 
L 202.894145 232.181487 
L 206.310472 232.585594 
L 209.726798 233.109655 
L 213.143125 233.666353 
L 216.559452 234.191825 
L 219.975778 234.670483 
L 223.392105 235.100262 
L 226.808431 235.483171 
L 230.224758 235.823582 
L 233.641084 236.126442 
L 237.057411 236.401663 
L 240.473737 236.661993 
L 243.890064 236.90706 
L 247.30639 237.133343 
L 250.722717 237.342253 
L 254.139043 237.536743 
L 257.55537 237.719543 
L 260.971696 237.892783 
L 264.388023 238.057971 
L 267.804349 238.216121 
L 271.220676 238.367953 
L 274.637003 238.514005 
L 278.053329 238.654535 
L 281.469656 238.789426 
L 284.885982 238.918351 
L 288.302309 239.041135 
L 291.718635 239.157989 
L 295.134962 239.269435 
L 298.551288 239.376054 
L 301.967615 239.478272 
L 305.383941 239.576306 
L 308.800268 239.67025 
L 312.216594 239.760219 
L 315.632921 239.846416 
L 319.049247 239.929106 
L 322.465574 240.008576 
L 325.881901 240.085098 
L 329.298227 240.158918 
L 332.714554 240.23025 
L 336.13088 240.299282 
L 339.547207 240.366181 
L 342.963533 240.431093 
L 346.37986 240.494153 
L 349.796186 240.555484 
L 353.212513 240.615202 
L 356.628839 240.673418 
L 360.045166 240.730243 
L 363.461492 240.785788 
L 366.877819 240.840168 
L 370.294145 240.89351 
L 373.710472 240.945949 
L 377.126798 240.997638 
L 380.543125 241.048741 
" style="fill:none;stroke:#ff0000;stroke-linecap:round;stroke-width:2;">
         </path>
        </g>
        <g id="line2d_14">
         <path clip-path="url(#p5125e43100)" d="M 49.159452 41.209695 
L 52.575778 42.890848 
L 55.992105 45.355749 
L 59.408431 49.152001 
L 62.824758 54.974952 
L 66.241084 63.055309 
L 69.657411 72.984859 
L 73.073737 83.899051 
L 76.490064 95.181861 
L 79.90639 106.635273 
L 83.322717 118.125288 
L 86.739043 129.279386 
L 90.15537 139.64467 
L 93.571696 149.020147 
L 96.988023 157.373597 
L 100.404349 164.649217 
L 103.820676 170.984992 
L 107.237003 176.680784 
L 110.653329 181.665327 
L 114.069656 185.988093 
L 117.485982 189.887406 
L 120.902309 193.035434 
L 124.318635 195.639183 
L 127.734962 198.074281 
L 131.151288 200.543379 
L 134.567615 202.444033 
L 137.983941 204.011561 
L 141.400268 205.491729 
L 144.816594 206.832246 
L 148.232921 207.98743 
L 151.649247 209.07492 
L 155.065574 210.07788 
L 158.481901 211.0634 
L 161.898227 211.923716 
L 165.314554 212.529619 
L 168.73088 213.298895 
L 172.147207 213.790769 
L 175.563533 214.25371 
L 178.97986 214.611004 
L 182.396186 214.890907 
L 185.812513 215.192598 
L 189.228839 215.390902 
L 192.645166 215.472257 
L 196.061492 215.716162 
L 199.477819 215.935257 
L 202.894145 216.164758 
L 206.310472 216.417773 
L 209.726798 216.693192 
L 213.143125 216.971745 
L 216.559452 217.243609 
L 219.975778 217.509629 
L 223.392105 217.771594 
L 226.808431 218.029316 
L 230.224758 218.281445 
L 233.641084 218.526595 
L 237.057411 218.763297 
L 240.473737 218.985347 
L 243.890064 219.185522 
L 247.30639 219.363882 
L 250.722717 219.523334 
L 254.139043 219.666509 
L 257.55537 219.794894 
L 260.971696 219.908968 
L 264.388023 220.009123 
L 267.804349 220.096723 
L 271.220676 220.174326 
L 274.637003 220.244982 
L 278.053329 220.311365 
L 281.469656 220.375268 
L 284.885982 220.437543 
L 288.302309 220.498381 
L 291.718635 220.557642 
L 295.134962 220.614958 
L 298.551288 220.669687 
L 301.967615 220.720984 
L 305.383941 220.768088 
L 308.800268 220.810651 
L 312.216594 220.848861 
L 315.632921 220.883274 
L 319.049247 220.91454 
L 322.465574 220.943218 
L 325.881901 220.969725 
L 329.298227 220.994346 
L 332.714554 221.017274 
L 336.13088 221.03863 
L 339.547207 221.058489 
L 342.963533 221.076885 
L 346.37986 221.093824 
L 349.796186 221.109277 
L 353.212513 221.123189 
L 356.628839 221.135472 
L 360.045166 221.146004 
L 363.461492 221.154627 
L 366.877819 221.161144 
L 370.294145 221.165316 
L 373.710472 221.16685 
L 377.126798 221.165394 
L 380.543125 221.160496 
" style="fill:none;stroke:#0000ff;stroke-linecap:round;stroke-width:3;">
         </path>
        </g>
        <g id="patch_3">
         <path d="M 45.743125 244.078125 
L 45.743125 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_4">
         <path d="M 380.543125 244.078125 
L 380.543125 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_5">
         <path d="M 45.743125 244.078125 
L 380.543125 244.078125 
" style="fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_6">
         <path d="M 45.743125 22.318125 
L 380.543125 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="text_14">
         <!-- Decrease of cost over backprop iteration -->
         <g style="fill:#262626;" transform="translate(90.64 16.318125)scale(0.12 -0.12)">
          <defs>
           <path d="M 1259 4147 
L 1259 519 
L 2022 519 
Q 2988 519 3436 956 
Q 3884 1394 3884 2338 
Q 3884 3275 3436 3711 
Q 2988 4147 2022 4147 
L 1259 4147 
z
M 628 4666 
L 1925 4666 
Q 3281 4666 3915 4102 
Q 4550 3538 4550 2338 
Q 4550 1131 3912 565 
Q 3275 0 1925 0 
L 628 0 
L 628 4666 
z
" id="DejaVuSans-44" transform="scale(0.015625)">
           </path>
           <path d="M 3122 3366 
L 3122 2828 
Q 2878 2963 2633 3030 
Q 2388 3097 2138 3097 
Q 1578 3097 1268 2742 
Q 959 2388 959 1747 
Q 959 1106 1268 751 
Q 1578 397 2138 397 
Q 2388 397 2633 464 
Q 2878 531 3122 666 
L 3122 134 
Q 2881 22 2623 -34 
Q 2366 -91 2075 -91 
Q 1284 -91 818 406 
Q 353 903 353 1747 
Q 353 2603 823 3093 
Q 1294 3584 2113 3584 
Q 2378 3584 2631 3529 
Q 2884 3475 3122 3366 
z
" id="DejaVuSans-63" transform="scale(0.015625)">
           </path>
           <path d="M 2834 3397 
L 2834 2853 
Q 2591 2978 2328 3040 
Q 2066 3103 1784 3103 
Q 1356 3103 1142 2972 
Q 928 2841 928 2578 
Q 928 2378 1081 2264 
Q 1234 2150 1697 2047 
L 1894 2003 
Q 2506 1872 2764 1633 
Q 3022 1394 3022 966 
Q 3022 478 2636 193 
Q 2250 -91 1575 -91 
Q 1294 -91 989 -36 
Q 684 19 347 128 
L 347 722 
Q 666 556 975 473 
Q 1284 391 1588 391 
Q 1994 391 2212 530 
Q 2431 669 2431 922 
Q 2431 1156 2273 1281 
Q 2116 1406 1581 1522 
L 1381 1569 
Q 847 1681 609 1914 
Q 372 2147 372 2553 
Q 372 3047 722 3315 
Q 1072 3584 1716 3584 
Q 2034 3584 2315 3537 
Q 2597 3491 2834 3397 
z
" id="DejaVuSans-73" transform="scale(0.015625)">
           </path>
           <path id="DejaVuSans-20" transform="scale(0.015625)">
           </path>
           <path d="M 2375 4863 
L 2375 4384 
L 1825 4384 
Q 1516 4384 1395 4259 
Q 1275 4134 1275 3809 
L 1275 3500 
L 2222 3500 
L 2222 3053 
L 1275 3053 
L 1275 0 
L 697 0 
L 697 3053 
L 147 3053 
L 147 3500 
L 697 3500 
L 697 3744 
Q 697 4328 969 4595 
Q 1241 4863 1831 4863 
L 2375 4863 
z
" id="DejaVuSans-66" transform="scale(0.015625)">
           </path>
           <path d="M 191 3500 
L 800 3500 
L 1894 563 
L 2988 3500 
L 3597 3500 
L 2284 0 
L 1503 0 
L 191 3500 
z
" id="DejaVuSans-76" transform="scale(0.015625)">
           </path>
           <path d="M 3116 1747 
Q 3116 2381 2855 2742 
Q 2594 3103 2138 3103 
Q 1681 3103 1420 2742 
Q 1159 2381 1159 1747 
Q 1159 1113 1420 752 
Q 1681 391 2138 391 
Q 2594 391 2855 752 
Q 3116 1113 3116 1747 
z
M 1159 2969 
Q 1341 3281 1617 3432 
Q 1894 3584 2278 3584 
Q 2916 3584 3314 3078 
Q 3713 2572 3713 1747 
Q 3713 922 3314 415 
Q 2916 -91 2278 -91 
Q 1894 -91 1617 61 
Q 1341 213 1159 525 
L 1159 0 
L 581 0 
L 581 4863 
L 1159 4863 
L 1159 2969 
z
" id="DejaVuSans-62" transform="scale(0.015625)">
           </path>
           <path d="M 581 4863 
L 1159 4863 
L 1159 1991 
L 2875 3500 
L 3609 3500 
L 1753 1863 
L 3688 0 
L 2938 0 
L 1159 1709 
L 1159 0 
L 581 0 
L 581 4863 
z
" id="DejaVuSans-6b" transform="scale(0.015625)">
           </path>
           <path d="M 1159 525 
L 1159 -1331 
L 581 -1331 
L 581 3500 
L 1159 3500 
L 1159 2969 
Q 1341 3281 1617 3432 
Q 1894 3584 2278 3584 
Q 2916 3584 3314 3078 
Q 3713 2572 3713 1747 
Q 3713 922 3314 415 
Q 2916 -91 2278 -91 
Q 1894 -91 1617 61 
Q 1341 213 1159 525 
z
M 3116 1747 
Q 3116 2381 2855 2742 
Q 2594 3103 2138 3103 
Q 1681 3103 1420 2742 
Q 1159 2381 1159 1747 
Q 1159 1113 1420 752 
Q 1681 391 2138 391 
Q 2594 391 2855 752 
Q 3116 1113 3116 1747 
z
" id="DejaVuSans-70" transform="scale(0.015625)">
           </path>
          </defs>
          <use xlink:href="#DejaVuSans-44">
          </use>
          <use x="77.001953" xlink:href="#DejaVuSans-65">
          </use>
          <use x="138.525391" xlink:href="#DejaVuSans-63">
          </use>
          <use x="193.505859" xlink:href="#DejaVuSans-72">
          </use>
          <use x="232.369141" xlink:href="#DejaVuSans-65">
          </use>
          <use x="293.892578" xlink:href="#DejaVuSans-61">
          </use>
          <use x="355.171875" xlink:href="#DejaVuSans-73">
          </use>
          <use x="407.271484" xlink:href="#DejaVuSans-65">
          </use>
          <use x="468.794922" xlink:href="#DejaVuSans-20">
          </use>
          <use x="500.582031" xlink:href="#DejaVuSans-6f">
          </use>
          <use x="561.763672" xlink:href="#DejaVuSans-66">
          </use>
          <use x="596.96875" xlink:href="#DejaVuSans-20">
          </use>
          <use x="628.755859" xlink:href="#DejaVuSans-63">
          </use>
          <use x="683.736328" xlink:href="#DejaVuSans-6f">
          </use>
          <use x="744.917969" xlink:href="#DejaVuSans-73">
          </use>
          <use x="797.017578" xlink:href="#DejaVuSans-74">
          </use>
          <use x="836.226562" xlink:href="#DejaVuSans-20">
          </use>
          <use x="868.013672" xlink:href="#DejaVuSans-6f">
          </use>
          <use x="929.195312" xlink:href="#DejaVuSans-76">
          </use>
          <use x="988.375" xlink:href="#DejaVuSans-65">
          </use>
          <use x="1049.898438" xlink:href="#DejaVuSans-72">
          </use>
          <use x="1091.011719" xlink:href="#DejaVuSans-20">
          </use>
          <use x="1122.798828" xlink:href="#DejaVuSans-62">
          </use>
          <use x="1186.275391" xlink:href="#DejaVuSans-61">
          </use>
          <use x="1247.554688" xlink:href="#DejaVuSans-63">
          </use>
          <use x="1302.535156" xlink:href="#DejaVuSans-6b">
          </use>
          <use x="1360.445312" xlink:href="#DejaVuSans-70">
          </use>
          <use x="1423.921875" xlink:href="#DejaVuSans-72">
          </use>
          <use x="1462.785156" xlink:href="#DejaVuSans-6f">
          </use>
          <use x="1523.966797" xlink:href="#DejaVuSans-70">
          </use>
          <use x="1587.443359" xlink:href="#DejaVuSans-20">
          </use>
          <use x="1619.230469" xlink:href="#DejaVuSans-69">
          </use>
          <use x="1647.013672" xlink:href="#DejaVuSans-74">
          </use>
          <use x="1686.222656" xlink:href="#DejaVuSans-65">
          </use>
          <use x="1747.746094" xlink:href="#DejaVuSans-72">
          </use>
          <use x="1788.859375" xlink:href="#DejaVuSans-61">
          </use>
          <use x="1850.138672" xlink:href="#DejaVuSans-74">
          </use>
          <use x="1889.347656" xlink:href="#DejaVuSans-69">
          </use>
          <use x="1917.130859" xlink:href="#DejaVuSans-6f">
          </use>
          <use x="1978.3125" xlink:href="#DejaVuSans-6e">
          </use>
         </g>
        </g>
        <g id="legend_1">
         <g id="patch_7">
          <path d="M 241.825938 74.3525 
L 373.543125 74.3525 
Q 375.543125 74.3525 375.543125 72.3525 
L 375.543125 29.318125 
Q 375.543125 27.318125 373.543125 27.318125 
L 241.825938 27.318125 
Q 239.825938 27.318125 239.825938 29.318125 
L 239.825938 72.3525 
Q 239.825938 74.3525 241.825938 74.3525 
z
" style="fill:#eaeaf2;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;">
          </path>
         </g>
         <g id="line2d_15">
          <path d="M 243.825938 35.416562 
L 263.825938 35.416562 
" style="fill:none;stroke:#000000;stroke-linecap:round;stroke-width:0.5;">
          </path>
         </g>
         <g id="line2d_16">
         </g>
         <g id="text_15">
          <!-- cost minibatches -->
          <g style="fill:#262626;" transform="translate(271.825938 38.916562)scale(0.1 -0.1)">
           <defs>
            <path d="M 3328 2828 
Q 3544 3216 3844 3400 
Q 4144 3584 4550 3584 
Q 5097 3584 5394 3201 
Q 5691 2819 5691 2113 
L 5691 0 
L 5113 0 
L 5113 2094 
Q 5113 2597 4934 2840 
Q 4756 3084 4391 3084 
Q 3944 3084 3684 2787 
Q 3425 2491 3425 1978 
L 3425 0 
L 2847 0 
L 2847 2094 
Q 2847 2600 2669 2842 
Q 2491 3084 2119 3084 
Q 1678 3084 1418 2786 
Q 1159 2488 1159 1978 
L 1159 0 
L 581 0 
L 581 3500 
L 1159 3500 
L 1159 2956 
Q 1356 3278 1631 3431 
Q 1906 3584 2284 3584 
Q 2666 3584 2933 3390 
Q 3200 3197 3328 2828 
z
" id="DejaVuSans-6d" transform="scale(0.015625)">
            </path>
            <path d="M 3513 2113 
L 3513 0 
L 2938 0 
L 2938 2094 
Q 2938 2591 2744 2837 
Q 2550 3084 2163 3084 
Q 1697 3084 1428 2787 
Q 1159 2491 1159 1978 
L 1159 0 
L 581 0 
L 581 4863 
L 1159 4863 
L 1159 2956 
Q 1366 3272 1645 3428 
Q 1925 3584 2291 3584 
Q 2894 3584 3203 3211 
Q 3513 2838 3513 2113 
z
" id="DejaVuSans-68" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-63">
           </use>
           <use x="54.980469" xlink:href="#DejaVuSans-6f">
           </use>
           <use x="116.162109" xlink:href="#DejaVuSans-73">
           </use>
           <use x="168.261719" xlink:href="#DejaVuSans-74">
           </use>
           <use x="207.470703" xlink:href="#DejaVuSans-20">
           </use>
           <use x="239.257812" xlink:href="#DejaVuSans-6d">
           </use>
           <use x="336.669922" xlink:href="#DejaVuSans-69">
           </use>
           <use x="364.453125" xlink:href="#DejaVuSans-6e">
           </use>
           <use x="427.832031" xlink:href="#DejaVuSans-69">
           </use>
           <use x="455.615234" xlink:href="#DejaVuSans-62">
           </use>
           <use x="519.091797" xlink:href="#DejaVuSans-61">
           </use>
           <use x="580.371094" xlink:href="#DejaVuSans-74">
           </use>
           <use x="619.580078" xlink:href="#DejaVuSans-63">
           </use>
           <use x="674.560547" xlink:href="#DejaVuSans-68">
           </use>
           <use x="737.939453" xlink:href="#DejaVuSans-65">
           </use>
           <use x="799.462891" xlink:href="#DejaVuSans-73">
           </use>
          </g>
         </g>
         <g id="line2d_17">
          <path d="M 243.825938 50.094687 
L 263.825938 50.094687 
" style="fill:none;stroke:#ff0000;stroke-linecap:round;stroke-width:2;">
          </path>
         </g>
         <g id="line2d_18">
         </g>
         <g id="text_16">
          <!-- cost full training set -->
          <g style="fill:#262626;" transform="translate(271.825938 53.594687)scale(0.1 -0.1)">
           <defs>
            <path d="M 544 1381 
L 544 3500 
L 1119 3500 
L 1119 1403 
Q 1119 906 1312 657 
Q 1506 409 1894 409 
Q 2359 409 2629 706 
Q 2900 1003 2900 1516 
L 2900 3500 
L 3475 3500 
L 3475 0 
L 2900 0 
L 2900 538 
Q 2691 219 2414 64 
Q 2138 -91 1772 -91 
Q 1169 -91 856 284 
Q 544 659 544 1381 
z
M 1991 3584 
L 1991 3584 
z
" id="DejaVuSans-75" transform="scale(0.015625)">
            </path>
            <path d="M 603 4863 
L 1178 4863 
L 1178 0 
L 603 0 
L 603 4863 
z
" id="DejaVuSans-6c" transform="scale(0.015625)">
            </path>
            <path d="M 2906 1791 
Q 2906 2416 2648 2759 
Q 2391 3103 1925 3103 
Q 1463 3103 1205 2759 
Q 947 2416 947 1791 
Q 947 1169 1205 825 
Q 1463 481 1925 481 
Q 2391 481 2648 825 
Q 2906 1169 2906 1791 
z
M 3481 434 
Q 3481 -459 3084 -895 
Q 2688 -1331 1869 -1331 
Q 1566 -1331 1297 -1286 
Q 1028 -1241 775 -1147 
L 775 -588 
Q 1028 -725 1275 -790 
Q 1522 -856 1778 -856 
Q 2344 -856 2625 -561 
Q 2906 -266 2906 331 
L 2906 616 
Q 2728 306 2450 153 
Q 2172 0 1784 0 
Q 1141 0 747 490 
Q 353 981 353 1791 
Q 353 2603 747 3093 
Q 1141 3584 1784 3584 
Q 2172 3584 2450 3431 
Q 2728 3278 2906 2969 
L 2906 3500 
L 3481 3500 
L 3481 434 
z
" id="DejaVuSans-67" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-63">
           </use>
           <use x="54.980469" xlink:href="#DejaVuSans-6f">
           </use>
           <use x="116.162109" xlink:href="#DejaVuSans-73">
           </use>
           <use x="168.261719" xlink:href="#DejaVuSans-74">
           </use>
           <use x="207.470703" xlink:href="#DejaVuSans-20">
           </use>
           <use x="239.257812" xlink:href="#DejaVuSans-66">
           </use>
           <use x="274.462891" xlink:href="#DejaVuSans-75">
           </use>
           <use x="337.841797" xlink:href="#DejaVuSans-6c">
           </use>
           <use x="365.625" xlink:href="#DejaVuSans-6c">
           </use>
           <use x="393.408203" xlink:href="#DejaVuSans-20">
           </use>
           <use x="425.195312" xlink:href="#DejaVuSans-74">
           </use>
           <use x="464.404297" xlink:href="#DejaVuSans-72">
           </use>
           <use x="505.517578" xlink:href="#DejaVuSans-61">
           </use>
           <use x="566.796875" xlink:href="#DejaVuSans-69">
           </use>
           <use x="594.580078" xlink:href="#DejaVuSans-6e">
           </use>
           <use x="657.958984" xlink:href="#DejaVuSans-69">
           </use>
           <use x="685.742188" xlink:href="#DejaVuSans-6e">
           </use>
           <use x="749.121094" xlink:href="#DejaVuSans-67">
           </use>
           <use x="812.597656" xlink:href="#DejaVuSans-20">
           </use>
           <use x="844.384766" xlink:href="#DejaVuSans-73">
           </use>
           <use x="896.484375" xlink:href="#DejaVuSans-65">
           </use>
           <use x="958.007812" xlink:href="#DejaVuSans-74">
           </use>
          </g>
         </g>
         <g id="line2d_19">
          <path d="M 243.825938 64.772812 
L 263.825938 64.772812 
" style="fill:none;stroke:#0000ff;stroke-linecap:round;stroke-width:3;">
          </path>
         </g>
         <g id="line2d_20">
         </g>
         <g id="text_17">
          <!-- cost validation set -->
          <g style="fill:#262626;" transform="translate(271.825938 68.272812)scale(0.1 -0.1)">
           <defs>
            <path d="M 2906 2969 
L 2906 4863 
L 3481 4863 
L 3481 0 
L 2906 0 
L 2906 525 
Q 2725 213 2448 61 
Q 2172 -91 1784 -91 
Q 1150 -91 751 415 
Q 353 922 353 1747 
Q 353 2572 751 3078 
Q 1150 3584 1784 3584 
Q 2172 3584 2448 3432 
Q 2725 3281 2906 2969 
z
M 947 1747 
Q 947 1113 1208 752 
Q 1469 391 1925 391 
Q 2381 391 2643 752 
Q 2906 1113 2906 1747 
Q 2906 2381 2643 2742 
Q 2381 3103 1925 3103 
Q 1469 3103 1208 2742 
Q 947 2381 947 1747 
z
" id="DejaVuSans-64" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-63">
           </use>
           <use x="54.980469" xlink:href="#DejaVuSans-6f">
           </use>
           <use x="116.162109" xlink:href="#DejaVuSans-73">
           </use>
           <use x="168.261719" xlink:href="#DejaVuSans-74">
           </use>
           <use x="207.470703" xlink:href="#DejaVuSans-20">
           </use>
           <use x="239.257812" xlink:href="#DejaVuSans-76">
           </use>
           <use x="298.4375" xlink:href="#DejaVuSans-61">
           </use>
           <use x="359.716797" xlink:href="#DejaVuSans-6c">
           </use>
           <use x="387.5" xlink:href="#DejaVuSans-69">
           </use>
           <use x="415.283203" xlink:href="#DejaVuSans-64">
           </use>
           <use x="478.759766" xlink:href="#DejaVuSans-61">
           </use>
           <use x="540.039062" xlink:href="#DejaVuSans-74">
           </use>
           <use x="579.248047" xlink:href="#DejaVuSans-69">
           </use>
           <use x="607.03125" xlink:href="#DejaVuSans-6f">
           </use>
           <use x="668.212891" xlink:href="#DejaVuSans-6e">
           </use>
           <use x="731.591797" xlink:href="#DejaVuSans-20">
           </use>
           <use x="763.378906" xlink:href="#DejaVuSans-73">
           </use>
           <use x="815.478516" xlink:href="#DejaVuSans-65">
           </use>
           <use x="877.001953" xlink:href="#DejaVuSans-74">
           </use>
          </g>
         </g>
        </g>
       </g>
      </g>
      <defs>
       <clippath id="p5125e43100">
        <rect height="221.76" width="334.8" x="45.743125" y="22.318125">
        </rect>
       </clippath>
      </defs>
     </svg>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Performance-on-the-test-set">
    Performance on the test set
    <a class="anchor-link" href="#Performance-on-the-test-set">
     ¶
    </a>
   </h2>
   <p>
    Finally, the
    <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">
     accuracy
    </a>
    on the independent test set is computed to measure the performance of the model. The
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">
     scikit-learn
    </a>
    <code>
     accuracy_score
    </code>
    method is used to compute this accuracy from the predictions made by the model. The final accuracy on the test set is $96\%$ as shown below.
   </p>
   <p>
    The results can be analyzed in more detail with the help of a
    <a href="https://en.wikipedia.org/wiki/Confusion_matrix">
     confusion table
    </a>
    . This table shows how many samples of which class are classified as one of the possible classes. The confusion table is shown in the figure below. It is computed by the
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">
     scikit-learn
    </a>
    <code>
     confusion_matrix
    </code>
    method.
    <br/>
    Notice that the digit '8' is misclassified five times, two times as '2', two times as '5', and one time as '9'.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [17]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Get results of test data</span>
<span class="c1"># Get the target outputs</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">T_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Get activation of test samples</span>
<span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
<span class="c1"># Get the predictions made by the network</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Test set accuracy</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'The accuracy on the test set is </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="s1">.0%</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>The accuracy on the test set is 96%
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [18]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Show confusion table</span>
<span class="c1"># Get confustion matrix</span>
<span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="c1"># Plot the confusion table</span>
<span class="c1"># Digit class names</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">'$</span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s1">d</span><span class="si">}</span><span class="s1">$'</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>
<span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">axes_style</span><span class="p">(</span><span class="s2">"white"</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Show class labels on each axis</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">tick_top</span><span class="p">()</span>
    <span class="n">major_ticks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">minor_ticks</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">major_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">major_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">minor_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">minor_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="c1"># Set plot labels</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s2">"right"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Predicted label'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'True label'</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">'Confusion table'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="c1"># Show a grid to seperate digits</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sa">u</span><span class="s1">'minor'</span><span class="p">)</span>
    <span class="c1"># Color each grid cell according to the number classes predicted</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'binary'</span><span class="p">)</span>
    <span class="c1"># Show the number of samples in each cell</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">conf_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">conf_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">'w'</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">y</span> <span class="k">else</span> <span class="s1">'k'</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">conf_matrix</span><span class="p">[</span><span class="n">y</span><span class="p">,</span><span class="n">x</span><span class="p">],</span> 
                    <span class="n">ha</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_svg output_subarea">
     <?xml version="1.0" encoding="utf-8" standalone="no"?>
     <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
     <svg height="346.030125pt" version="1.1" viewbox="0 0 308.870125 346.030125" width="308.870125pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <metadata>
       <rdf:rdf xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
        <cc:work>
         <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage">
         </dc:type>
         <dc:date>
          2021-05-05T21:28:18.292582
         </dc:date>
         <dc:format>
          image/svg+xml
         </dc:format>
         <dc:creator>
          <cc:agent>
           <dc:title>
            Matplotlib v3.4.1, https://matplotlib.org/
           </dc:title>
          </cc:agent>
         </dc:creator>
        </cc:work>
       </rdf:rdf>
      </metadata>
      <defs>
       <style type="text/css">
        *{stroke-linecap:butt;stroke-linejoin:round;}
       </style>
      </defs>
      <g id="figure_1">
       <g id="patch_1">
        <path d="M 0 346.030125 
L 308.870125 346.030125 
L 308.870125 0 
L 0 0 
z
" style="fill:#ffffff;">
        </path>
       </g>
       <g id="axes_1">
        <g id="patch_2">
         <path d="M 21.88 325.152 
L 287.992 325.152 
L 287.992 59.04 
L 21.88 59.04 
z
" style="fill:#ffffff;">
         </path>
        </g>
        <g clip-path="url(#ped74d1a50b)">
         <image height="266.4" id="image4b0f0c6b28" transform="scale(1 -1)translate(0 -266.4)" width="266.4" x="21.88" xlink:href="data:image/png;base64,
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" y="-58.752"/>
        </g>
        <g id="matplotlib.axis_1">
         <g id="xtick_1">
          <g id="line2d_1">
           <defs>
            <path d="M 0 0 
L 0 -3.5 
" id="m00752f63da" style="stroke:#262626;stroke-width:0.8;">
            </path>
           </defs>
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="35.1856" xlink:href="#m00752f63da" y="59.04">
            </use>
           </g>
          </g>
          <g id="text_1">
           <!-- $0$ -->
           <g style="fill:#262626;" transform="translate(31.3456 49.544375)scale(0.12 -0.12)">
            <defs>
             <path d="M 2034 4250 
Q 1547 4250 1301 3770 
Q 1056 3291 1056 2328 
Q 1056 1369 1301 889 
Q 1547 409 2034 409 
Q 2525 409 2770 889 
Q 3016 1369 3016 2328 
Q 3016 3291 2770 3770 
Q 2525 4250 2034 4250 
z
M 2034 4750 
Q 2819 4750 3233 4129 
Q 3647 3509 3647 2328 
Q 3647 1150 3233 529 
Q 2819 -91 2034 -91 
Q 1250 -91 836 529 
Q 422 1150 422 2328 
Q 422 3509 836 4129 
Q 1250 4750 2034 4750 
z
" id="DejaVuSans-30" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_2">
          <g id="line2d_2">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="61.7968" xlink:href="#m00752f63da" y="59.04">
            </use>
           </g>
          </g>
          <g id="text_2">
           <!-- $1$ -->
           <g style="fill:#262626;" transform="translate(57.9568 49.544375)scale(0.12 -0.12)">
            <defs>
             <path d="M 794 531 
L 1825 531 
L 1825 4091 
L 703 3866 
L 703 4441 
L 1819 4666 
L 2450 4666 
L 2450 531 
L 3481 531 
L 3481 0 
L 794 0 
L 794 531 
z
" id="DejaVuSans-31" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-31">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_3">
          <g id="line2d_3">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="88.408" xlink:href="#m00752f63da" y="59.04">
            </use>
           </g>
          </g>
          <g id="text_3">
           <!-- $2$ -->
           <g style="fill:#262626;" transform="translate(84.568 49.544375)scale(0.12 -0.12)">
            <defs>
             <path d="M 1228 531 
L 3431 531 
L 3431 0 
L 469 0 
L 469 531 
Q 828 903 1448 1529 
Q 2069 2156 2228 2338 
Q 2531 2678 2651 2914 
Q 2772 3150 2772 3378 
Q 2772 3750 2511 3984 
Q 2250 4219 1831 4219 
Q 1534 4219 1204 4116 
Q 875 4013 500 3803 
L 500 4441 
Q 881 4594 1212 4672 
Q 1544 4750 1819 4750 
Q 2544 4750 2975 4387 
Q 3406 4025 3406 3419 
Q 3406 3131 3298 2873 
Q 3191 2616 2906 2266 
Q 2828 2175 2409 1742 
Q 1991 1309 1228 531 
z
" id="DejaVuSans-32" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-32">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_4">
          <g id="line2d_4">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="115.0192" xlink:href="#m00752f63da" y="59.04">
            </use>
           </g>
          </g>
          <g id="text_4">
           <!-- $3$ -->
           <g style="fill:#262626;" transform="translate(111.1792 49.544375)scale(0.12 -0.12)">
            <defs>
             <path d="M 2597 2516 
Q 3050 2419 3304 2112 
Q 3559 1806 3559 1356 
Q 3559 666 3084 287 
Q 2609 -91 1734 -91 
Q 1441 -91 1130 -33 
Q 819 25 488 141 
L 488 750 
Q 750 597 1062 519 
Q 1375 441 1716 441 
Q 2309 441 2620 675 
Q 2931 909 2931 1356 
Q 2931 1769 2642 2001 
Q 2353 2234 1838 2234 
L 1294 2234 
L 1294 2753 
L 1863 2753 
Q 2328 2753 2575 2939 
Q 2822 3125 2822 3475 
Q 2822 3834 2567 4026 
Q 2313 4219 1838 4219 
Q 1578 4219 1281 4162 
Q 984 4106 628 3988 
L 628 4550 
Q 988 4650 1302 4700 
Q 1616 4750 1894 4750 
Q 2613 4750 3031 4423 
Q 3450 4097 3450 3541 
Q 3450 3153 3228 2886 
Q 3006 2619 2597 2516 
z
" id="DejaVuSans-33" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-33">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_5">
          <g id="line2d_5">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="141.6304" xlink:href="#m00752f63da" y="59.04">
            </use>
           </g>
          </g>
          <g id="text_5">
           <!-- $4$ -->
           <g style="fill:#262626;" transform="translate(137.7904 49.544375)scale(0.12 -0.12)">
            <defs>
             <path d="M 2419 4116 
L 825 1625 
L 2419 1625 
L 2419 4116 
z
M 2253 4666 
L 3047 4666 
L 3047 1625 
L 3713 1625 
L 3713 1100 
L 3047 1100 
L 3047 0 
L 2419 0 
L 2419 1100 
L 313 1100 
L 313 1709 
L 2253 4666 
z
" id="DejaVuSans-34" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-34">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_6">
          <g id="line2d_6">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="168.2416" xlink:href="#m00752f63da" y="59.04">
            </use>
           </g>
          </g>
          <g id="text_6">
           <!-- $5$ -->
           <g style="fill:#262626;" transform="translate(164.4016 49.544375)scale(0.12 -0.12)">
            <defs>
             <path d="M 691 4666 
L 3169 4666 
L 3169 4134 
L 1269 4134 
L 1269 2991 
Q 1406 3038 1543 3061 
Q 1681 3084 1819 3084 
Q 2600 3084 3056 2656 
Q 3513 2228 3513 1497 
Q 3513 744 3044 326 
Q 2575 -91 1722 -91 
Q 1428 -91 1123 -41 
Q 819 9 494 109 
L 494 744 
Q 775 591 1075 516 
Q 1375 441 1709 441 
Q 2250 441 2565 725 
Q 2881 1009 2881 1497 
Q 2881 1984 2565 2268 
Q 2250 2553 1709 2553 
Q 1456 2553 1204 2497 
Q 953 2441 691 2322 
L 691 4666 
z
" id="DejaVuSans-35" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-35">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_7">
          <g id="line2d_7">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="194.8528" xlink:href="#m00752f63da" y="59.04">
            </use>
           </g>
          </g>
          <g id="text_7">
           <!-- $6$ -->
           <g style="fill:#262626;" transform="translate(191.0128 49.544375)scale(0.12 -0.12)">
            <defs>
             <path d="M 2113 2584 
Q 1688 2584 1439 2293 
Q 1191 2003 1191 1497 
Q 1191 994 1439 701 
Q 1688 409 2113 409 
Q 2538 409 2786 701 
Q 3034 994 3034 1497 
Q 3034 2003 2786 2293 
Q 2538 2584 2113 2584 
z
M 3366 4563 
L 3366 3988 
Q 3128 4100 2886 4159 
Q 2644 4219 2406 4219 
Q 1781 4219 1451 3797 
Q 1122 3375 1075 2522 
Q 1259 2794 1537 2939 
Q 1816 3084 2150 3084 
Q 2853 3084 3261 2657 
Q 3669 2231 3669 1497 
Q 3669 778 3244 343 
Q 2819 -91 2113 -91 
Q 1303 -91 875 529 
Q 447 1150 447 2328 
Q 447 3434 972 4092 
Q 1497 4750 2381 4750 
Q 2619 4750 2861 4703 
Q 3103 4656 3366 4563 
z
" id="DejaVuSans-36" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-36">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_8">
          <g id="line2d_8">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="221.464" xlink:href="#m00752f63da" y="59.04">
            </use>
           </g>
          </g>
          <g id="text_8">
           <!-- $7$ -->
           <g style="fill:#262626;" transform="translate(217.624 49.544375)scale(0.12 -0.12)">
            <defs>
             <path d="M 525 4666 
L 3525 4666 
L 3525 4397 
L 1831 0 
L 1172 0 
L 2766 4134 
L 525 4134 
L 525 4666 
z
" id="DejaVuSans-37" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-37">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_9">
          <g id="line2d_9">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="248.0752" xlink:href="#m00752f63da" y="59.04">
            </use>
           </g>
          </g>
          <g id="text_9">
           <!-- $8$ -->
           <g style="fill:#262626;" transform="translate(244.2352 49.544375)scale(0.12 -0.12)">
            <defs>
             <path d="M 2034 2216 
Q 1584 2216 1326 1975 
Q 1069 1734 1069 1313 
Q 1069 891 1326 650 
Q 1584 409 2034 409 
Q 2484 409 2743 651 
Q 3003 894 3003 1313 
Q 3003 1734 2745 1975 
Q 2488 2216 2034 2216 
z
M 1403 2484 
Q 997 2584 770 2862 
Q 544 3141 544 3541 
Q 544 4100 942 4425 
Q 1341 4750 2034 4750 
Q 2731 4750 3128 4425 
Q 3525 4100 3525 3541 
Q 3525 3141 3298 2862 
Q 3072 2584 2669 2484 
Q 3125 2378 3379 2068 
Q 3634 1759 3634 1313 
Q 3634 634 3220 271 
Q 2806 -91 2034 -91 
Q 1263 -91 848 271 
Q 434 634 434 1313 
Q 434 1759 690 2068 
Q 947 2378 1403 2484 
z
M 1172 3481 
Q 1172 3119 1398 2916 
Q 1625 2713 2034 2713 
Q 2441 2713 2670 2916 
Q 2900 3119 2900 3481 
Q 2900 3844 2670 4047 
Q 2441 4250 2034 4250 
Q 1625 4250 1398 4047 
Q 1172 3844 1172 3481 
z
" id="DejaVuSans-38" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-38">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_10">
          <g id="line2d_10">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="274.6864" xlink:href="#m00752f63da" y="59.04">
            </use>
           </g>
          </g>
          <g id="text_10">
           <!-- $9$ -->
           <g style="fill:#262626;" transform="translate(270.8464 49.544375)scale(0.12 -0.12)">
            <defs>
             <path d="M 703 97 
L 703 672 
Q 941 559 1184 500 
Q 1428 441 1663 441 
Q 2288 441 2617 861 
Q 2947 1281 2994 2138 
Q 2813 1869 2534 1725 
Q 2256 1581 1919 1581 
Q 1219 1581 811 2004 
Q 403 2428 403 3163 
Q 403 3881 828 4315 
Q 1253 4750 1959 4750 
Q 2769 4750 3195 4129 
Q 3622 3509 3622 2328 
Q 3622 1225 3098 567 
Q 2575 -91 1691 -91 
Q 1453 -91 1209 -44 
Q 966 3 703 97 
z
M 1959 2075 
Q 2384 2075 2632 2365 
Q 2881 2656 2881 3163 
Q 2881 3666 2632 3958 
Q 2384 4250 1959 4250 
Q 1534 4250 1286 3958 
Q 1038 3666 1038 3163 
Q 1038 2656 1286 2365 
Q 1534 2075 1959 2075 
z
" id="DejaVuSans-39" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-39">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_11">
          <g id="line2d_11">
           <path clip-path="url(#ped74d1a50b)" d="M 48.4912 325.152 
L 48.4912 59.04 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_12">
           <defs>
            <path d="M 0 0 
L 0 -2 
" id="m8428e028e3" style="stroke:#262626;stroke-width:0.6;">
            </path>
           </defs>
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="48.4912" xlink:href="#m8428e028e3" y="59.04">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_12">
          <g id="line2d_13">
           <path clip-path="url(#ped74d1a50b)" d="M 75.1024 325.152 
L 75.1024 59.04 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_14">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="75.1024" xlink:href="#m8428e028e3" y="59.04">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_13">
          <g id="line2d_15">
           <path clip-path="url(#ped74d1a50b)" d="M 101.7136 325.152 
L 101.7136 59.04 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_16">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="101.7136" xlink:href="#m8428e028e3" y="59.04">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_14">
          <g id="line2d_17">
           <path clip-path="url(#ped74d1a50b)" d="M 128.3248 325.152 
L 128.3248 59.04 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_18">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="128.3248" xlink:href="#m8428e028e3" y="59.04">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_15">
          <g id="line2d_19">
           <path clip-path="url(#ped74d1a50b)" d="M 154.936 325.152 
L 154.936 59.04 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_20">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="154.936" xlink:href="#m8428e028e3" y="59.04">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_16">
          <g id="line2d_21">
           <path clip-path="url(#ped74d1a50b)" d="M 181.5472 325.152 
L 181.5472 59.04 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_22">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="181.5472" xlink:href="#m8428e028e3" y="59.04">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_17">
          <g id="line2d_23">
           <path clip-path="url(#ped74d1a50b)" d="M 208.1584 325.152 
L 208.1584 59.04 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_24">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="208.1584" xlink:href="#m8428e028e3" y="59.04">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_18">
          <g id="line2d_25">
           <path clip-path="url(#ped74d1a50b)" d="M 234.7696 325.152 
L 234.7696 59.04 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_26">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="234.7696" xlink:href="#m8428e028e3" y="59.04">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_19">
          <g id="line2d_27">
           <path clip-path="url(#ped74d1a50b)" d="M 261.3808 325.152 
L 261.3808 59.04 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_28">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="261.3808" xlink:href="#m8428e028e3" y="59.04">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_20">
          <g id="line2d_29">
           <path clip-path="url(#ped74d1a50b)" d="M 287.992 325.152 
L 287.992 59.04 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_30">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="287.992" xlink:href="#m8428e028e3" y="59.04">
            </use>
           </g>
          </g>
         </g>
         <g id="text_11">
          <!-- Predicted label -->
          <g style="fill:#262626;" transform="translate(117.783656 336.750437)scale(0.1 -0.1)">
           <defs>
            <path d="M 1259 4147 
L 1259 2394 
L 2053 2394 
Q 2494 2394 2734 2622 
Q 2975 2850 2975 3272 
Q 2975 3691 2734 3919 
Q 2494 4147 2053 4147 
L 1259 4147 
z
M 628 4666 
L 2053 4666 
Q 2838 4666 3239 4311 
Q 3641 3956 3641 3272 
Q 3641 2581 3239 2228 
Q 2838 1875 2053 1875 
L 1259 1875 
L 1259 0 
L 628 0 
L 628 4666 
z
" id="DejaVuSans-50" transform="scale(0.015625)">
            </path>
            <path d="M 2631 2963 
Q 2534 3019 2420 3045 
Q 2306 3072 2169 3072 
Q 1681 3072 1420 2755 
Q 1159 2438 1159 1844 
L 1159 0 
L 581 0 
L 581 3500 
L 1159 3500 
L 1159 2956 
Q 1341 3275 1631 3429 
Q 1922 3584 2338 3584 
Q 2397 3584 2469 3576 
Q 2541 3569 2628 3553 
L 2631 2963 
z
" id="DejaVuSans-72" transform="scale(0.015625)">
            </path>
            <path d="M 3597 1894 
L 3597 1613 
L 953 1613 
Q 991 1019 1311 708 
Q 1631 397 2203 397 
Q 2534 397 2845 478 
Q 3156 559 3463 722 
L 3463 178 
Q 3153 47 2828 -22 
Q 2503 -91 2169 -91 
Q 1331 -91 842 396 
Q 353 884 353 1716 
Q 353 2575 817 3079 
Q 1281 3584 2069 3584 
Q 2775 3584 3186 3129 
Q 3597 2675 3597 1894 
z
M 3022 2063 
Q 3016 2534 2758 2815 
Q 2500 3097 2075 3097 
Q 1594 3097 1305 2825 
Q 1016 2553 972 2059 
L 3022 2063 
z
" id="DejaVuSans-65" transform="scale(0.015625)">
            </path>
            <path d="M 2906 2969 
L 2906 4863 
L 3481 4863 
L 3481 0 
L 2906 0 
L 2906 525 
Q 2725 213 2448 61 
Q 2172 -91 1784 -91 
Q 1150 -91 751 415 
Q 353 922 353 1747 
Q 353 2572 751 3078 
Q 1150 3584 1784 3584 
Q 2172 3584 2448 3432 
Q 2725 3281 2906 2969 
z
M 947 1747 
Q 947 1113 1208 752 
Q 1469 391 1925 391 
Q 2381 391 2643 752 
Q 2906 1113 2906 1747 
Q 2906 2381 2643 2742 
Q 2381 3103 1925 3103 
Q 1469 3103 1208 2742 
Q 947 2381 947 1747 
z
" id="DejaVuSans-64" transform="scale(0.015625)">
            </path>
            <path d="M 603 3500 
L 1178 3500 
L 1178 0 
L 603 0 
L 603 3500 
z
M 603 4863 
L 1178 4863 
L 1178 4134 
L 603 4134 
L 603 4863 
z
" id="DejaVuSans-69" transform="scale(0.015625)">
            </path>
            <path d="M 3122 3366 
L 3122 2828 
Q 2878 2963 2633 3030 
Q 2388 3097 2138 3097 
Q 1578 3097 1268 2742 
Q 959 2388 959 1747 
Q 959 1106 1268 751 
Q 1578 397 2138 397 
Q 2388 397 2633 464 
Q 2878 531 3122 666 
L 3122 134 
Q 2881 22 2623 -34 
Q 2366 -91 2075 -91 
Q 1284 -91 818 406 
Q 353 903 353 1747 
Q 353 2603 823 3093 
Q 1294 3584 2113 3584 
Q 2378 3584 2631 3529 
Q 2884 3475 3122 3366 
z
" id="DejaVuSans-63" transform="scale(0.015625)">
            </path>
            <path d="M 1172 4494 
L 1172 3500 
L 2356 3500 
L 2356 3053 
L 1172 3053 
L 1172 1153 
Q 1172 725 1289 603 
Q 1406 481 1766 481 
L 2356 481 
L 2356 0 
L 1766 0 
Q 1100 0 847 248 
Q 594 497 594 1153 
L 594 3053 
L 172 3053 
L 172 3500 
L 594 3500 
L 594 4494 
L 1172 4494 
z
" id="DejaVuSans-74" transform="scale(0.015625)">
            </path>
            <path id="DejaVuSans-20" transform="scale(0.015625)">
            </path>
            <path d="M 603 4863 
L 1178 4863 
L 1178 0 
L 603 0 
L 603 4863 
z
" id="DejaVuSans-6c" transform="scale(0.015625)">
            </path>
            <path d="M 2194 1759 
Q 1497 1759 1228 1600 
Q 959 1441 959 1056 
Q 959 750 1161 570 
Q 1363 391 1709 391 
Q 2188 391 2477 730 
Q 2766 1069 2766 1631 
L 2766 1759 
L 2194 1759 
z
M 3341 1997 
L 3341 0 
L 2766 0 
L 2766 531 
Q 2569 213 2275 61 
Q 1981 -91 1556 -91 
Q 1019 -91 701 211 
Q 384 513 384 1019 
Q 384 1609 779 1909 
Q 1175 2209 1959 2209 
L 2766 2209 
L 2766 2266 
Q 2766 2663 2505 2880 
Q 2244 3097 1772 3097 
Q 1472 3097 1187 3025 
Q 903 2953 641 2809 
L 641 3341 
Q 956 3463 1253 3523 
Q 1550 3584 1831 3584 
Q 2591 3584 2966 3190 
Q 3341 2797 3341 1997 
z
" id="DejaVuSans-61" transform="scale(0.015625)">
            </path>
            <path d="M 3116 1747 
Q 3116 2381 2855 2742 
Q 2594 3103 2138 3103 
Q 1681 3103 1420 2742 
Q 1159 2381 1159 1747 
Q 1159 1113 1420 752 
Q 1681 391 2138 391 
Q 2594 391 2855 752 
Q 3116 1113 3116 1747 
z
M 1159 2969 
Q 1341 3281 1617 3432 
Q 1894 3584 2278 3584 
Q 2916 3584 3314 3078 
Q 3713 2572 3713 1747 
Q 3713 922 3314 415 
Q 2916 -91 2278 -91 
Q 1894 -91 1617 61 
Q 1341 213 1159 525 
L 1159 0 
L 581 0 
L 581 4863 
L 1159 4863 
L 1159 2969 
z
" id="DejaVuSans-62" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-50">
           </use>
           <use x="58.552734" xlink:href="#DejaVuSans-72">
           </use>
           <use x="97.416016" xlink:href="#DejaVuSans-65">
           </use>
           <use x="158.939453" xlink:href="#DejaVuSans-64">
           </use>
           <use x="222.416016" xlink:href="#DejaVuSans-69">
           </use>
           <use x="250.199219" xlink:href="#DejaVuSans-63">
           </use>
           <use x="305.179688" xlink:href="#DejaVuSans-74">
           </use>
           <use x="344.388672" xlink:href="#DejaVuSans-65">
           </use>
           <use x="405.912109" xlink:href="#DejaVuSans-64">
           </use>
           <use x="469.388672" xlink:href="#DejaVuSans-20">
           </use>
           <use x="501.175781" xlink:href="#DejaVuSans-6c">
           </use>
           <use x="528.958984" xlink:href="#DejaVuSans-61">
           </use>
           <use x="590.238281" xlink:href="#DejaVuSans-62">
           </use>
           <use x="653.714844" xlink:href="#DejaVuSans-65">
           </use>
           <use x="715.238281" xlink:href="#DejaVuSans-6c">
           </use>
          </g>
         </g>
        </g>
        <g id="matplotlib.axis_2">
         <g id="ytick_1">
          <g id="text_12">
           <!-- $0$ -->
           <g style="fill:#262626;" transform="translate(7.2 76.904662)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_2">
          <g id="text_13">
           <!-- $1$ -->
           <g style="fill:#262626;" transform="translate(7.2 103.515862)scale(0.12 -0.12)">
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-31">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_3">
          <g id="text_14">
           <!-- $2$ -->
           <g style="fill:#262626;" transform="translate(7.2 130.127062)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-32">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_4">
          <g id="text_15">
           <!-- $3$ -->
           <g style="fill:#262626;" transform="translate(7.2 156.738262)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-33">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_5">
          <g id="text_16">
           <!-- $4$ -->
           <g style="fill:#262626;" transform="translate(7.2 183.349462)scale(0.12 -0.12)">
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-34">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_6">
          <g id="text_17">
           <!-- $5$ -->
           <g style="fill:#262626;" transform="translate(7.2 209.960662)scale(0.12 -0.12)">
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-35">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_7">
          <g id="text_18">
           <!-- $6$ -->
           <g style="fill:#262626;" transform="translate(7.2 236.571862)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-36">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_8">
          <g id="text_19">
           <!-- $7$ -->
           <g style="fill:#262626;" transform="translate(7.2 263.183062)scale(0.12 -0.12)">
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-37">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_9">
          <g id="text_20">
           <!-- $8$ -->
           <g style="fill:#262626;" transform="translate(7.2 289.794262)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-38">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_10">
          <g id="text_21">
           <!-- $9$ -->
           <g style="fill:#262626;" transform="translate(7.2 316.405462)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-39">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_11">
          <g id="line2d_31">
           <path clip-path="url(#ped74d1a50b)" d="M 21.88 85.6512 
L 287.992 85.6512 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_12">
          <g id="line2d_32">
           <path clip-path="url(#ped74d1a50b)" d="M 21.88 112.2624 
L 287.992 112.2624 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_13">
          <g id="line2d_33">
           <path clip-path="url(#ped74d1a50b)" d="M 21.88 138.8736 
L 287.992 138.8736 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_14">
          <g id="line2d_34">
           <path clip-path="url(#ped74d1a50b)" d="M 21.88 165.4848 
L 287.992 165.4848 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_15">
          <g id="line2d_35">
           <path clip-path="url(#ped74d1a50b)" d="M 21.88 192.096 
L 287.992 192.096 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_16">
          <g id="line2d_36">
           <path clip-path="url(#ped74d1a50b)" d="M 21.88 218.7072 
L 287.992 218.7072 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_17">
          <g id="line2d_37">
           <path clip-path="url(#ped74d1a50b)" d="M 21.88 245.3184 
L 287.992 245.3184 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_18">
          <g id="line2d_38">
           <path clip-path="url(#ped74d1a50b)" d="M 21.88 271.9296 
L 287.992 271.9296 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_19">
          <g id="line2d_39">
           <path clip-path="url(#ped74d1a50b)" d="M 21.88 298.5408 
L 287.992 298.5408 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_20">
          <g id="line2d_40">
           <path clip-path="url(#ped74d1a50b)" d="M 21.88 325.152 
L 287.992 325.152 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="text_22">
          <!-- True label -->
          <g style="fill:#262626;" transform="translate(299.590437 216.395219)rotate(-90)scale(0.1 -0.1)">
           <defs>
            <path d="M -19 4666 
L 3928 4666 
L 3928 4134 
L 2272 4134 
L 2272 0 
L 1638 0 
L 1638 4134 
L -19 4134 
L -19 4666 
z
" id="DejaVuSans-54" transform="scale(0.015625)">
            </path>
            <path d="M 544 1381 
L 544 3500 
L 1119 3500 
L 1119 1403 
Q 1119 906 1312 657 
Q 1506 409 1894 409 
Q 2359 409 2629 706 
Q 2900 1003 2900 1516 
L 2900 3500 
L 3475 3500 
L 3475 0 
L 2900 0 
L 2900 538 
Q 2691 219 2414 64 
Q 2138 -91 1772 -91 
Q 1169 -91 856 284 
Q 544 659 544 1381 
z
M 1991 3584 
L 1991 3584 
z
" id="DejaVuSans-75" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-54">
           </use>
           <use x="46.333984" xlink:href="#DejaVuSans-72">
           </use>
           <use x="87.447266" xlink:href="#DejaVuSans-75">
           </use>
           <use x="150.826172" xlink:href="#DejaVuSans-65">
           </use>
           <use x="212.349609" xlink:href="#DejaVuSans-20">
           </use>
           <use x="244.136719" xlink:href="#DejaVuSans-6c">
           </use>
           <use x="271.919922" xlink:href="#DejaVuSans-61">
           </use>
           <use x="333.199219" xlink:href="#DejaVuSans-62">
           </use>
           <use x="396.675781" xlink:href="#DejaVuSans-65">
           </use>
           <use x="458.199219" xlink:href="#DejaVuSans-6c">
           </use>
          </g>
         </g>
        </g>
        <g id="patch_3">
         <path d="M 21.88 325.152 
L 21.88 59.04 
" style="fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_4">
         <path d="M 287.992 325.152 
L 287.992 59.04 
" style="fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_5">
         <path d="M 21.88 325.152 
L 287.992 325.152 
" style="fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_6">
         <path d="M 21.88 59.04 
L 287.992 59.04 
" style="fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="text_23">
         <!-- 39 -->
         <g style="fill:#ffffff;" transform="translate(28.8231 75.104975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-39">
          </use>
         </g>
        </g>
        <g id="text_24">
         <!-- 0 -->
         <g transform="translate(32.00435 101.716175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_25">
         <!-- 0 -->
         <g transform="translate(32.00435 128.327375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_26">
         <!-- 0 -->
         <g transform="translate(32.00435 154.938575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_27">
         <!-- 0 -->
         <g transform="translate(32.00435 181.549775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_28">
         <!-- 0 -->
         <g transform="translate(32.00435 208.160975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_29">
         <!-- 0 -->
         <g transform="translate(32.00435 234.772175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_30">
         <!-- 0 -->
         <g transform="translate(32.00435 261.383375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_31">
         <!-- 0 -->
         <g transform="translate(32.00435 287.994575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_32">
         <!-- 0 -->
         <g transform="translate(32.00435 314.605775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_33">
         <!-- 0 -->
         <g transform="translate(58.61555 75.104975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_34">
         <!-- 37 -->
         <g style="fill:#ffffff;" transform="translate(55.4343 101.716175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-37">
          </use>
         </g>
        </g>
        <g id="text_35">
         <!-- 0 -->
         <g transform="translate(58.61555 128.327375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_36">
         <!-- 0 -->
         <g transform="translate(58.61555 154.938575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_37">
         <!-- 1 -->
         <g transform="translate(58.61555 181.549775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_38">
         <!-- 0 -->
         <g transform="translate(58.61555 208.160975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_39">
         <!-- 0 -->
         <g transform="translate(58.61555 234.772175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_40">
         <!-- 1 -->
         <g transform="translate(58.61555 261.383375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_41">
         <!-- 0 -->
         <g transform="translate(58.61555 287.994575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_42">
         <!-- 0 -->
         <g transform="translate(58.61555 314.605775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_43">
         <!-- 0 -->
         <g transform="translate(85.22675 75.104975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_44">
         <!-- 0 -->
         <g transform="translate(85.22675 101.716175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_45">
         <!-- 29 -->
         <g style="fill:#ffffff;" transform="translate(82.0455 128.327375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-39">
          </use>
         </g>
        </g>
        <g id="text_46">
         <!-- 1 -->
         <g transform="translate(85.22675 154.938575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_47">
         <!-- 0 -->
         <g transform="translate(85.22675 181.549775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_48">
         <!-- 0 -->
         <g transform="translate(85.22675 208.160975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_49">
         <!-- 0 -->
         <g transform="translate(85.22675 234.772175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_50">
         <!-- 0 -->
         <g transform="translate(85.22675 261.383375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_51">
         <!-- 2 -->
         <g transform="translate(85.22675 287.994575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
         </g>
        </g>
        <g id="text_52">
         <!-- 0 -->
         <g transform="translate(85.22675 314.605775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_53">
         <!-- 0 -->
         <g transform="translate(111.83795 75.104975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_54">
         <!-- 0 -->
         <g transform="translate(111.83795 101.716175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_55">
         <!-- 0 -->
         <g transform="translate(111.83795 128.327375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_56">
         <!-- 38 -->
         <g style="fill:#ffffff;" transform="translate(108.6567 154.938575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-38">
          </use>
         </g>
        </g>
        <g id="text_57">
         <!-- 0 -->
         <g transform="translate(111.83795 181.549775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_58">
         <!-- 1 -->
         <g transform="translate(111.83795 208.160975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_59">
         <!-- 0 -->
         <g transform="translate(111.83795 234.772175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_60">
         <!-- 0 -->
         <g transform="translate(111.83795 261.383375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_61">
         <!-- 0 -->
         <g transform="translate(111.83795 287.994575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_62">
         <!-- 1 -->
         <g transform="translate(111.83795 314.605775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_63">
         <!-- 1 -->
         <g transform="translate(138.44915 75.104975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_64">
         <!-- 0 -->
         <g transform="translate(138.44915 101.716175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_65">
         <!-- 0 -->
         <g transform="translate(138.44915 128.327375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_66">
         <!-- 0 -->
         <g transform="translate(138.44915 154.938575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_67">
         <!-- 40 -->
         <g style="fill:#ffffff;" transform="translate(135.2679 181.549775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-34">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_68">
         <!-- 0 -->
         <g transform="translate(138.44915 208.160975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_69">
         <!-- 0 -->
         <g transform="translate(138.44915 234.772175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_70">
         <!-- 2 -->
         <g transform="translate(138.44915 261.383375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
         </g>
        </g>
        <g id="text_71">
         <!-- 0 -->
         <g transform="translate(138.44915 287.994575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_72">
         <!-- 0 -->
         <g transform="translate(138.44915 314.605775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_73">
         <!-- 0 -->
         <g transform="translate(165.06035 75.104975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_74">
         <!-- 0 -->
         <g transform="translate(165.06035 101.716175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_75">
         <!-- 0 -->
         <g transform="translate(165.06035 128.327375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_76">
         <!-- 0 -->
         <g transform="translate(165.06035 154.938575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_77">
         <!-- 0 -->
         <g transform="translate(165.06035 181.549775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_78">
         <!-- 33 -->
         <g style="fill:#ffffff;" transform="translate(161.8791 208.160975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-33">
          </use>
         </g>
        </g>
        <g id="text_79">
         <!-- 0 -->
         <g transform="translate(165.06035 234.772175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_80">
         <!-- 0 -->
         <g transform="translate(165.06035 261.383375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_81">
         <!-- 2 -->
         <g transform="translate(165.06035 287.994575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
         </g>
        </g>
        <g id="text_82">
         <!-- 1 -->
         <g transform="translate(165.06035 314.605775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_83">
         <!-- 0 -->
         <g transform="translate(191.67155 75.104975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_84">
         <!-- 0 -->
         <g transform="translate(191.67155 101.716175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_85">
         <!-- 0 -->
         <g transform="translate(191.67155 128.327375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_86">
         <!-- 0 -->
         <g transform="translate(191.67155 154.938575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_87">
         <!-- 0 -->
         <g transform="translate(191.67155 181.549775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_88">
         <!-- 0 -->
         <g transform="translate(191.67155 208.160975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_89">
         <!-- 26 -->
         <g style="fill:#ffffff;" transform="translate(188.4903 234.772175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-36">
          </use>
         </g>
        </g>
        <g id="text_90">
         <!-- 0 -->
         <g transform="translate(191.67155 261.383375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_91">
         <!-- 0 -->
         <g transform="translate(191.67155 287.994575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_92">
         <!-- 0 -->
         <g transform="translate(191.67155 314.605775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_93">
         <!-- 0 -->
         <g transform="translate(218.28275 75.104975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_94">
         <!-- 0 -->
         <g transform="translate(218.28275 101.716175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_95">
         <!-- 0 -->
         <g transform="translate(218.28275 128.327375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_96">
         <!-- 0 -->
         <g transform="translate(218.28275 154.938575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_97">
         <!-- 0 -->
         <g transform="translate(218.28275 181.549775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_98">
         <!-- 0 -->
         <g transform="translate(218.28275 208.160975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_99">
         <!-- 0 -->
         <g transform="translate(218.28275 234.772175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_100">
         <!-- 39 -->
         <g style="fill:#ffffff;" transform="translate(215.1015 261.383375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-39">
          </use>
         </g>
        </g>
        <g id="text_101">
         <!-- 0 -->
         <g transform="translate(218.28275 287.994575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_102">
         <!-- 0 -->
         <g transform="translate(218.28275 314.605775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_103">
         <!-- 0 -->
         <g transform="translate(244.89395 75.104975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_104">
         <!-- 0 -->
         <g transform="translate(244.89395 101.716175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_105">
         <!-- 0 -->
         <g transform="translate(244.89395 128.327375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_106">
         <!-- 0 -->
         <g transform="translate(244.89395 154.938575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_107">
         <!-- 0 -->
         <g transform="translate(244.89395 181.549775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_108">
         <!-- 1 -->
         <g transform="translate(244.89395 208.160975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_109">
         <!-- 0 -->
         <g transform="translate(244.89395 234.772175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_110">
         <!-- 0 -->
         <g transform="translate(244.89395 261.383375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_111">
         <!-- 28 -->
         <g style="fill:#ffffff;" transform="translate(241.7127 287.994575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-38">
          </use>
         </g>
        </g>
        <g id="text_112">
         <!-- 0 -->
         <g transform="translate(244.89395 314.605775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_113">
         <!-- 0 -->
         <g transform="translate(271.50515 75.104975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_114">
         <!-- 0 -->
         <g transform="translate(271.50515 101.716175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_115">
         <!-- 0 -->
         <g transform="translate(271.50515 128.327375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_116">
         <!-- 0 -->
         <g transform="translate(271.50515 154.938575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_117">
         <!-- 0 -->
         <g transform="translate(271.50515 181.549775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_118">
         <!-- 1 -->
         <g transform="translate(271.50515 208.160975)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_119">
         <!-- 0 -->
         <g transform="translate(271.50515 234.772175)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_120">
         <!-- 0 -->
         <g transform="translate(271.50515 261.383375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_121">
         <!-- 1 -->
         <g transform="translate(271.50515 287.994575)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_122">
         <!-- 35 -->
         <g style="fill:#ffffff;" transform="translate(268.3239 314.605775)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-35">
          </use>
         </g>
        </g>
       </g>
       <g id="text_123">
        <!-- Confusion table -->
        <g style="fill:#262626;" transform="translate(102.227875 16.318125)scale(0.12 -0.12)">
         <defs>
          <path d="M 4122 4306 
L 4122 3641 
Q 3803 3938 3442 4084 
Q 3081 4231 2675 4231 
Q 1875 4231 1450 3742 
Q 1025 3253 1025 2328 
Q 1025 1406 1450 917 
Q 1875 428 2675 428 
Q 3081 428 3442 575 
Q 3803 722 4122 1019 
L 4122 359 
Q 3791 134 3420 21 
Q 3050 -91 2638 -91 
Q 1578 -91 968 557 
Q 359 1206 359 2328 
Q 359 3453 968 4101 
Q 1578 4750 2638 4750 
Q 3056 4750 3426 4639 
Q 3797 4528 4122 4306 
z
" id="DejaVuSans-43" transform="scale(0.015625)">
          </path>
          <path d="M 1959 3097 
Q 1497 3097 1228 2736 
Q 959 2375 959 1747 
Q 959 1119 1226 758 
Q 1494 397 1959 397 
Q 2419 397 2687 759 
Q 2956 1122 2956 1747 
Q 2956 2369 2687 2733 
Q 2419 3097 1959 3097 
z
M 1959 3584 
Q 2709 3584 3137 3096 
Q 3566 2609 3566 1747 
Q 3566 888 3137 398 
Q 2709 -91 1959 -91 
Q 1206 -91 779 398 
Q 353 888 353 1747 
Q 353 2609 779 3096 
Q 1206 3584 1959 3584 
z
" id="DejaVuSans-6f" transform="scale(0.015625)">
          </path>
          <path d="M 3513 2113 
L 3513 0 
L 2938 0 
L 2938 2094 
Q 2938 2591 2744 2837 
Q 2550 3084 2163 3084 
Q 1697 3084 1428 2787 
Q 1159 2491 1159 1978 
L 1159 0 
L 581 0 
L 581 3500 
L 1159 3500 
L 1159 2956 
Q 1366 3272 1645 3428 
Q 1925 3584 2291 3584 
Q 2894 3584 3203 3211 
Q 3513 2838 3513 2113 
z
" id="DejaVuSans-6e" transform="scale(0.015625)">
          </path>
          <path d="M 2375 4863 
L 2375 4384 
L 1825 4384 
Q 1516 4384 1395 4259 
Q 1275 4134 1275 3809 
L 1275 3500 
L 2222 3500 
L 2222 3053 
L 1275 3053 
L 1275 0 
L 697 0 
L 697 3053 
L 147 3053 
L 147 3500 
L 697 3500 
L 697 3744 
Q 697 4328 969 4595 
Q 1241 4863 1831 4863 
L 2375 4863 
z
" id="DejaVuSans-66" transform="scale(0.015625)">
          </path>
          <path d="M 2834 3397 
L 2834 2853 
Q 2591 2978 2328 3040 
Q 2066 3103 1784 3103 
Q 1356 3103 1142 2972 
Q 928 2841 928 2578 
Q 928 2378 1081 2264 
Q 1234 2150 1697 2047 
L 1894 2003 
Q 2506 1872 2764 1633 
Q 3022 1394 3022 966 
Q 3022 478 2636 193 
Q 2250 -91 1575 -91 
Q 1294 -91 989 -36 
Q 684 19 347 128 
L 347 722 
Q 666 556 975 473 
Q 1284 391 1588 391 
Q 1994 391 2212 530 
Q 2431 669 2431 922 
Q 2431 1156 2273 1281 
Q 2116 1406 1581 1522 
L 1381 1569 
Q 847 1681 609 1914 
Q 372 2147 372 2553 
Q 372 3047 722 3315 
Q 1072 3584 1716 3584 
Q 2034 3584 2315 3537 
Q 2597 3491 2834 3397 
z
" id="DejaVuSans-73" transform="scale(0.015625)">
          </path>
         </defs>
         <use xlink:href="#DejaVuSans-43">
         </use>
         <use x="69.824219" xlink:href="#DejaVuSans-6f">
         </use>
         <use x="131.005859" xlink:href="#DejaVuSans-6e">
         </use>
         <use x="194.384766" xlink:href="#DejaVuSans-66">
         </use>
         <use x="229.589844" xlink:href="#DejaVuSans-75">
         </use>
         <use x="292.96875" xlink:href="#DejaVuSans-73">
         </use>
         <use x="345.068359" xlink:href="#DejaVuSans-69">
         </use>
         <use x="372.851562" xlink:href="#DejaVuSans-6f">
         </use>
         <use x="434.033203" xlink:href="#DejaVuSans-6e">
         </use>
         <use x="497.412109" xlink:href="#DejaVuSans-20">
         </use>
         <use x="529.199219" xlink:href="#DejaVuSans-74">
         </use>
         <use x="568.408203" xlink:href="#DejaVuSans-61">
         </use>
         <use x="629.6875" xlink:href="#DejaVuSans-62">
         </use>
         <use x="693.164062" xlink:href="#DejaVuSans-6c">
         </use>
         <use x="720.947266" xlink:href="#DejaVuSans-65">
         </use>
        </g>
       </g>
      </g>
      <defs>
       <clippath id="ped74d1a50b">
        <rect height="266.112" width="266.112" x="21.88" y="59.04">
        </rect>
       </clippath>
      </defs>
     </svg>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <p>
    This was the last part of a 5-part tutorial on how to implement neural networks from scratch in Python:
   </p>
   <ul>
    <li>
     <a href="{% post_url 2015-06-10-neural-network-implementation-part01 %}">
      Part 1: Gradient descent
     </a>
    </li>
    <li>
     <a href="{% post_url 2015-06-10-neural-network-implementation-part02 %}">
      Part 2: Classification
     </a>
    </li>
    <li>
     <a href="{% post_url 2015-06-10-neural-network-implementation-part03 %}">
      Part 3: Hidden layers trained by backpropagation
     </a>
    </li>
    <li>
     <a href="{% post_url 2015-06-10-neural-network-implementation-part04 %}">
      Part 4: Vectorization of the operations
     </a>
    </li>
    <li>
     <a href="{% post_url 2015-06-10-neural-network-implementation-part05 %}">
      Part 5: Generalization to multiple layers (this)
     </a>
    </li>
   </ul>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [19]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa fa-1x fa-minus-square-o">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Python package versions used</span>
<span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> --python
<span class="o">%</span><span class="k">watermark</span> --iversions
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>Python implementation: CPython
Python version       : 3.9.2
IPython version      : 7.23.0

numpy     : 1.20.2
matplotlib: 3.4.1
seaborn   : 0.11.1
sklearn   : 0.24.2

</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h6 id="Version-history">
    Version history
    <a class="anchor-link" href="#Version-history">
     ¶
    </a>
   </h6>
   <ul>
    <li>
     <em>
      2015-06-10
     </em>
     : Notebook (left unpublished as blogpost)
    </li>
    <li>
     <em>
      2021-05-05
     </em>
     : Updated packages, use watermark for versions, minor edits in text, published post.
    </li>
   </ul>
  </div>
 </div>
</div>

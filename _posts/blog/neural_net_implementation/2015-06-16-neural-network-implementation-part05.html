---
layout: post
title: How to implement a neural network (5/5) - Generalization to multiple layers
description: Generalization of neural networks to multiple layers. Illustrated on a simple network build from scratch using Python and NumPy. The network is trained on a digit classification toy problem using stochastic gradient descent.
tags:
- Neural Networks
- Machine Learning
- NumPy
- Classification
- Backpropagation
- Gradient Descent
- Notebook
---

<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h1 id="How-to-implement-a-neural-network-(5/5)---Generalization-to-multiple-layers">
    How to implement a neural network (5/5) - Generalization to multiple layers
    <a class="anchor-link" href="#How-to-implement-a-neural-network-(5/5)---Generalization-to-multiple-layers">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h1>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <p>
    This last part of our tutorial on how to implement a neural network, generalizes the single layer feedforward neural network into any number of layers. The concepts of a linear projection via matrix multiplication and non-linear transformation will be generalized. The usage of the generalization will be illustrated by building a small feedforward network that consists of two hidden layers to classify handwritten digits. This network will be trained by
    <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">
     stochastic gradient descent
    </a>
    , a popular variant of gradient descent that updates the parameters each step on only a subset of the parameters.
   </p>
   <p>
    This part will cover:
   </p>
   <ul>
    <li>
     <a href="http://peterroelants.github.io/posts/neural_network_implementation_part05/#Generalization-of-the-layers">
      Generalization to multiple layers
     </a>
    </li>
    <li>
     Minibatches with
     <a href="http://peterroelants.github.io/posts/neural_network_implementation_part05/#Stochastic-gradient-descent-backpropagation">
      stochastic gradient descent
     </a>
    </li>
   </ul>
   <p>
    This is the last part of a 5-part tutorial on how to implement neural networks from scratch in Python:
   </p>
   <ul>
    <li>
     <a href="{% post_url /blog/neural_net_implementation/2015-06-12-neural-network-implementation-part01 %}">
      Part 1: Gradient descent
     </a>
    </li>
    <li>
     <a href="{% post_url /blog/neural_net_implementation/2015-06-13-neural-network-implementation-part02 %}">
      Part 2: Classification
     </a>
    </li>
    <li>
     <a href="{% post_url /blog/neural_net_implementation/2015-06-14-neural-network-implementation-part03 %}">
      Part 3: Hidden layers trained by backpropagation
     </a>
    </li>
    <li>
     <a href="{% post_url /blog/neural_net_implementation/2015-06-15-neural-network-implementation-part04 %}">
      Part 4: Vectorization of the operations
     </a>
    </li>
    <li>
     <a href="{% post_url /blog/neural_net_implementation/2015-06-16-neural-network-implementation-part05 %}">
      Part 5: Generalization to multiple layers (this)
     </a>
    </li>
   </ul>
   <p>
    The notebook starts out with importing the libraries we need:
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [1]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Imports</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_formats = ['svg']

<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>  <span class="c1"># Matrix and vector computation package</span>
<span class="c1"># data and evaluation utils</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">model_selection</span><span class="p">,</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  <span class="c1"># Plotting library</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">colorConverter</span><span class="p">,</span> <span class="n">ListedColormap</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>  <span class="c1"># Fancier plots</span>

<span class="c1"># Set seaborn plotting style</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">'darkgrid'</span><span class="p">)</span>
<span class="c1"># Set the seed for reproducability</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Handwritten-digits-dataset">
    Handwritten digits dataset
    <a class="anchor-link" href="#Handwritten-digits-dataset">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h2>
   <p>
    The dataset used in this tutorial is the
    <a href="http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html">
     digits dataset
    </a>
    provided by
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html">
     scikit-learn
    </a>
    . This dataset consists of 1797 8x8 images of handwritten digits between 0 and 9. Each 8x8 pixel image is provided as a flattened input vector of 64 variables. Example images for each digit are shown below. Note that this dataset is different from the larger and more famous
    <a href="https://en.wikipedia.org/wiki/MNIST_database">
     MNIST
    </a>
    dataset. The smaller dataset from scikit-learn was chosen to minimize training time for this tutorial. Feel free to experiment and adapt this tutorial to classify the MNIST digits.
   </p>
   <p>
    The dataset will be split into:
   </p>
   <ul>
    <li>
     A training set used to train the model. (inputs:
     <code>
      X_train
     </code>
     , targets:
     <code>
      T_train
     </code>
     )
    </li>
    <li>
     A validation set used to validate the model performance and to stop training if the model starts
     <a href="https://en.wikipedia.org/wiki/Overfitting">
      overfitting
     </a>
     on the training data. (inputs:
     <code>
      X_validation
     </code>
     , targets:
     <code>
      T_validation
     </code>
     )
    </li>
    <li>
     A final
     <a href="https://en.wikipedia.org/wiki/Test_set">
      test set
     </a>
     to evaluate the trained model on data is independent of the training and validation data. (inputs:
     <code>
      X_test
     </code>
     , targets:
     <code>
      T_test
     </code>
     )
    </li>
   </ul>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [2]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Load the data from scikit-learn.</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>

<span class="c1"># Load the targets.</span>
<span class="c1"># Note that the targets are stored as digits, these need to be </span>
<span class="c1">#  converted to one-hot-encoding for the output sofmax layer.</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">T</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">T</span><span class="p">)),</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Divide the data into a train and test set.</span>
<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">T_train</span><span class="p">,</span> <span class="n">T_test</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="c1"># Divide the test set into a validation set and final test set.</span>
<span class="p">(</span><span class="n">X_validation</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">T_validation</span><span class="p">,</span> <span class="n">T_test</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">T_test</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [3]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Plot an example of each image.</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'binary'</span><span class="p">)</span> 
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_svg output_subarea">
     <?xml version="1.0" encoding="utf-8" standalone="no"?>
     <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
     <svg height="61.688136pt" version="1.1" viewbox="0 0 572.4 61.688136" width="572.4pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <metadata>
       <rdf:rdf xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
        <cc:work>
         <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage">
         </dc:type>
         <dc:date>
          2021-05-13T19:56:55.424464
         </dc:date>
         <dc:format>
          image/svg+xml
         </dc:format>
         <dc:creator>
          <cc:agent>
           <dc:title>
            Matplotlib v3.4.2, https://matplotlib.org/
           </dc:title>
          </cc:agent>
         </dc:creator>
        </cc:work>
       </rdf:rdf>
      </metadata>
      <defs>
       <style type="text/css">
        *{stroke-linecap:butt;stroke-linejoin:round;}
       </style>
      </defs>
      <g id="figure_1">
       <g id="patch_1">
        <path d="M 0 61.688136 
L 572.4 61.688136 
L 572.4 0 
L 0 0 
z
" style="fill:#ffffff;">
        </path>
       </g>
       <g id="axes_1">
        <g clip-path="url(#p1d30f46eea)">
         <image height="48" id="image9d8a470af4" transform="scale(1 -1)translate(0 -48)" width="48" x="7.2" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAABE0lEQVR4nO2YUY2FQBRD923WABIACSABC1jAAhbAAhJAAlgAByABJLAG2o+bvKSZpOfzAAPNTZphPu/7vj8BpmmCfhgG6Ou6hn6e58hrKb9fWUWIA6hxADUf1kLXdcEHqqqCnrXTsizQn+cJ/b7v0DOSn4ADqHEANX/sAmshtrdp2xb6pmlCfhxH6Pu+hz75CTiAGgdQQ1voeR7o2V6IkWUZ9KzN2HsZyU/AAdQ4gBraQqw91nWFnp0LRdePkvwEHECNA6gJt9BxHNCzP7iiKKBn5z/RvVbyE3AANQ6ghrYQa4M8z6EvyxJ6dl60bRv0XdexT4IkPwEHUOMAamgLMaKnx2xPdd936H5G8hNwADUOoOYfO7FC+NkVC/kAAAAASUVORK5CYII=" y="-6.488136"/>
        </g>
       </g>
       <g id="axes_2">
        <g clip-path="url(#pe803180fad)">
         <image height="48" id="imagecbbb37fcc7" transform="scale(1 -1)translate(0 -48)" width="48" x="63.945763" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAAA30lEQVR4nO2ZUQ2FMAxFHy8ImJNNAjiYgyFlEnAwLOAEKUzBnoH2o0mT+5bc83kDKydNGijLGGN8HMg5i/l932JeShHz67pMdb+mq/8QCqChAJrVesP7vmKuTRuNbduspUWm7wAF0FAADQXQUAANBdCY34W8SCm5nDN9ByiAhgJo1Cl0nqeY11pdCocQXM6ZvgMUQEMBNIt1O/08j5hre57eu5i31sT8OA7L48zfAQqgoQAa8xeZ9iWlTSFta61NMyvTd4ACaCiAxm0vpP07izGK+b7vLnWn7wAF0FAAzQ/UPCrnpZoNsQAAAABJRU5ErkJggg==" y="-6.488136"/>
        </g>
       </g>
       <g id="axes_3">
        <g clip-path="url(#p5f00fd6144)">
         <image height="48" id="imagef8523fd1c3" transform="scale(1 -1)translate(0 -48)" width="48" x="120.691525" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAABEklEQVR4nO2YYQmEQBSEz+P+awQjaANNoBFsoBG0gRGMoAmsoA2MYIO9AjPIg4N3C/P9/MRdhweDu0kIIbx+wHEc0I/jCP22bdD3fQ/9PM/Qv58+7N9RAG8UwJsPe8Bapes66M/zNG3cNA30dV2b1ol+AgrgjQJ4Q1toWRborW2zriv0rIWsRD8BBfBGAbxJ2Insvm/4AjsZsba5rsvksyyDnhH9BBTAGwXwhrYQg7UTa6FhGKCvqsq0DiP6CSiANwrgjbmF2G0za4+2baGfpgl662V59BNQAG8UwBtzC7Fb67IsoU/TFHp28mInNUb0E1AAbxTAG3o7zSiKAvp936Fn/0J5nlu3hkQ/AQXwRgG8+QIB11RuPnyUZwAAAABJRU5ErkJggg==" y="-6.488136"/>
        </g>
       </g>
       <g id="axes_4">
        <g clip-path="url(#pefd5197998)">
         <image height="48" id="image1e5de4d19d" transform="scale(1 -1)translate(0 -48)" width="48" x="177.437288" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAAA/0lEQVR4nO2YYQ2DYAxExzIDSCBIwAJYAQtIAAkgAQ1YAQtIYAZ62S5Zcmty72fzUXhpculHcd/3/SCY5zmsL8vCtHl0XfeTPk/q9B9iATUWUPNiH7iuK6yv60qd7/ueei9Kp/QTsIAaC6ihU2iaJuo8SiG0C5VlSfVPPwELqLGAmoK9kSFQ2rRtS/XZ9z2so3RKPwELqLGAGnoXOs8zrNd1HdZRCm3bFta9C2XDAmrSC8AUQmmDUqKqqrB+HAfVhyX9BCygxgJqYAqhv81ohxnHMawPwxDW0c2LvcGln4AF1FhADUwh9Bca7TAobdCO1DTNp2/7ivQTsIAaC6h5A6LpPY7iHOShAAAAAElFTkSuQmCC" y="-6.488136"/>
        </g>
       </g>
       <g id="axes_5">
        <g clip-path="url(#p40f5581ad4)">
         <image height="48" id="image956789ee98" transform="scale(1 -1)translate(0 -48)" width="48" x="234.183051" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAABD0lEQVR4nO2ZQa2EQBBE//zsHSSAAizgAByAA5CAAywgAQdgAQc4ABSwBqoOk0zS20m94wswqXTS6WnC+77vXwLO84S+LEvot22Dvq7rqHP/o57+QRTAGgWw5pPqQ/d9Q59lGfTLskCvLuQNBbDGfYDAZqF93+ELbdtC/zwP9FVVQX8cB/TXdUGf5zn07iugANYogDV0FmJdhfmu66BnM08IAfp1XaHv+x569xVQAGsUwBrahZqmgX6eZ+inaYo6eBgG6Nl+ieG+AgpgjQJYQ29kDLb/YfuccRyjnmcURQG9+woogDUKYE30dprtZ9iNic02rKvE4r4CCmCNAliT7B8Z60LMp8J9BRTAGgWw5gtKqERmLsMhawAAAABJRU5ErkJggg==" y="-6.488136"/>
        </g>
       </g>
       <g id="axes_6">
        <g clip-path="url(#pb2f5a5936d)">
         <image height="48" id="imagedb5d080a16" transform="scale(1 -1)translate(0 -48)" width="48" x="290.928814" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAABB0lEQVR4nO2ZUcmEUBSEd5f/XSMYRRvYwAgawQZqAyuYQG1gBCNoArfAnIeBhfkvzPf4cTkwHBi81/fzPM+LoOs66KdpYsa8mqaBfp5nas6HOv0PcQA1DqDmzbbQsizQZ1kGfd/30O/7Dv26rtCXZQl98htwADUOoIZuIZZt26Cvqgr6tm2hH8cR+uQ34ABqHEDN368GRW1zHAc1pygK6nzyG3AANQ6ghm6huq6hj25qEcMwQB+9O0UkvwEHUOMAaugWOs+TOh+9F0XfSNd1QZ/nOfTJb8AB1DiAGrqFopaIuO8b+ugGF7VNRPIbcAA1DqCGbqHo3Sb6h8X+eWdJfgMOoMYB1HwBnYc/vJk9QYcAAAAASUVORK5CYII=" y="-6.488136"/>
        </g>
       </g>
       <g id="axes_7">
        <g clip-path="url(#p4b7bd4cf74)">
         <image height="48" id="image525a5d7d5e" transform="scale(1 -1)translate(0 -48)" width="48" x="347.674576" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAAA9ElEQVR4nO2ZUQmFUBBEnw8DGMEIRrCBRtIINjCCEYygDYygCe4rMPsxIMxbmPM5oNzDwuB6q1JK+RDc9w3zaZpgvq4rzPu+p97TdR3MvzBNhAXUWEBNzT4QtcqyLDAfhgHm+77DfNs2mLuF/hULqEkv8FoLRVzXBfPneWA+jiP1/vQTsIAaC6ip2I2saRqYR98qUatEedu2zHHyT8ACaiyghm6hqCWidjqOgzwSR/oJWECNBdTQG1nUNud5wjxqoejbiSX9BCygxgJq6Baa5xnm0YbF/m1mST8BC6ixgBp6I4uIbt6jm/23NrX0E7CAGguo+QG76UGuS/HnfQAAAABJRU5ErkJggg==" y="-6.488136"/>
        </g>
       </g>
       <g id="axes_8">
        <g clip-path="url(#p67a3a41730)">
         <image height="48" id="imageb45196e9cc" transform="scale(1 -1)translate(0 -48)" width="48" x="404.420339" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAAA+UlEQVR4nO2ZUQ2EQBBDby/3D1KQgANwABKwgAOkIAELOEACDvYMTD+acNdM0vfZwGyaJpNmt9Ra64ug67pQX9c11IdhYMbTvH86/Q/YgBobUPNhf+j7PtSnaQr167pCvW1b9uiQ9AnYgBobUFPYLgQHlRLq+76H+lMdKX0CNqDGBtTALYQ6zLIsoX4cB3Uw2k6oayHSJ2ADamxADdxCaKuw22bbtke+n+c51NMnYANqbEANvBdCnYTtKgjUhcZxpOakT8AG1NiAGvpeiO026E0NzbnvO9RRB0ufgA2osQE19BsZAm0PdI90nmeoN01DzU+fgA2osQE1X/ApSAsQJDzRAAAAAElFTkSuQmCC" y="-6.488136"/>
        </g>
       </g>
       <g id="axes_9">
        <g clip-path="url(#pc8f36ab0ac)">
         <image height="48" id="imagea3eb0588e5" transform="scale(1 -1)translate(0 -48)" width="48" x="461.166102" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAABEklEQVR4nO2YQQ2DUBBE26Z3kIADQAHgAAcgAQcgAQlYwAFOQAIo+DUwc9jTdpN5x0cLTDaZLP+dUkovA33fQ7/vO/RZlkHfti3027ZBn+c59B9oA6EA3iiAN2/WQvd9wz8URQH9dV0mX9c19PM8Q78sC/ThJ6AA3iiAN192gbXH8zzQr+tquk9ZltCP48heCRJ+AgrgjQJ4Q3chxnEc0HddZ3rweZ7Qs12LEX4CCuCNAnhDdyEGO59hNE0DPTsXYi3H2in8BBTAGwXwxtxC7PR4GAbT71mbsS84tdC/ogDehA9g/iJjOwxrDwZrFbYLMcJPQAG8UQBvzLtQVVXQsxZip83TNFkfDQk/AQXwRgG8+QE0/ku/eBvHIgAAAABJRU5ErkJggg==" y="-6.488136"/>
        </g>
       </g>
       <g id="axes_10">
        <g clip-path="url(#pe818f95965)">
         <image height="48" id="image33cd0ebc49" transform="scale(1 -1)translate(0 -48)" width="48" x="517.911864" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAAA+UlEQVR4nO2YXQ2EMBCE4XJCigIqAQk4oZKQgAQktA5wAA56BnYeNiGZ22S+xwk//bLJpO3Ye++Dg1KKmddazfx5HjPf993Mc86e5Qwf19N/iATYSIDN6G0hL6i1UAsdx2Hmy7KYefgJSICNBNi81kJoL4T2Nqidrusyc9RO4ScgATYSYPP1voBapbVm5tu2mTnaC6E2Q4SfgATYSIANbCF0n4Pa5r5vM1/X1bWglJLr+fATkAAbCbCBLYRORgjUNqhVvN9HhJ+ABNhIgA28F0ItMU3TKz+e59nMdSKLhgTYhBdw306jPQ86wZ3n6VySj/ATkAAbCbD5AavKUN4xkyAAAAAAAElFTkSuQmCC" y="-6.488136"/>
        </g>
       </g>
      </g>
      <defs>
       <clippath id="p1d30f46eea">
        <rect height="47.288136" width="47.288136" x="7.2" y="7.2">
        </rect>
       </clippath>
       <clippath id="pe803180fad">
        <rect height="47.288136" width="47.288136" x="63.945763" y="7.2">
        </rect>
       </clippath>
       <clippath id="p5f00fd6144">
        <rect height="47.288136" width="47.288136" x="120.691525" y="7.2">
        </rect>
       </clippath>
       <clippath id="pefd5197998">
        <rect height="47.288136" width="47.288136" x="177.437288" y="7.2">
        </rect>
       </clippath>
       <clippath id="p40f5581ad4">
        <rect height="47.288136" width="47.288136" x="234.183051" y="7.2">
        </rect>
       </clippath>
       <clippath id="pb2f5a5936d">
        <rect height="47.288136" width="47.288136" x="290.928814" y="7.2">
        </rect>
       </clippath>
       <clippath id="p4b7bd4cf74">
        <rect height="47.288136" width="47.288136" x="347.674576" y="7.2">
        </rect>
       </clippath>
       <clippath id="p67a3a41730">
        <rect height="47.288136" width="47.288136" x="404.420339" y="7.2">
        </rect>
       </clippath>
       <clippath id="pc8f36ab0ac">
        <rect height="47.288136" width="47.288136" x="461.166102" y="7.2">
        </rect>
       </clippath>
       <clippath id="pe818f95965">
        <rect height="47.288136" width="47.288136" x="517.911864" y="7.2">
        </rect>
       </clippath>
      </defs>
     </svg>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Generalization-of-the-layers">
    Generalization of the layers
    <a class="anchor-link" href="#Generalization-of-the-layers">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h2>
   <p>
    <a href="{% post_url /blog/neural_net_implementation/2015-06-15-neural-network-implementation-part04 %}">
     Part 4
    </a>
    of this tutorial series took on the classical view of neural networks where each layer consists of a
    <a href="https://en.wikipedia.org/wiki/Linear_map">
     linear transformation
    </a>
    by a
    <a href="https://en.wikipedia.org/wiki/Transformation_matrix">
     matrix multiplication
    </a>
    and vector addition followed by a non-linear function.
    <br/>
    Here the linear transformation is split from the non-linear function and each is abstracted into its own layer. This has the benefit that the forward and backward step of each layer can easily be calculated separately.
   </p>
   <p>
    This tutorial defines three example layers as
    <a href="https://docs.python.org/3/tutorial/classes.html">
     Python classes
    </a>
    :
   </p>
   <ul>
    <li>
     A layer to apply the linear transformation (
     <code>
      LinearLayer
     </code>
     ).
    </li>
    <li>
     A layer to apply the logistic function (
     <code>
      LogisticLayer
     </code>
     ).
    </li>
    <li>
     A layer to compute the softmax classification probabilities at the output (
     <code>
      SoftmaxOutputLayer
     </code>
     ).
    </li>
   </ul>
   <p>
    Each layer can compute its output in the forward step with
    <code>
     get_output
    </code>
    , which can then be used as the input for the next layer. The gradient at the input of each layer in the backpropagation step can be computed with
    <code>
     get_input_grad
    </code>
    . This function computes the gradient with the help of the targets if it's the last layer, or the gradients at its outputs (gradients at input of next layer) if it's an intermediate layer.
Each layer has the option to
    <a href="https://docs.python.org/3/library/stdtypes.html#iterator-types">
     iterate
    </a>
    over the parameters (if any) with
    <code>
     get_params_iter
    </code>
    , and get the gradients of these parameters in the same order with
    <code>
     get_params_grad
    </code>
    .
   </p>
   <p>
    Notice that the gradient and cost computed by the softmax layer are divided by the number of input samples. This is to make this gradient and cost independent of the number of input samples so that the size of the mini-batches can be changed without affecting other parameters. More on this later.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [4]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Non-linear functions used</span>
<span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">logistic_deriv</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>  <span class="c1"># Derivative of logistic function</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [5]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Layers used in this model</span>
<span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Base class for the different layers.</span>
<span class="sd">    Defines base methods and documentation of methods."""</span>
    
    <span class="k">def</span> <span class="nf">get_params_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return an iterator over the parameters (if any).</span>
<span class="sd">        The iterator has the same order as get_params_grad.</span>
<span class="sd">        The elements returned by the iterator are editable in-place."""</span>
        <span class="k">return</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="nf">get_params_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return a list of gradients over the parameters.</span>
<span class="sd">        The list has the same order as the get_params_iter iterator.</span>
<span class="sd">        X is the input.</span>
<span class="sd">        output_grad is the gradient at the output of this layer.</span>
<span class="sd">        """</span>
        <span class="k">return</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step linear transformation.</span>
<span class="sd">        X is the input."""</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer.</span>
<span class="sd">        Y is the pre-computed output of this layer (not needed in </span>
<span class="sd">        this case).</span>
<span class="sd">        output_grad is the gradient at the output of this layer </span>
<span class="sd">         (gradient at input of next layer).</span>
<span class="sd">        Output layer uses targets T to compute the gradient based on the </span>
<span class="sd">         output error instead of output_grad"""</span>
        <span class="k">pass</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [6]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""The linear layer performs a linear transformation to its input."""</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
        <span class="sd">"""Initialize hidden layer parameters.</span>
<span class="sd">        n_in is the number of input variables.</span>
<span class="sd">        n_out is the number of output variables."""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_out</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">get_params_iter</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return an iterator over the parameters."""</span>
        <span class="k">return</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]),</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">'readwrite'</span><span class="p">]))</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step linear transformation."""</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        
    <span class="k">def</span> <span class="nf">get_params_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return a list of gradients over the parameters."""</span>
        <span class="n">JW</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">output_grad</span>
        <span class="n">Jb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">output_grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">g</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">JW</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">Jb</span><span class="p">))]</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="k">return</span> <span class="n">output_grad</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [7]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="k">class</span> <span class="nc">LogisticLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""The logistic layer applies the logistic function to its inputs."""</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="n">logistic</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">logistic_deriv</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="n">output_grad</span><span class="p">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [8]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="k">class</span> <span class="nc">SoftmaxOutputLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""The softmax output layer computes the classification </span>
<span class="sd">    propabilities at the output."""</span>
    
    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">"""Perform the forward step transformation."""</span>
        <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_input_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the gradient at the inputs of this layer."""</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">get_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="sd">"""Return the cost at the output of this output layer."""</span>
        <span class="k">return</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Sample-model">
    Sample model
    <a class="anchor-link" href="#Sample-model">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h3>
   <p>
    The following sections will refer to a layer as a layer defined above, and hidden-layer or output-layer as the classical neural network view of a linear transformation followed by a non-linear function.
   </p>
   <p>
    The sample model used to classify the handwritten digits in this tutorial consists of two hidden-layers with logistic functions and a softmax output-layer. The fist hidden-layer takes a vector of 64 pixel values and transforms them to a vector of 20 values. The second hidden-layer projects the previous 20 values to 20 new values. The output-layer outputs probabilities for the 10 possible classes. This architecture is illustrated in the following figure (biases are not shown to keep figure clean).
   </p>
   <p>
    <img alt="Image of the sample neural network model" src="/images/neural_net_implementation/SimpleANN05.png"/>
   </p>
   <p>
    The full network is represented as a sequential list where each next layer is added on top of the previous layer by putting it in the next position in the list. The first layer is at position 0 in this list, the last layer is at the last index of this list.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [9]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Sample model to be trained on the data</span>
<span class="n">hidden_neurons_1</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of neurons in the first hidden-layer</span>
<span class="n">hidden_neurons_2</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Number of neurons in the second hidden-layer</span>
<span class="c1"># Create the model</span>
<span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># Define a list of layers</span>
<span class="c1"># Add first hidden layer</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">hidden_neurons_1</span><span class="p">))</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LogisticLayer</span><span class="p">())</span>
<span class="c1"># Add second hidden layer</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_neurons_1</span><span class="p">,</span> <span class="n">hidden_neurons_2</span><span class="p">))</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LogisticLayer</span><span class="p">())</span>
<span class="c1"># Add output layer</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_neurons_2</span><span class="p">,</span> <span class="n">T_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SoftmaxOutputLayer</span><span class="p">())</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Backpropagation">
    Backpropagation
    <a class="anchor-link" href="#Backpropagation">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h2>
   <p>
    The details of how backpropagation works in the forward and backward step of vectorized model are explained in
    <a href="{% post_url /blog/neural_net_implementation/2015-06-15-neural-network-implementation-part04 %}">
     part 4
    </a>
    of this tutorial. This section will only illustrate how to perform backpropagation over any number of layers with the generalized model described here.
   </p>
   <h3 id="Forward-step">
    Forward step
    <a class="anchor-link" href="#Forward-step">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h3>
   <p>
    The forward steps are computed by the
    <code>
     forward_step
    </code>
    method defined below. This method iteratively computes the outputs of each layer and feeds it as input to the next layer until the last layer. Each layer's output is computed by calling the
    <code>
     get_output
    </code>
    method. These output activations are stored in the
    <code>
     activations
    </code>
    list.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [10]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Forward propagation step as a method.</span>
<span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="n">input_samples</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Compute and return the forward activation of each layer in layers.</span>
<span class="sd">    Input:</span>
<span class="sd">        input_samples: A matrix of input samples (each row </span>
<span class="sd">                       is an input vector)</span>
<span class="sd">        layers: A list of Layers</span>
<span class="sd">    Output:</span>
<span class="sd">        A list of activations where the activation at each index </span>
<span class="sd">        i+1 corresponds to the activation of layer i in layers. </span>
<span class="sd">        activations[0] contains the input samples.  </span>
<span class="sd">    """</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_samples</span><span class="p">]</span> <span class="c1"># List of layer activations</span>
    <span class="c1"># Compute the forward activations for each layer starting </span>
    <span class="c1">#  from the first</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">input_samples</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
        <span class="c1"># Get the output of the current layer</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Store the output for future processing</span>
        <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
        <span class="c1"># Set the current input as the activations of the previous layer</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">activations</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="Backward-step">
    Backward step
    <a class="anchor-link" href="#Backward-step">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h3>
   <p>
    The gradients computed in the backward step are computed by the
    <code>
     backward_step
    </code>
    method defined below. The backward step goes over all the layers in the reversed order. It first gets the initial gradients from the output layer and uses these gradients to compute the gradients of the layers below by iteratively calling the
    <code>
     get_input_grad
    </code>
    method. During each step, it computes the gradients of the cost with respect to the parameters by calling the
    <code>
     get_params_grad
    </code>
    method and returns them in a list.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [11]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Define the backward propagation step as a method</span>
<span class="k">def</span> <span class="nf">backward_step</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Perform the backpropagation step over all the layers and return the parameter gradients.</span>
<span class="sd">    Input:</span>
<span class="sd">        activations: A list of forward step activations where the activation at </span>
<span class="sd">            each index i+1 corresponds to the activation of layer i in layers. </span>
<span class="sd">            activations[0] contains the input samples. </span>
<span class="sd">        targets: The output targets of the output layer.</span>
<span class="sd">        layers: A list of Layers corresponding that generated the outputs in activations.</span>
<span class="sd">    Output:</span>
<span class="sd">        A list of parameter gradients where the gradients at each index corresponds to</span>
<span class="sd">        the parameters gradients of the layer at the same index in layers. </span>
<span class="sd">    """</span>
    <span class="n">param_grads</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">()</span>  <span class="c1"># List of parameter gradients for each layer</span>
    <span class="n">output_grad</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># The error gradient at the output of the current layer</span>
    <span class="c1"># Propagate the error backwards through all the layers.</span>
    <span class="c1">#  Use reversed to iterate backwards over the list of layers.</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>   
        <span class="n">Y</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>  <span class="c1"># Get the activations of the last layer on the stack</span>
        <span class="c1"># Compute the error at the output layer.</span>
        <span class="c1"># The output layer error is calculated different then hidden layer error.</span>
        <span class="k">if</span> <span class="n">output_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_grad</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_input_grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># output_grad is not None (layer is not output layer)</span>
            <span class="n">input_grad</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_input_grad</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
        <span class="c1"># Get the input of this layer (activations of the previous layer)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Compute the layer parameter gradients used to update the parameters</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_params_grad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">output_grad</span><span class="p">)</span>
        <span class="n">param_grads</span><span class="o">.</span><span class="n">appendleft</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="c1"># Compute gradient at output of previous layer (input of current layer):</span>
        <span class="n">output_grad</span> <span class="o">=</span> <span class="n">input_grad</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">param_grads</span><span class="p">)</span>  <span class="c1"># Return the parameter gradients</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Gradient-Checking">
    Gradient Checking
    <a class="anchor-link" href="#Gradient-Checking">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h2>
   <p>
    As in
    <a href="{% post_url /blog/neural_net_implementation/2015-06-15-neural-network-implementation-part04 %}">
     part 4
    </a>
    of this tutorial the gradient computed by backpropagation is compared with the
    <a href="https://en.wikipedia.org/wiki/Numerical_differentiation">
     numerical gradient
    </a>
    to assert that there are no bugs in the code to compute the gradients.
   </p>
   <p>
    The code below gets the parameters of each layer by the help of the
    <code>
     get_params_iter
    </code>
    method that returns an iterator over all the parameters in the layer. The order of parameters returned corresponds to the order of parameter gradients returned by
    <code>
     get_params_grad
    </code>
    during backpropagation.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [12]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Perform gradient checking</span>
<span class="c1"># Test the gradients on a subset of the data</span>
<span class="n">nb_samples_gradientcheck</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">X_temp</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">nb_samples_gradientcheck</span><span class="p">,:]</span>
<span class="n">T_temp</span> <span class="o">=</span> <span class="n">T_train</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">nb_samples_gradientcheck</span><span class="p">,:]</span>
<span class="c1"># Get the parameter gradients with backpropagation</span>
<span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
<span class="n">param_grads</span> <span class="o">=</span> <span class="n">backward_step</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">T_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>

<span class="c1"># Set the small change to compute the numerical gradient</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="c1"># Compute the numerical gradients of the parameters in all layers.</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)):</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">layer_backprop_grads</span> <span class="o">=</span> <span class="n">param_grads</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="c1"># Compute the numerical gradient for each parameter in the layer</span>
    <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">()):</span>
        <span class="n">grad_backprop</span> <span class="o">=</span> <span class="n">layer_backprop_grads</span><span class="p">[</span><span class="n">p_idx</span><span class="p">]</span>
        <span class="c1"># + eps</span>
        <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
        <span class="n">plus_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span>
            <span class="n">forward_step</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_temp</span><span class="p">)</span>
        <span class="c1"># - eps</span>
        <span class="n">param</span> <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span>
        <span class="n">min_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span>
            <span class="n">forward_step</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">layers</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_temp</span><span class="p">)</span>
        <span class="c1"># reset param value</span>
        <span class="n">param</span> <span class="o">+=</span> <span class="n">eps</span>
        <span class="c1"># calculate numerical gradient</span>
        <span class="n">grad_num</span> <span class="o">=</span> <span class="p">(</span><span class="n">plus_cost</span> <span class="o">-</span> <span class="n">min_cost</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">eps</span><span class="p">)</span>
        <span class="c1"># Raise error if the numerical grade is not close to the </span>
        <span class="c1">#  backprop gradient</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">grad_num</span><span class="p">,</span> <span class="n">grad_backprop</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span>
                <span class="sa">f</span><span class="s1">'Numerical gradient of </span><span class="si">{</span><span class="n">grad_num</span><span class="si">:</span><span class="s1">.6f</span><span class="si">}</span><span class="s1"> is '</span>
                <span class="s1">'not close to the backpropagation gradient '</span>
                <span class="sa">f</span><span class="s1">'of </span><span class="si">{</span><span class="n">grad_backprop</span><span class="si">:</span><span class="s1">.6f</span><span class="si">}</span><span class="s1">!'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'No gradient errors found'</span><span class="p">)</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>No gradient errors found
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Stochastic-gradient-descent-backpropagation">
    Stochastic gradient descent backpropagation
    <a class="anchor-link" href="#Stochastic-gradient-descent-backpropagation">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h2>
   <p>
    This tutorial uses a variant of gradient descent called
    <a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">
     Stochastic gradient descent
    </a>
    (SGD) to optimize the cost function. SGD follows the negative gradient of the cost function on subsets of the total training set. This has a few benefits: One of them is that the training time on large datasets can be reduced because the matrix of samples is much smaller during each sub-iteration and the gradients can be computed faster and with less memory. Another benefit is that computing the cost function on subsets results in noise, i.e. each subset will give a different cost depending on the samples. This will result in noisy (stochastic) gradient updates which might be able to push the gradient descent out of local minima. These, and other, benefits contribute to the popularity of SGD on training large scale machine learning methods such as neural networks.
   </p>
   <p>
    The cost function needs to be independent of the number of input samples because the size of the subsets used during SGD can vary. This is why the
    <a href="https://en.wikipedia.org/wiki/Mean_squared_error">
     mean squared error
    </a>
    (MSE) cost function is used instead of just the squared error. Using the mean instead of the sum is reflected in the gradient and cost computed by the softmax layer being divided by the number of input samples.
   </p>
   <h3 id="Minibatches">
    Minibatches
    <a class="anchor-link" href="#Minibatches">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h3>
   <p>
    The subsets of the training set are often called mini-batches. The following code will divide the training set into mini-batches of around 25 samples per batch. The inputs and targets are combined together in a list of (input, target) tuples.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [13]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Create the minibatches</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">25</span>  <span class="c1"># Approximately 25 samples per batch</span>
<span class="n">nb_of_batches</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">batch_size</span>  <span class="c1"># Number of batches</span>
<span class="c1"># Create batches (X,Y) from the training set</span>
<span class="n">XT_batches</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">nb_of_batches</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>   <span class="c1"># X samples</span>
    <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">T_train</span><span class="p">,</span> <span class="n">nb_of_batches</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>  <span class="c1"># Y targets</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h3 id="SGD-updates">
    SGD updates
    <a class="anchor-link" href="#SGD-updates">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h3>
   <p>
    The parameters $\mathbf{\theta}$ of the network are updated by the
    <code>
     update_params
    </code>
    method that iterates over each parameter of each layer and applies the simple
    <a href="https://en.wikipedia.org/wiki/Gradient_descent">
     gradient descent
    </a>
    rule on each mini-batch: $\mathbf{\theta}(k+1) = \mathbf{\theta}(k) - \Delta \mathbf{\theta}(k+1)$. $\Delta \mathbf{\theta}$ is defined as: $\Delta \mathbf{\theta} = \mu \frac{\partial \xi}{\partial \mathbf{\theta}}$ with $\mu$ the learning rate.
   </p>
   <p>
    The update steps will be performed for a number of iterations (
    <code>
     nb_of_iterations
    </code>
    ) over the full training set, where each full iterations consists of multiple updates over the mini-batches. After each full iteration, the resulting network will be tested on the validation set. The training will stop if the cost on the validation set doesn't increase after three full iterations to prevent overfitting or after maximum 300 iterations. All costs will be stored in between for future analysis.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [14]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Define a method to update the parameters</span>
<span class="k">def</span> <span class="nf">update_params</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">param_grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Function to update the parameters of the given layers with the given </span>
<span class="sd">    gradients by gradient descent with the given learning rate.</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">layer_backprop_grads</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">param_grads</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">get_params_iter</span><span class="p">(),</span> 
                               <span class="n">layer_backprop_grads</span><span class="p">):</span>
            <span class="c1"># The parameter returned by the iterator point to the </span>
            <span class="c1">#  memory space of the original layer and can thus be </span>
            <span class="c1">#  modified inplace.</span>
            <span class="n">param</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>  <span class="c1"># Update each parameter</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [15]:
  </div>
  <div class="inner_cell">
   <div class="input_area">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Perform backpropagation</span>
<span class="c1"># initalize some lists to store the cost for future analysis        </span>
<span class="n">batch_costs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_costs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_costs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">max_nb_of_iterations</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># Train for a maximum of 300 iterations</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Gradient descent learning rate</span>

<span class="c1"># Train for the maximum number of iterations</span>
<span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_nb_of_iterations</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">T</span> <span class="ow">in</span> <span class="n">XT_batches</span><span class="p">:</span>  <span class="c1"># For each minibatch sub-iteration</span>
        <span class="c1"># Get the activations</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
        <span class="c1"># Get cost</span>
        <span class="n">batch_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">batch_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_cost</span><span class="p">)</span>
        <span class="c1"># Get the gradients</span>
        <span class="n">param_grads</span> <span class="o">=</span> <span class="n">backward_step</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
        <span class="c1"># Update the parameters</span>
        <span class="n">update_params</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">param_grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
    <span class="c1"># Get full training cost for future analysis (plots)</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
    <span class="n">train_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_train</span><span class="p">)</span>
    <span class="n">train_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_cost</span><span class="p">)</span>
    <span class="c1"># Get full validation cost</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_validation</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
    <span class="n">validation_cost</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">get_cost</span><span class="p">(</span>
        <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">T_validation</span><span class="p">)</span>
    <span class="n">val_costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">validation_cost</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_costs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Stop training if the cost on the validation set doesn't </span>
        <span class="c1"># decrease for 3 iterations</span>
        <span class="k">if</span> <span class="n">val_costs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">val_costs</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">val_costs</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]:</span>
            <span class="k">break</span>
    
<span class="c1"># The number of iterations that have been executed</span>
<span class="n">nb_of_iterations</span> <span class="o">=</span> <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span>
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <p>
    The costs stored during training can be plotted to visualize the performance during training. The resulting plot is shown in the next figure. The cost on the training samples and validation samples goes down very quickly and flattens out after about 40 iterations on the full training set. Notice that the cost on the training set is lower than the cost on the validation set, this is because the network is optimized on the training set and is slightly overfitting. The training stops after around 90 iterations because the validation cost stops decreasing.
    <br/>
    Also, notice that the cost of the mini-batches fluctuates around the cost of the full training set. This is the stochastic effect of mini-batches in SGD.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [16]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Plot the minibatch, full training set, and validation costs</span>
<span class="n">batch_x_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
    <span class="mi">0</span><span class="p">,</span> <span class="n">nb_of_iterations</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">nb_of_iterations</span><span class="o">*</span><span class="n">nb_of_batches</span><span class="p">)</span>
<span class="n">iteration_x_inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
    <span class="mi">1</span><span class="p">,</span> <span class="n">nb_of_iterations</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">nb_of_iterations</span><span class="p">)</span>
<span class="c1"># Plot the cost over the iterations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">batch_x_inds</span><span class="p">,</span> <span class="n">batch_costs</span><span class="p">,</span> 
         <span class="s1">'k-'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'cost minibatches'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iteration_x_inds</span><span class="p">,</span> <span class="n">train_costs</span><span class="p">,</span> 
         <span class="s1">'r-'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'cost full training set'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">iteration_x_inds</span><span class="p">,</span> <span class="n">val_costs</span><span class="p">,</span> 
         <span class="s1">'b-'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'cost validation set'</span><span class="p">)</span>
<span class="c1"># Add labels to the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'$</span><span class="se">\\</span><span class="s1">xi$'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Decrease of cost over backprop iteration'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># x1, x2, y1, y2 = plt.axis()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">nb_of_iterations</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_svg output_subarea">
     <?xml version="1.0" encoding="utf-8" standalone="no"?>
     <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
     <svg height="277.314375pt" version="1.1" viewbox="0 0 387.743125 277.314375" width="387.743125pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <metadata>
       <rdf:rdf xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
        <cc:work>
         <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage">
         </dc:type>
         <dc:date>
          2021-05-13T19:57:18.867975
         </dc:date>
         <dc:format>
          image/svg+xml
         </dc:format>
         <dc:creator>
          <cc:agent>
           <dc:title>
            Matplotlib v3.4.2, https://matplotlib.org/
           </dc:title>
          </cc:agent>
         </dc:creator>
        </cc:work>
       </rdf:rdf>
      </metadata>
      <defs>
       <style type="text/css">
        *{stroke-linecap:butt;stroke-linejoin:round;}
       </style>
      </defs>
      <g id="figure_1">
       <g id="patch_1">
        <path d="M 0 277.314375 
L 387.743125 277.314375 
L 387.743125 0 
L 0 0 
z
" style="fill:#ffffff;">
        </path>
       </g>
       <g id="axes_1">
        <g id="patch_2">
         <path d="M 45.743125 239.758125 
L 380.543125 239.758125 
L 380.543125 22.318125 
L 45.743125 22.318125 
z
" style="fill:#eaeaf2;">
         </path>
        </g>
        <g id="matplotlib.axis_1">
         <g id="xtick_1">
          <g id="line2d_1">
           <path clip-path="url(#p31b5e10f6c)" d="M 45.743125 239.758125 
L 45.743125 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_1">
           <!-- 0 -->
           <g style="fill:#262626;" transform="translate(42.561875 254.356562)scale(0.1 -0.1)">
            <defs>
             <path d="M 2034 4250 
Q 1547 4250 1301 3770 
Q 1056 3291 1056 2328 
Q 1056 1369 1301 889 
Q 1547 409 2034 409 
Q 2525 409 2770 889 
Q 3016 1369 3016 2328 
Q 3016 3291 2770 3770 
Q 2525 4250 2034 4250 
z
M 2034 4750 
Q 2819 4750 3233 4129 
Q 3647 3509 3647 2328 
Q 3647 1150 3233 529 
Q 2819 -91 2034 -91 
Q 1250 -91 836 529 
Q 422 1150 422 2328 
Q 422 3509 836 4129 
Q 1250 4750 2034 4750 
z
" id="DejaVuSans-30" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_2">
          <g id="line2d_2">
           <path clip-path="url(#p31b5e10f6c)" d="M 114.069656 239.758125 
L 114.069656 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_2">
           <!-- 20 -->
           <g style="fill:#262626;" transform="translate(107.707156 254.356562)scale(0.1 -0.1)">
            <defs>
             <path d="M 1228 531 
L 3431 531 
L 3431 0 
L 469 0 
L 469 531 
Q 828 903 1448 1529 
Q 2069 2156 2228 2338 
Q 2531 2678 2651 2914 
Q 2772 3150 2772 3378 
Q 2772 3750 2511 3984 
Q 2250 4219 1831 4219 
Q 1534 4219 1204 4116 
Q 875 4013 500 3803 
L 500 4441 
Q 881 4594 1212 4672 
Q 1544 4750 1819 4750 
Q 2544 4750 2975 4387 
Q 3406 4025 3406 3419 
Q 3406 3131 3298 2873 
Q 3191 2616 2906 2266 
Q 2828 2175 2409 1742 
Q 1991 1309 1228 531 
z
" id="DejaVuSans-32" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-32">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_3">
          <g id="line2d_3">
           <path clip-path="url(#p31b5e10f6c)" d="M 182.396186 239.758125 
L 182.396186 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_3">
           <!-- 40 -->
           <g style="fill:#262626;" transform="translate(176.033686 254.356562)scale(0.1 -0.1)">
            <defs>
             <path d="M 2419 4116 
L 825 1625 
L 2419 1625 
L 2419 4116 
z
M 2253 4666 
L 3047 4666 
L 3047 1625 
L 3713 1625 
L 3713 1100 
L 3047 1100 
L 3047 0 
L 2419 0 
L 2419 1100 
L 313 1100 
L 313 1709 
L 2253 4666 
z
" id="DejaVuSans-34" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-34">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_4">
          <g id="line2d_4">
           <path clip-path="url(#p31b5e10f6c)" d="M 250.722717 239.758125 
L 250.722717 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_4">
           <!-- 60 -->
           <g style="fill:#262626;" transform="translate(244.360217 254.356562)scale(0.1 -0.1)">
            <defs>
             <path d="M 2113 2584 
Q 1688 2584 1439 2293 
Q 1191 2003 1191 1497 
Q 1191 994 1439 701 
Q 1688 409 2113 409 
Q 2538 409 2786 701 
Q 3034 994 3034 1497 
Q 3034 2003 2786 2293 
Q 2538 2584 2113 2584 
z
M 3366 4563 
L 3366 3988 
Q 3128 4100 2886 4159 
Q 2644 4219 2406 4219 
Q 1781 4219 1451 3797 
Q 1122 3375 1075 2522 
Q 1259 2794 1537 2939 
Q 1816 3084 2150 3084 
Q 2853 3084 3261 2657 
Q 3669 2231 3669 1497 
Q 3669 778 3244 343 
Q 2819 -91 2113 -91 
Q 1303 -91 875 529 
Q 447 1150 447 2328 
Q 447 3434 972 4092 
Q 1497 4750 2381 4750 
Q 2619 4750 2861 4703 
Q 3103 4656 3366 4563 
z
" id="DejaVuSans-36" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-36">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_5">
          <g id="line2d_5">
           <path clip-path="url(#p31b5e10f6c)" d="M 319.049247 239.758125 
L 319.049247 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_5">
           <!-- 80 -->
           <g style="fill:#262626;" transform="translate(312.686747 254.356562)scale(0.1 -0.1)">
            <defs>
             <path d="M 2034 2216 
Q 1584 2216 1326 1975 
Q 1069 1734 1069 1313 
Q 1069 891 1326 650 
Q 1584 409 2034 409 
Q 2484 409 2743 651 
Q 3003 894 3003 1313 
Q 3003 1734 2745 1975 
Q 2488 2216 2034 2216 
z
M 1403 2484 
Q 997 2584 770 2862 
Q 544 3141 544 3541 
Q 544 4100 942 4425 
Q 1341 4750 2034 4750 
Q 2731 4750 3128 4425 
Q 3525 4100 3525 3541 
Q 3525 3141 3298 2862 
Q 3072 2584 2669 2484 
Q 3125 2378 3379 2068 
Q 3634 1759 3634 1313 
Q 3634 634 3220 271 
Q 2806 -91 2034 -91 
Q 1263 -91 848 271 
Q 434 634 434 1313 
Q 434 1759 690 2068 
Q 947 2378 1403 2484 
z
M 1172 3481 
Q 1172 3119 1398 2916 
Q 1625 2713 2034 2713 
Q 2441 2713 2670 2916 
Q 2900 3119 2900 3481 
Q 2900 3844 2670 4047 
Q 2441 4250 2034 4250 
Q 1625 4250 1398 4047 
Q 1172 3844 1172 3481 
z
" id="DejaVuSans-38" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-38">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="text_6">
          <!-- iteration -->
          <g style="fill:#262626;" transform="translate(192.020469 268.034687)scale(0.1 -0.1)">
           <defs>
            <path d="M 603 3500 
L 1178 3500 
L 1178 0 
L 603 0 
L 603 3500 
z
M 603 4863 
L 1178 4863 
L 1178 4134 
L 603 4134 
L 603 4863 
z
" id="DejaVuSans-69" transform="scale(0.015625)">
            </path>
            <path d="M 1172 4494 
L 1172 3500 
L 2356 3500 
L 2356 3053 
L 1172 3053 
L 1172 1153 
Q 1172 725 1289 603 
Q 1406 481 1766 481 
L 2356 481 
L 2356 0 
L 1766 0 
Q 1100 0 847 248 
Q 594 497 594 1153 
L 594 3053 
L 172 3053 
L 172 3500 
L 594 3500 
L 594 4494 
L 1172 4494 
z
" id="DejaVuSans-74" transform="scale(0.015625)">
            </path>
            <path d="M 3597 1894 
L 3597 1613 
L 953 1613 
Q 991 1019 1311 708 
Q 1631 397 2203 397 
Q 2534 397 2845 478 
Q 3156 559 3463 722 
L 3463 178 
Q 3153 47 2828 -22 
Q 2503 -91 2169 -91 
Q 1331 -91 842 396 
Q 353 884 353 1716 
Q 353 2575 817 3079 
Q 1281 3584 2069 3584 
Q 2775 3584 3186 3129 
Q 3597 2675 3597 1894 
z
M 3022 2063 
Q 3016 2534 2758 2815 
Q 2500 3097 2075 3097 
Q 1594 3097 1305 2825 
Q 1016 2553 972 2059 
L 3022 2063 
z
" id="DejaVuSans-65" transform="scale(0.015625)">
            </path>
            <path d="M 2631 2963 
Q 2534 3019 2420 3045 
Q 2306 3072 2169 3072 
Q 1681 3072 1420 2755 
Q 1159 2438 1159 1844 
L 1159 0 
L 581 0 
L 581 3500 
L 1159 3500 
L 1159 2956 
Q 1341 3275 1631 3429 
Q 1922 3584 2338 3584 
Q 2397 3584 2469 3576 
Q 2541 3569 2628 3553 
L 2631 2963 
z
" id="DejaVuSans-72" transform="scale(0.015625)">
            </path>
            <path d="M 2194 1759 
Q 1497 1759 1228 1600 
Q 959 1441 959 1056 
Q 959 750 1161 570 
Q 1363 391 1709 391 
Q 2188 391 2477 730 
Q 2766 1069 2766 1631 
L 2766 1759 
L 2194 1759 
z
M 3341 1997 
L 3341 0 
L 2766 0 
L 2766 531 
Q 2569 213 2275 61 
Q 1981 -91 1556 -91 
Q 1019 -91 701 211 
Q 384 513 384 1019 
Q 384 1609 779 1909 
Q 1175 2209 1959 2209 
L 2766 2209 
L 2766 2266 
Q 2766 2663 2505 2880 
Q 2244 3097 1772 3097 
Q 1472 3097 1187 3025 
Q 903 2953 641 2809 
L 641 3341 
Q 956 3463 1253 3523 
Q 1550 3584 1831 3584 
Q 2591 3584 2966 3190 
Q 3341 2797 3341 1997 
z
" id="DejaVuSans-61" transform="scale(0.015625)">
            </path>
            <path d="M 1959 3097 
Q 1497 3097 1228 2736 
Q 959 2375 959 1747 
Q 959 1119 1226 758 
Q 1494 397 1959 397 
Q 2419 397 2687 759 
Q 2956 1122 2956 1747 
Q 2956 2369 2687 2733 
Q 2419 3097 1959 3097 
z
M 1959 3584 
Q 2709 3584 3137 3096 
Q 3566 2609 3566 1747 
Q 3566 888 3137 398 
Q 2709 -91 1959 -91 
Q 1206 -91 779 398 
Q 353 888 353 1747 
Q 353 2609 779 3096 
Q 1206 3584 1959 3584 
z
" id="DejaVuSans-6f" transform="scale(0.015625)">
            </path>
            <path d="M 3513 2113 
L 3513 0 
L 2938 0 
L 2938 2094 
Q 2938 2591 2744 2837 
Q 2550 3084 2163 3084 
Q 1697 3084 1428 2787 
Q 1159 2491 1159 1978 
L 1159 0 
L 581 0 
L 581 3500 
L 1159 3500 
L 1159 2956 
Q 1366 3272 1645 3428 
Q 1925 3584 2291 3584 
Q 2894 3584 3203 3211 
Q 3513 2838 3513 2113 
z
" id="DejaVuSans-6e" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-69">
           </use>
           <use x="27.783203" xlink:href="#DejaVuSans-74">
           </use>
           <use x="66.992188" xlink:href="#DejaVuSans-65">
           </use>
           <use x="128.515625" xlink:href="#DejaVuSans-72">
           </use>
           <use x="169.628906" xlink:href="#DejaVuSans-61">
           </use>
           <use x="230.908203" xlink:href="#DejaVuSans-74">
           </use>
           <use x="270.117188" xlink:href="#DejaVuSans-69">
           </use>
           <use x="297.900391" xlink:href="#DejaVuSans-6f">
           </use>
           <use x="359.082031" xlink:href="#DejaVuSans-6e">
           </use>
          </g>
         </g>
        </g>
        <g id="matplotlib.axis_2">
         <g id="ytick_1">
          <g id="line2d_6">
           <path clip-path="url(#p31b5e10f6c)" d="M 45.743125 239.758125 
L 380.543125 239.758125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_7">
           <!-- 0.0 -->
           <g style="fill:#262626;" transform="translate(22.84 243.557344)scale(0.1 -0.1)">
            <defs>
             <path d="M 684 794 
L 1344 794 
L 1344 0 
L 684 0 
L 684 794 
z
" id="DejaVuSans-2e" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-30">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_2">
          <g id="line2d_7">
           <path clip-path="url(#p31b5e10f6c)" d="M 45.743125 196.270125 
L 380.543125 196.270125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_8">
           <!-- 0.5 -->
           <g style="fill:#262626;" transform="translate(22.84 200.069344)scale(0.1 -0.1)">
            <defs>
             <path d="M 691 4666 
L 3169 4666 
L 3169 4134 
L 1269 4134 
L 1269 2991 
Q 1406 3038 1543 3061 
Q 1681 3084 1819 3084 
Q 2600 3084 3056 2656 
Q 3513 2228 3513 1497 
Q 3513 744 3044 326 
Q 2575 -91 1722 -91 
Q 1428 -91 1123 -41 
Q 819 9 494 109 
L 494 744 
Q 775 591 1075 516 
Q 1375 441 1709 441 
Q 2250 441 2565 725 
Q 2881 1009 2881 1497 
Q 2881 1984 2565 2268 
Q 2250 2553 1709 2553 
Q 1456 2553 1204 2497 
Q 953 2441 691 2322 
L 691 4666 
z
" id="DejaVuSans-35" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-30">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-35">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_3">
          <g id="line2d_8">
           <path clip-path="url(#p31b5e10f6c)" d="M 45.743125 152.782125 
L 380.543125 152.782125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_9">
           <!-- 1.0 -->
           <g style="fill:#262626;" transform="translate(22.84 156.581344)scale(0.1 -0.1)">
            <defs>
             <path d="M 794 531 
L 1825 531 
L 1825 4091 
L 703 3866 
L 703 4441 
L 1819 4666 
L 2450 4666 
L 2450 531 
L 3481 531 
L 3481 0 
L 794 0 
L 794 531 
z
" id="DejaVuSans-31" transform="scale(0.015625)">
             </path>
            </defs>
            <use xlink:href="#DejaVuSans-31">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_4">
          <g id="line2d_9">
           <path clip-path="url(#p31b5e10f6c)" d="M 45.743125 109.294125 
L 380.543125 109.294125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_10">
           <!-- 1.5 -->
           <g style="fill:#262626;" transform="translate(22.84 113.093344)scale(0.1 -0.1)">
            <use xlink:href="#DejaVuSans-31">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-35">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_5">
          <g id="line2d_10">
           <path clip-path="url(#p31b5e10f6c)" d="M 45.743125 65.806125 
L 380.543125 65.806125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_11">
           <!-- 2.0 -->
           <g style="fill:#262626;" transform="translate(22.84 69.605344)scale(0.1 -0.1)">
            <use xlink:href="#DejaVuSans-32">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_6">
          <g id="line2d_11">
           <path clip-path="url(#p31b5e10f6c)" d="M 45.743125 22.318125 
L 380.543125 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="text_12">
           <!-- 2.5 -->
           <g style="fill:#262626;" transform="translate(22.84 26.117344)scale(0.1 -0.1)">
            <use xlink:href="#DejaVuSans-32">
            </use>
            <use x="63.623047" xlink:href="#DejaVuSans-2e">
            </use>
            <use x="95.410156" xlink:href="#DejaVuSans-35">
            </use>
           </g>
          </g>
         </g>
         <g id="text_13">
          <!-- $\xi$ -->
          <g style="fill:#262626;" transform="translate(16.32 134.398125)rotate(-90)scale(0.12 -0.12)">
           <defs>
            <path d="M 2016 397 
Q 2428 394 2628 159 
Q 2844 -88 2772 -463 
Q 2700 -822 2422 -1072 
Q 2119 -1344 1609 -1344 
Q 1656 -1109 1700 -872 
Q 1913 -888 2072 -750 
Q 2194 -641 2216 -525 
Q 2247 -359 2175 -222 
Q 2100 -91 1922 -91 
Q -25 -91 241 1275 
Q 422 2213 1516 2488 
Q 663 2600 822 3413 
Q 941 4028 1678 4284 
L 1028 4284 
L 1141 4863 
L 3606 4863 
L 3494 4284 
Q 1528 4284 1350 3375 
Q 1234 2778 2881 2750 
L 2778 2219 
Q 1009 2288 819 1275 
Q 659 428 2016 397 
z
" id="DejaVuSans-Oblique-3be" transform="scale(0.015625)">
            </path>
           </defs>
           <use transform="translate(0 0.015625)" xlink:href="#DejaVuSans-Oblique-3be">
           </use>
          </g>
         </g>
        </g>
        <g id="line2d_12">
         <path clip-path="url(#p31b5e10f6c)" d="M 45.743125 39.250366 
L 45.902062 40.910783 
L 45.98153 37.167099 
L 46.060998 42.106869 
L 46.140467 39.495726 
L 46.219935 41.758299 
L 46.299403 40.733347 
L 46.378871 37.814491 
L 46.45834 38.254647 
L 46.617276 40.159812 
L 46.696745 40.49781 
L 46.776213 41.793845 
L 46.93515 36.610024 
L 47.014618 40.105547 
L 47.094086 38.056728 
L 47.173555 38.408649 
L 47.253023 38.692915 
L 47.332491 42.029269 
L 47.41196 40.892279 
L 47.491428 40.974667 
L 47.570896 42.796177 
L 47.650364 40.762741 
L 47.729833 41.936089 
L 47.809301 36.013398 
L 47.968238 40.527409 
L 48.047706 36.752991 
L 48.286111 42.373471 
L 48.365579 43.056225 
L 48.445048 42.585757 
L 48.524516 39.292146 
L 48.603984 42.112287 
L 48.683453 37.386263 
L 48.762921 41.211988 
L 48.842389 40.499196 
L 48.921857 38.067165 
L 49.001326 39.00395 
L 49.080794 39.331884 
L 49.319199 41.327406 
L 49.398667 40.575957 
L 49.478136 42.135188 
L 49.557604 39.360294 
L 49.637072 42.553138 
L 49.716541 42.368636 
L 49.796009 39.625883 
L 49.875477 41.000775 
L 49.954946 40.322473 
L 50.034414 42.340372 
L 50.113882 41.025804 
L 50.19335 42.742761 
L 50.352287 39.021726 
L 50.431755 42.198733 
L 50.511224 39.560955 
L 50.749629 43.352835 
L 50.908565 42.035262 
L 50.988034 43.781061 
L 51.067502 42.391033 
L 51.14697 42.998503 
L 51.226439 37.483172 
L 51.385375 42.2636 
L 51.464843 38.534412 
L 51.703248 44.649662 
L 51.782717 44.94751 
L 51.862185 44.370902 
L 51.941653 40.928984 
L 52.021122 43.853698 
L 52.10059 38.880609 
L 52.180058 43.096262 
L 52.259527 41.937774 
L 52.338995 39.315938 
L 52.418463 40.924956 
L 52.497932 40.873435 
L 52.656868 42.895469 
L 52.736336 42.65813 
L 52.815805 41.981651 
L 52.895273 43.473535 
L 52.974741 41.22431 
L 53.133678 44.136783 
L 53.213146 41.797705 
L 53.292615 42.475043 
L 53.372083 41.825357 
L 53.451551 44.295408 
L 53.53102 42.601197 
L 53.610488 44.682309 
L 53.769425 40.868361 
L 53.848893 43.703839 
L 53.928361 41.369295 
L 54.166766 45.274086 
L 54.325703 43.745004 
L 54.405171 45.770834 
L 54.643576 39.682823 
L 54.802513 44.504061 
L 54.881981 40.306605 
L 55.120386 46.91394 
L 55.199854 47.178758 
L 55.279322 46.217381 
L 55.358791 43.278364 
L 55.438259 46.191327 
L 55.517727 40.838015 
L 55.597196 45.490312 
L 55.676664 44.200156 
L 55.756132 41.247672 
L 55.835601 43.85611 
L 55.915069 43.192443 
L 55.994537 43.530034 
L 56.074006 45.453676 
L 56.153474 44.691013 
L 56.232942 43.879752 
L 56.312411 45.446902 
L 56.391879 44.005652 
L 56.550815 46.635533 
L 56.78922 44.176758 
L 56.868689 47.030009 
L 56.948157 45.070126 
L 57.027625 47.599673 
L 57.186562 43.677981 
L 57.26603 45.791756 
L 57.345499 44.066605 
L 57.583904 48.088848 
L 57.663372 47.823109 
L 57.74284 46.313426 
L 57.822308 48.860859 
L 58.060713 42.95054 
L 58.21965 47.781139 
L 58.299118 42.924808 
L 58.537523 50.281802 
L 58.616992 50.609336 
L 58.775928 46.845498 
L 58.855397 49.757781 
L 58.934865 43.827497 
L 59.014333 49.186743 
L 59.093801 47.557001 
L 59.17327 44.158318 
L 59.252738 48.485416 
L 59.411675 46.483622 
L 59.491143 49.472514 
L 59.65008 46.785151 
L 59.888485 49.850045 
L 59.967953 50.500573 
L 60.047421 50.406975 
L 60.12689 47.821641 
L 60.206358 47.833982 
L 60.285826 51.181588 
L 60.365294 48.942722 
L 60.444763 52.127118 
L 60.524231 47.992022 
L 60.603699 48.139659 
L 60.683168 48.936903 
L 60.762636 48.063922 
L 61.001041 52.382127 
L 61.080509 52.465772 
L 61.159978 50.293905 
L 61.239446 53.57052 
L 61.477851 47.916545 
L 61.636787 52.904009 
L 61.716256 47.023295 
L 61.954661 55.451294 
L 62.034129 55.991838 
L 62.193066 52.220976 
L 62.272534 55.184401 
L 62.352002 48.489024 
L 62.431471 54.947022 
L 62.510939 52.53447 
L 62.590407 48.678029 
L 62.669876 55.681058 
L 62.828812 51.01529 
L 62.90828 55.728071 
L 63.067217 51.154849 
L 63.464559 58.413558 
L 63.544027 52.698266 
L 63.623495 53.40191 
L 63.702964 57.254875 
L 63.782432 54.954002 
L 63.8619 58.958547 
L 63.941369 53.693362 
L 64.020837 54.803505 
L 64.100305 53.677218 
L 64.179773 53.824302 
L 64.33871 58.342633 
L 64.497647 59.453556 
L 64.577115 56.329829 
L 64.656583 60.431604 
L 64.894988 54.950632 
L 65.053925 60.698172 
L 65.133393 53.261981 
L 65.371798 63.000544 
L 65.451266 63.661489 
L 65.530735 59.450086 
L 65.610203 59.832969 
L 65.689671 62.842114 
L 65.76914 55.137181 
L 65.848608 63.014802 
L 66.007545 55.172346 
L 66.087013 65.649096 
L 66.24595 57.37534 
L 66.325418 64.326625 
L 66.484355 57.516884 
L 66.881696 69.122009 
L 66.961164 59.522271 
L 67.040633 60.975604 
L 67.120101 65.360215 
L 67.199569 63.38225 
L 67.279038 68.448499 
L 67.358506 61.769227 
L 67.437974 63.615959 
L 67.517443 60.376125 
L 67.596911 61.461838 
L 67.755848 67.727271 
L 67.835316 66.857868 
L 67.914784 68.907403 
L 67.994252 64.381442 
L 68.073721 69.216617 
L 68.312126 63.948614 
L 68.391594 65.24274 
L 68.471062 70.962523 
L 68.550531 61.758836 
L 68.788936 72.963092 
L 68.868404 73.402337 
L 68.947872 67.595909 
L 69.027341 69.520046 
L 69.106809 72.510006 
L 69.186277 63.562735 
L 69.265745 72.94485 
L 69.424682 63.659268 
L 69.50415 77.505869 
L 69.663087 65.370615 
L 69.742555 74.978306 
L 69.901492 65.836638 
L 69.98096 67.225107 
L 70.060429 74.315539 
L 70.139897 71.973718 
L 70.219365 74.658007 
L 70.298834 81.459671 
L 70.378302 68.075607 
L 70.45777 70.361302 
L 70.696175 79.893992 
L 70.775643 71.899285 
L 70.855112 73.728606 
L 70.93458 68.949683 
L 71.014048 70.570135 
L 71.172985 78.699468 
L 71.252453 76.392703 
L 71.331922 80.233605 
L 71.41139 73.937202 
L 71.490858 79.274584 
L 71.570327 77.575891 
L 71.729263 74.091912 
L 71.808731 75.678677 
L 71.8882 82.843065 
L 71.967668 72.23582 
L 72.206073 84.629323 
L 72.285541 84.512841 
L 72.36501 77.134477 
L 72.523946 83.43361 
L 72.603415 72.947664 
L 72.682883 83.974497 
L 72.84182 73.590267 
L 72.921288 89.837389 
L 73.080224 74.561073 
L 73.159693 86.724506 
L 73.318629 75.647844 
L 73.398098 76.035446 
L 73.477566 86.735096 
L 73.557034 83.306867 
L 73.636503 85.868755 
L 73.715971 94.279146 
L 73.795439 77.738381 
L 73.874908 81.01597 
L 74.113313 92.143777 
L 74.192781 83.277601 
L 74.272249 84.33075 
L 74.351717 79.01325 
L 74.431186 80.69707 
L 74.590122 90.394601 
L 74.669591 86.946288 
L 74.749059 92.562355 
L 74.828527 84.448044 
L 74.907996 90.046694 
L 74.987464 89.162301 
L 75.146401 84.643267 
L 75.225869 87.810134 
L 75.305337 95.46448 
L 75.384806 84.206065 
L 75.62321 97.146936 
L 75.702679 96.237289 
L 75.782147 87.615462 
L 75.941084 95.044056 
L 76.020552 82.694583 
L 76.10002 95.609656 
L 76.258957 84.433082 
L 76.338425 102.062121 
L 76.497362 84.477913 
L 76.57683 98.831477 
L 76.735767 86.39168 
L 76.815235 85.714612 
L 76.894703 100.041015 
L 76.974172 95.869907 
L 77.05364 97.750059 
L 77.133108 107.18884 
L 77.212577 88.194251 
L 77.292045 92.481056 
L 77.371513 98.101542 
L 77.450982 95.30829 
L 77.53045 104.506946 
L 77.609918 95.056052 
L 77.689387 95.134739 
L 77.768855 90.260544 
L 77.848323 91.68982 
L 78.00726 102.458248 
L 78.086728 98.38994 
L 78.166196 105.341795 
L 78.245665 95.675248 
L 78.325133 101.369013 
L 78.404601 101.348769 
L 78.563538 95.527107 
L 78.722475 108.405637 
L 78.801943 97.183466 
L 78.881411 101.230479 
L 79.040348 110.230085 
L 79.119816 108.196955 
L 79.199285 98.61744 
L 79.358221 107.165517 
L 79.437689 92.828714 
L 79.517158 107.729901 
L 79.676094 95.977349 
L 79.755563 114.086042 
L 79.914499 94.853057 
L 79.993968 111.06852 
L 80.152904 97.733235 
L 80.232373 96.124643 
L 80.311841 113.884757 
L 80.391309 109.179232 
L 80.470778 110.004271 
L 80.550246 119.743044 
L 80.629714 99.529731 
L 80.709182 104.738624 
L 80.788651 110.85713 
L 80.868119 106.250703 
L 80.947587 116.716554 
L 81.106524 105.993504 
L 81.185992 102.379997 
L 81.265461 103.397612 
L 81.424397 114.788293 
L 81.503866 110.405962 
L 81.583334 118.133551 
L 81.662802 107.395231 
L 81.821739 113.898944 
L 81.980675 106.699283 
L 82.139612 121.315855 
L 82.21908 110.498266 
L 82.298549 113.407262 
L 82.457485 123.531602 
L 82.536954 120.09561 
L 82.616422 109.497034 
L 82.775359 119.720176 
L 82.854827 102.991617 
L 82.934295 119.76545 
L 83.013763 107.737556 
L 83.093232 107.989932 
L 83.1727 125.777596 
L 83.331637 105.819509 
L 83.411105 123.356667 
L 83.570042 109.434919 
L 83.64951 107.070596 
L 83.728978 127.562441 
L 83.808447 122.728181 
L 83.887915 122.527779 
L 83.967383 131.618603 
L 84.046852 111.115521 
L 84.12632 117.585795 
L 84.205788 123.483656 
L 84.285256 117.438197 
L 84.364725 128.462477 
L 84.523661 116.839081 
L 84.60313 114.906625 
L 84.682598 115.467115 
L 84.841535 127.188206 
L 84.921003 122.485411 
L 85.000471 130.624874 
L 85.07994 119.573073 
L 85.238876 126.301579 
L 85.397813 117.836344 
L 85.556749 133.905231 
L 85.636218 123.57582 
L 85.715686 125.187744 
L 85.874623 136.357618 
L 85.954091 131.470179 
L 86.033559 119.712151 
L 86.192496 132.267149 
L 86.271964 112.825326 
L 86.351433 131.363332 
L 86.430901 119.345207 
L 86.510369 120.053862 
L 86.589838 137.090908 
L 86.748774 117.278076 
L 86.828242 135.354588 
L 86.907711 121.197182 
L 86.987179 121.2393 
L 87.066647 118.284613 
L 87.146116 140.239223 
L 87.225584 136.076889 
L 87.305052 134.871914 
L 87.384521 142.592137 
L 87.463989 122.417585 
L 87.543457 130.196987 
L 87.622926 135.705849 
L 87.702394 128.808015 
L 87.781862 139.831212 
L 87.940799 127.416426 
L 88.020267 127.396874 
L 88.099735 127.257801 
L 88.258672 139.15569 
L 88.33814 134.065742 
L 88.417609 142.512628 
L 88.497077 131.908911 
L 88.576545 135.396798 
L 88.656014 138.230317 
L 88.81495 128.401305 
L 88.973887 145.881607 
L 89.053355 135.869056 
L 89.132824 136.094863 
L 89.29176 148.161621 
L 89.450697 129.147563 
L 89.609633 144.287217 
L 89.689102 122.27773 
L 89.76857 142.33309 
L 89.848038 130.341564 
L 89.927507 131.734546 
L 90.006975 147.779153 
L 90.086443 128.471544 
L 90.165912 128.809584 
L 90.24538 146.611767 
L 90.324848 132.494131 
L 90.404317 132.802344 
L 90.483785 129.290357 
L 90.563253 151.655682 
L 90.642721 148.473044 
L 90.72219 146.643246 
L 90.801658 152.592203 
L 90.881126 133.082134 
L 91.040063 146.925526 
L 91.119531 139.611416 
L 91.199 150.558818 
L 91.357936 137.365641 
L 91.437405 139.359178 
L 91.516873 138.553949 
L 91.67581 150.07319 
L 91.755278 144.701288 
L 91.834746 153.433605 
L 91.914214 143.712433 
L 91.993683 146.011018 
L 92.073151 149.259934 
L 92.232088 138.083699 
L 92.391024 157.069289 
L 92.549961 145.537302 
L 92.708898 158.790549 
L 92.867834 137.730792 
L 93.026771 155.336278 
L 93.106239 130.878196 
L 93.185707 152.491462 
L 93.265176 140.454503 
L 93.344644 142.702641 
L 93.424112 157.367846 
L 93.503581 139.043646 
L 93.583049 139.174125 
L 93.662517 156.70543 
L 93.741986 143.117559 
L 93.821454 143.530775 
L 93.900922 139.567482 
L 93.980391 161.856395 
L 94.059859 159.206242 
L 94.139327 157.420418 
L 94.218796 161.418651 
L 94.298264 142.914861 
L 94.4572 156.790618 
L 94.536669 149.026424 
L 94.616137 160.455816 
L 94.775074 146.782036 
L 95.092947 159.599517 
L 95.172415 154.147787 
L 95.251884 163.2889 
L 95.331352 154.621922 
L 95.41082 155.757875 
L 95.490289 159.067625 
L 95.649225 146.786553 
L 95.808162 167.047488 
L 95.967098 153.72995 
L 96.126035 168.388568 
L 96.284972 145.673226 
L 96.443908 165.072427 
L 96.523377 138.505187 
L 96.602845 161.530409 
L 96.682313 149.593084 
L 96.761782 152.75807 
L 96.84125 165.949122 
L 97.000186 148.510673 
L 97.079655 165.541043 
L 97.159123 152.758809 
L 97.238591 153.176824 
L 97.31806 149.32249 
L 97.397528 170.732396 
L 97.476996 168.670712 
L 97.556465 166.971216 
L 97.635933 168.957189 
L 97.715401 151.463617 
L 97.874338 165.794801 
L 97.953806 157.221842 
L 98.033275 169.03976 
L 98.192211 155.695866 
L 98.271679 160.842883 
L 98.351148 158.703888 
L 98.430616 168.590586 
L 98.510084 167.727109 
L 98.589553 162.397527 
L 98.669021 172.106185 
L 98.748489 164.210901 
L 98.827958 164.60971 
L 98.907426 167.636286 
L 99.066363 154.377192 
L 99.145831 175.818232 
L 99.225299 175.409238 
L 99.384236 161.249726 
L 99.543172 176.878539 
L 99.702109 152.736034 
L 99.861046 173.559399 
L 99.940514 145.368009 
L 100.019982 169.469931 
L 100.099451 157.834068 
L 100.178919 161.413485 
L 100.258387 173.719657 
L 100.417324 157.199708 
L 100.496792 173.290936 
L 100.576261 161.116482 
L 100.655729 162.190125 
L 100.735197 158.0979 
L 100.814665 178.316021 
L 100.894134 177.162683 
L 100.973602 175.317996 
L 101.05307 175.390792 
L 101.132539 158.636306 
L 101.291475 173.922596 
L 101.370944 164.497063 
L 101.450412 176.379151 
L 101.609349 164.043204 
L 101.688817 170.286705 
L 101.768285 167.638874 
L 101.847754 176.538086 
L 101.927222 174.836431 
L 102.00669 169.691366 
L 102.086158 179.80124 
L 102.165627 172.209376 
L 102.245095 172.592907 
L 102.324563 175.116016 
L 102.4835 161.412886 
L 102.562968 183.567617 
L 102.642437 182.370249 
L 102.801373 168.598904 
L 102.96031 184.249145 
L 103.039778 177.403346 
L 103.119247 159.17245 
L 103.278183 180.969298 
L 103.357651 151.995745 
L 103.43712 176.456064 
L 103.516588 165.293613 
L 103.596056 168.805618 
L 103.675525 180.652322 
L 103.754993 164.898698 
L 103.834461 165.385091 
L 103.91393 180.174197 
L 103.993398 168.538929 
L 104.072866 170.378544 
L 104.152335 165.841505 
L 104.231803 184.795538 
L 104.311271 184.451585 
L 104.470208 181.070123 
L 104.549676 164.673229 
L 104.708613 180.995071 
L 104.788081 171.101217 
L 104.867549 182.623826 
L 105.026486 171.723514 
L 105.105954 178.572532 
L 105.185423 175.770288 
L 105.264891 183.546642 
L 105.344359 181.180502 
L 105.423828 176.008954 
L 105.503296 186.446742 
L 105.582764 179.060346 
L 105.662233 179.884036 
L 105.741701 181.804495 
L 105.900637 168.204509 
L 105.980106 189.989028 
L 106.059574 188.299075 
L 106.218511 175.414591 
L 106.377447 190.472502 
L 106.456916 184.460425 
L 106.536384 164.951416 
L 106.695321 187.386855 
L 106.774789 158.507317 
L 106.854257 182.599959 
L 106.933726 172.004411 
L 107.013194 175.254719 
L 107.092662 186.970386 
L 107.17213 171.524293 
L 107.251599 173.118254 
L 107.331067 186.423896 
L 107.410535 175.276579 
L 107.490004 177.482756 
L 107.569472 172.790373 
L 107.728409 190.711767 
L 107.887345 186.013609 
L 107.966814 169.78721 
L 108.12575 187.062686 
L 108.205219 177.002402 
L 108.284687 187.987966 
L 108.364155 178.20661 
L 108.443623 178.521081 
L 108.523092 185.902154 
L 108.60256 182.97747 
L 108.682028 189.821895 
L 108.840965 181.554055 
L 108.920433 192.1637 
L 108.999902 185.158492 
L 109.07937 186.480928 
L 109.158838 187.714618 
L 109.317775 174.998976 
L 109.397243 195.174491 
L 109.476712 193.432143 
L 109.635648 181.463205 
L 109.794585 195.721398 
L 109.874053 190.414651 
L 109.953521 169.928991 
L 110.112458 192.838711 
L 110.191926 164.534162 
L 110.271395 188.122737 
L 110.350863 178.120818 
L 110.430331 180.887844 
L 110.5098 192.896912 
L 110.589268 177.762325 
L 110.668736 180.020124 
L 110.748205 191.912851 
L 110.827673 181.382465 
L 110.907141 183.669945 
L 110.986609 178.999482 
L 111.145546 196.069954 
L 111.225014 194.448599 
L 111.304483 190.442177 
L 111.383951 173.990737 
L 111.542888 192.630901 
L 111.622356 182.496189 
L 111.701824 192.604206 
L 111.781293 182.645668 
L 111.860761 184.89126 
L 111.940229 192.159228 
L 112.019698 189.139425 
L 112.099166 195.347917 
L 112.258102 186.563623 
L 112.337571 197.109111 
L 112.417039 190.645218 
L 112.496507 192.253487 
L 112.575976 192.963992 
L 112.734912 182.078052 
L 112.814381 199.617968 
L 112.893849 197.751026 
L 113.052786 186.774499 
L 113.211722 200.021245 
L 113.291191 195.514409 
L 113.370659 174.651606 
L 113.529595 197.47719 
L 113.609064 170.1618 
L 113.688532 193.264331 
L 113.768 183.612053 
L 113.847469 185.882306 
L 113.926937 198.287766 
L 114.006405 183.827848 
L 114.085874 186.042506 
L 114.165342 196.867046 
L 114.24481 186.826417 
L 114.324279 188.967621 
L 114.403747 184.3497 
L 114.562684 200.724063 
L 114.642152 199.367592 
L 114.72162 194.508139 
L 114.801088 178.125867 
L 114.960025 197.971937 
L 115.039493 187.410271 
L 115.118962 196.77666 
L 115.19843 187.396119 
L 115.277898 190.079876 
L 115.357367 197.441069 
L 115.436835 194.710007 
L 115.516303 200.119976 
L 115.67524 191.593209 
L 115.754708 201.311267 
L 115.834177 195.486577 
L 115.913645 197.201605 
L 115.993113 197.704925 
L 116.15205 188.06142 
L 116.231518 203.819601 
L 116.310986 201.403696 
L 116.469923 191.705353 
L 116.549391 204.152122 
L 116.62886 203.451967 
L 116.708328 200.043748 
L 116.787796 179.505438 
L 116.867265 201.539435 
L 116.946733 201.506682 
L 117.026201 175.531016 
L 117.10567 198.001285 
L 117.185138 188.206079 
L 117.264606 190.309441 
L 117.344074 203.05209 
L 117.423543 189.292514 
L 117.503011 191.568411 
L 117.582479 201.493188 
L 117.661948 191.028748 
L 117.741416 193.225678 
L 117.820884 188.963838 
L 117.979821 204.618881 
L 118.059289 203.733521 
L 118.138758 198.095415 
L 118.218226 182.614487 
L 118.377163 202.604974 
L 118.456631 191.894367 
L 118.536099 200.585806 
L 118.615567 191.794424 
L 118.695036 194.41975 
L 118.774504 202.130627 
L 118.853972 199.331195 
L 118.933441 204.177803 
L 119.092377 196.205686 
L 119.171846 205.240459 
L 119.251314 199.791244 
L 119.330782 201.524359 
L 119.410251 201.928273 
L 119.569187 192.756721 
L 119.648656 207.318758 
L 119.728124 204.758056 
L 119.88706 196.252413 
L 119.966529 207.898384 
L 120.045997 206.517171 
L 120.125465 203.823486 
L 120.204934 183.86596 
L 120.284402 205.651824 
L 120.36387 205.03266 
L 120.443339 180.427288 
L 120.522807 202.193371 
L 120.602275 191.996357 
L 120.681744 194.18038 
L 120.761212 207.200752 
L 120.84068 193.833221 
L 120.920148 196.240198 
L 120.999617 205.504753 
L 121.079085 194.727938 
L 121.158553 196.799947 
L 121.238022 192.835506 
L 121.396958 207.837237 
L 121.476427 207.495767 
L 121.635363 187.411796 
L 121.7943 206.273357 
L 121.873768 196.164795 
L 121.953237 204.018177 
L 122.032705 195.31628 
L 122.112173 198.549378 
L 122.191641 206.150432 
L 122.27111 203.134572 
L 122.350578 207.654867 
L 122.509515 200.04324 
L 122.588983 208.697226 
L 122.668451 203.383739 
L 122.74792 205.337269 
L 122.827388 205.673372 
L 122.986325 196.753478 
L 123.065793 210.254527 
L 123.145261 207.832915 
L 123.304198 200.115097 
L 123.383666 211.164731 
L 123.463134 209.526386 
L 123.542603 206.970988 
L 123.622071 187.858395 
L 123.701539 209.345085 
L 123.781008 208.046496 
L 123.860476 184.585656 
L 123.939944 205.86681 
L 124.019413 195.39148 
L 124.098881 197.65525 
L 124.178349 210.711002 
L 124.257818 197.519843 
L 124.337286 200.140708 
L 124.416754 208.921138 
L 124.496223 197.959108 
L 124.575691 199.914045 
L 124.655159 196.000412 
L 124.814096 210.484952 
L 124.893564 210.665701 
L 125.052501 192.313321 
L 125.211437 209.268758 
L 125.290906 199.842753 
L 125.370374 206.862296 
L 125.449842 197.804777 
L 125.767716 210.649176 
L 125.926652 203.024549 
L 126.00612 211.430898 
L 126.085589 206.278218 
L 126.244525 208.968015 
L 126.403462 200.274698 
L 126.48293 212.827573 
L 126.562399 210.573509 
L 126.721335 203.363946 
L 126.800804 213.915444 
L 126.880272 212.35432 
L 126.95974 209.696703 
L 127.039209 191.399757 
L 127.118677 212.6673 
L 127.198145 210.643459 
L 127.277613 188.369943 
L 127.357082 208.779116 
L 127.43655 198.681309 
L 127.516018 200.994647 
L 127.595487 213.673042 
L 127.674955 200.850416 
L 127.754423 203.712167 
L 127.833892 211.861256 
L 127.91336 200.964474 
L 127.992828 202.58235 
L 128.072297 198.925626 
L 128.231233 212.795371 
L 128.310702 213.167369 
L 128.469638 197.183737 
L 128.628575 211.703547 
L 128.708043 202.960586 
L 128.787511 209.093797 
L 128.86698 200.16288 
L 129.025916 212.589125 
L 129.105385 209.190998 
L 129.184853 213.312846 
L 129.34379 205.474356 
L 129.423258 213.561277 
L 129.502726 208.783385 
L 129.661663 211.917833 
L 129.820599 203.221742 
L 129.900068 215.175011 
L 129.979536 213.008216 
L 130.138473 206.274839 
L 130.217941 216.172815 
L 130.297409 214.894538 
L 130.376878 212.147244 
L 130.456346 194.668873 
L 130.535814 215.42642 
L 130.615283 212.961821 
L 130.694751 191.939995 
L 130.774219 210.987873 
L 130.853688 201.890102 
L 130.933156 204.179645 
L 131.012624 216.139068 
L 131.092092 203.950098 
L 131.171561 207.352576 
L 131.251029 214.48247 
L 131.330497 204.161064 
L 131.409966 204.840203 
L 131.489434 201.779129 
L 131.648371 215.028407 
L 131.727839 215.136022 
L 131.886776 201.811341 
L 132.045712 213.844028 
L 132.125181 205.924976 
L 132.204649 211.122288 
L 132.284117 202.421854 
L 132.443054 215.352179 
L 132.522522 211.960707 
L 132.60199 215.558164 
L 132.760927 207.711872 
L 132.840395 215.172931 
L 132.919864 210.953787 
L 133.0788 214.529216 
L 133.237737 205.55108 
L 133.317205 217.218125 
L 133.396674 215.13679 
L 133.55561 208.719691 
L 133.635078 218.187191 
L 133.714547 216.857749 
L 133.794015 214.409243 
L 133.873483 197.630481 
L 133.952952 217.6611 
L 134.03242 215.074764 
L 134.111888 195.32479 
L 134.191357 212.757004 
L 134.270825 204.651817 
L 134.350293 207.274856 
L 134.429762 218.277531 
L 134.50923 206.363645 
L 134.588698 210.387612 
L 134.668167 216.731138 
L 134.827103 207.157534 
L 134.906571 204.651904 
L 134.98604 217.061392 
L 135.065508 217.008414 
L 135.144976 216.885942 
L 135.303913 205.814115 
L 135.383381 216.543197 
L 135.46285 216.049567 
L 135.542318 208.449732 
L 135.621786 214.001133 
L 135.701255 204.566893 
L 135.860191 217.943834 
L 135.93966 214.530452 
L 136.019128 217.353335 
L 136.178064 209.994045 
L 136.257533 216.563049 
L 136.337001 212.81332 
L 136.495938 216.742814 
L 136.575406 214.169313 
L 136.654874 207.483297 
L 136.734343 219.043684 
L 136.813811 216.996343 
L 136.972748 210.676188 
L 137.052216 220.121927 
L 137.131684 218.483794 
L 137.211153 216.414146 
L 137.290621 200.213261 
L 137.370089 219.570486 
L 137.449557 216.873367 
L 137.529026 198.515658 
L 137.608494 214.340816 
L 137.687962 206.875615 
L 137.767431 210.083931 
L 137.846899 220.135158 
L 137.926367 208.272478 
L 138.005836 212.932041 
L 138.085304 218.704736 
L 138.244241 209.37271 
L 138.323709 207.488162 
L 138.403177 218.909682 
L 138.482646 218.696646 
L 138.562114 218.533033 
L 138.72105 208.774669 
L 138.800519 218.81029 
L 138.879987 217.737906 
L 138.959455 210.652912 
L 139.038924 216.323085 
L 139.118392 206.348694 
L 139.277329 220.204486 
L 139.356797 216.445621 
L 139.436265 218.807462 
L 139.595202 212.074752 
L 139.67467 218.035578 
L 139.754139 214.427859 
L 139.913075 218.563965 
L 139.992543 216.270274 
L 140.072012 209.217751 
L 140.15148 220.721804 
L 140.230948 218.70086 
L 140.389885 212.646957 
L 140.469353 221.949804 
L 140.548822 219.836173 
L 140.62829 218.175967 
L 140.707758 202.483925 
L 140.787227 221.224938 
L 140.866695 218.342367 
L 140.946163 201.442238 
L 141.025632 215.921106 
L 141.1051 208.820969 
L 141.184568 212.38033 
L 141.264036 221.779766 
L 141.343505 209.852851 
L 141.502441 220.316481 
L 141.661378 211.33431 
L 141.740846 209.888578 
L 141.820315 220.62368 
L 141.899783 220.267258 
L 141.979251 220.096496 
L 142.138188 210.860775 
L 142.217656 220.462872 
L 142.297125 219.128876 
L 142.376593 212.50896 
L 142.456061 217.805422 
L 142.535529 208.159782 
L 142.694466 221.917433 
L 142.773934 217.661204 
L 142.853403 220.167197 
L 143.012339 213.843005 
L 143.091808 219.577184 
L 143.171276 215.80403 
L 143.330213 220.15192 
L 143.409681 217.881799 
L 143.489149 210.729618 
L 143.568618 222.246796 
L 143.648086 220.464573 
L 143.807022 214.425864 
L 143.886491 223.593292 
L 143.965959 221.09048 
L 144.045427 219.632527 
L 144.124896 204.538817 
L 144.204364 222.620198 
L 144.283832 219.593188 
L 144.363301 204.007326 
L 144.442769 217.454644 
L 144.522237 210.699368 
L 144.601706 214.304891 
L 144.681174 223.250833 
L 144.760642 211.342973 
L 144.919579 221.648242 
L 145.078515 213.205625 
L 145.157984 211.908512 
L 145.237452 222.152019 
L 145.31692 221.678082 
L 145.396389 221.443238 
L 145.555325 212.43964 
L 145.634794 221.829325 
L 145.714262 220.516034 
L 145.79373 214.110249 
L 145.873199 218.755668 
L 145.952667 210.296935 
L 146.032135 212.79974 
L 146.111604 223.453283 
L 146.191072 220.043061 
L 146.27054 221.599266 
L 146.429477 215.731929 
L 146.508945 221.270931 
L 146.588413 216.921822 
L 146.74735 221.567597 
L 146.826818 219.269067 
L 146.906287 211.924796 
L 146.985755 223.571572 
L 147.065223 221.967796 
L 147.22416 215.973632 
L 147.303628 224.995126 
L 147.383097 222.212534 
L 147.462565 220.899461 
L 147.542033 206.332744 
L 147.621501 223.850026 
L 147.70097 220.708932 
L 147.780438 206.283548 
L 147.859906 218.718736 
L 147.939375 212.435308 
L 148.018843 215.940757 
L 148.098311 224.540551 
L 148.17778 212.839415 
L 148.336716 222.699333 
L 148.416185 214.815477 
L 148.495653 214.89912 
L 148.575121 213.804041 
L 148.65459 223.509296 
L 148.734058 222.932963 
L 148.813526 222.581651 
L 148.972463 213.75065 
L 149.051931 222.953813 
L 149.131399 221.834388 
L 149.210868 215.529963 
L 149.290336 219.564206 
L 149.369804 212.876519 
L 149.449273 215.040474 
L 149.528741 224.360139 
L 149.608209 221.740716 
L 149.687678 222.76369 
L 149.846614 217.711783 
L 149.926083 223.027371 
L 150.005551 218.037458 
L 150.164487 222.93961 
L 150.243956 220.797949 
L 150.323424 213.057909 
L 150.402892 224.76847 
L 150.482361 223.144702 
L 150.641297 217.490361 
L 150.720766 226.138094 
L 150.800234 223.092689 
L 150.879702 222.029393 
L 150.959171 207.958473 
L 151.038639 224.921751 
L 151.118107 221.710412 
L 151.197576 208.355402 
L 151.277044 219.689953 
L 151.356512 214.035437 
L 151.43598 217.383359 
L 151.515449 225.652972 
L 151.594917 214.28892 
L 151.753854 223.462113 
L 151.833322 216.233203 
L 151.91279 216.419989 
L 151.992259 215.654109 
L 152.071727 224.655651 
L 152.151195 224.040926 
L 152.230664 223.592135 
L 152.3896 214.8593 
L 152.469069 223.910647 
L 152.548537 223.027055 
L 152.628005 216.884999 
L 152.707473 220.302445 
L 152.786942 215.262946 
L 152.945878 225.058843 
L 153.025347 223.078151 
L 153.104815 223.762323 
L 153.263752 219.327653 
L 153.34322 224.717926 
L 153.422688 219.342473 
L 153.581625 224.150235 
L 153.661093 222.220321 
L 153.740562 214.088589 
L 153.82003 225.886895 
L 153.899498 224.148163 
L 154.058435 219.041242 
L 154.137903 227.076599 
L 154.217371 223.891289 
L 154.29684 223.021858 
L 154.376308 209.440645 
L 154.455776 225.852181 
L 154.535245 222.585417 
L 154.614713 210.209367 
L 154.694181 220.484897 
L 154.77365 215.677065 
L 154.853118 218.758038 
L 154.932586 226.632525 
L 155.012055 215.668921 
L 155.170991 224.167051 
L 155.250459 217.539605 
L 155.329928 217.703578 
L 155.409396 217.240086 
L 155.488864 225.602147 
L 155.568333 224.988178 
L 155.647801 224.568636 
L 155.806738 215.756041 
L 155.886206 224.750112 
L 155.965674 224.066704 
L 156.045143 218.249622 
L 156.124611 221.047401 
L 156.204079 216.908639 
L 156.363016 225.648896 
L 156.680889 220.55197 
L 156.760357 226.04526 
L 156.839826 220.595112 
L 156.998762 225.147804 
L 157.078231 223.439823 
L 157.157699 215.053132 
L 157.237167 226.865011 
L 157.316636 225.186069 
L 157.475572 220.508469 
L 157.55504 227.89382 
L 157.713977 223.884735 
L 157.793445 210.984307 
L 157.872914 226.719762 
L 157.952382 223.408099 
L 158.03185 211.765408 
L 158.111319 221.112268 
L 158.190787 217.326911 
L 158.270255 220.008422 
L 158.349724 227.517723 
L 158.429192 216.954611 
L 158.588129 225.078432 
L 158.667597 218.482678 
L 158.747065 218.735133 
L 158.826533 218.500419 
L 158.906002 226.398835 
L 158.98547 225.765822 
L 159.064938 225.529037 
L 159.223875 216.511534 
L 159.303343 225.504566 
L 159.382812 225.078666 
L 159.46228 219.617941 
L 159.541748 221.797389 
L 159.621217 218.345168 
L 159.780153 226.03128 
L 159.859622 224.574245 
L 159.93909 225.165038 
L 160.098026 221.520869 
L 160.177495 227.05042 
L 160.256963 221.661972 
L 160.4159 226.017235 
L 160.495368 224.412488 
L 160.574836 215.989067 
L 160.654305 227.736345 
L 160.733773 226.236974 
L 160.813241 221.752625 
L 160.89271 221.758162 
L 160.972178 228.646871 
L 161.131115 224.684172 
L 161.210583 212.495917 
L 161.290051 227.504581 
L 161.369519 224.234305 
L 161.448988 213.082458 
L 161.528456 221.62666 
L 161.607924 219.020387 
L 161.687393 220.874779 
L 161.766861 228.298702 
L 161.846329 218.173598 
L 162.005266 226.066475 
L 162.084734 219.212025 
L 162.164203 219.59701 
L 162.243671 219.621127 
L 162.323139 227.096558 
L 162.402608 226.444747 
L 162.482076 226.424819 
L 162.641012 217.209568 
L 162.720481 226.30522 
L 162.799949 226.038684 
L 162.879417 220.800269 
L 162.958886 222.507514 
L 163.038354 219.871615 
L 163.197291 226.533395 
L 163.515164 222.614997 
L 163.594632 228.003748 
L 163.674101 222.663505 
L 163.833037 226.761004 
L 163.912505 225.243581 
L 163.991974 217.122637 
L 164.071442 228.536442 
L 164.15091 227.370613 
L 164.230379 222.57899 
L 164.309847 222.836221 
L 164.389315 229.381435 
L 164.548252 225.443781 
L 164.62772 213.845663 
L 164.707189 228.221233 
L 164.786657 225.133309 
L 164.866125 214.372894 
L 164.945594 221.954032 
L 165.025062 221.051565 
L 165.10453 221.427819 
L 165.183998 228.973718 
L 165.263467 219.156768 
L 165.422403 227.052191 
L 165.501872 219.890035 
L 165.58134 220.286525 
L 165.660808 220.638022 
L 165.740277 227.767108 
L 165.819745 227.124261 
L 165.899213 227.301011 
L 166.05815 217.776221 
L 166.137618 227.038753 
L 166.217087 226.96256 
L 166.296555 221.879739 
L 166.376023 223.236076 
L 166.455491 221.337122 
L 166.614428 227.188396 
L 166.932301 224.053327 
L 167.01177 229.001761 
L 167.091238 223.855373 
L 167.250175 227.369289 
L 167.329643 225.770173 
L 167.409111 218.039391 
L 167.48858 229.247834 
L 167.568048 228.61402 
L 167.647516 223.359349 
L 167.726984 223.79622 
L 167.806453 230.122374 
L 167.965389 226.127733 
L 168.044858 214.883541 
L 168.124326 228.872755 
L 168.203794 226.389644 
L 168.283263 216.476467 
L 168.362731 222.289767 
L 168.442199 222.261364 
L 168.521668 222.963411 
L 168.601136 229.650395 
L 168.680604 219.69273 
L 168.839541 228.094712 
L 168.919009 220.377345 
L 168.998477 220.956916 
L 169.077946 221.504476 
L 169.157414 228.43045 
L 169.236882 227.780017 
L 169.316351 228.074636 
L 169.475287 218.448922 
L 169.634224 227.836446 
L 169.713692 222.914115 
L 169.793161 223.932165 
L 169.872629 222.480212 
L 170.031566 227.931651 
L 170.111034 227.21705 
L 170.190502 227.227862 
L 170.26997 226.921594 
L 170.349439 224.990587 
L 170.428907 229.712155 
L 170.508375 225.373733 
L 170.587844 226.960614 
L 170.667312 227.899713 
L 170.74678 225.96 
L 170.826249 218.672396 
L 170.905717 229.876585 
L 170.985185 229.4806 
L 171.064654 224.164161 
L 171.144122 224.742056 
L 171.22359 230.75509 
L 171.382527 226.774926 
L 171.461995 215.748778 
L 171.541463 229.463672 
L 171.620932 227.646317 
L 171.7004 218.094816 
L 171.859337 223.246233 
L 171.938805 223.624206 
L 172.018273 230.243487 
L 172.097742 220.283651 
L 172.256678 228.864438 
L 172.336147 221.220696 
L 172.415615 221.847475 
L 172.495083 222.908773 
L 172.574552 229.04414 
L 172.65402 228.237757 
L 172.733488 228.779018 
L 172.892425 219.028136 
L 173.051361 228.601498 
L 173.13083 223.865564 
L 173.210298 224.600862 
L 173.289766 223.45802 
L 173.448703 228.618919 
L 173.60764 227.670212 
L 173.687108 228.559941 
L 173.766576 225.141353 
L 173.846045 230.373214 
L 173.925513 227.361255 
L 174.004981 227.616773 
L 174.084449 228.534832 
L 174.163918 226.806946 
L 174.243386 220.363704 
L 174.322854 230.474096 
L 174.402323 230.062332 
L 174.481791 224.913687 
L 174.561259 225.800175 
L 174.640728 231.219346 
L 174.799664 227.318573 
L 174.879133 216.706993 
L 174.958601 230.008023 
L 175.038069 228.361739 
L 175.117538 219.368895 
L 175.276474 224.164889 
L 175.355942 224.258347 
L 175.435411 230.829522 
L 175.514879 221.044579 
L 175.673816 229.443126 
L 175.753284 222.510149 
L 175.832752 222.952228 
L 176.150626 229.503683 
L 176.309562 219.606694 
L 176.468499 229.283802 
L 176.547967 224.770621 
L 176.627435 225.401251 
L 176.706904 224.513153 
L 176.86584 229.393314 
L 177.024777 228.031784 
L 177.104245 229.612869 
L 177.183714 225.072633 
L 177.263182 230.93492 
L 177.34265 228.434237 
L 177.422119 228.641643 
L 177.501587 229.190578 
L 177.581055 227.709748 
L 177.660524 221.762055 
L 177.739992 230.973217 
L 177.81946 230.58245 
L 177.898928 225.720157 
L 177.978397 226.581944 
L 178.057865 231.648424 
L 178.216802 227.833535 
L 178.29627 217.580595 
L 178.375738 230.486002 
L 178.455207 228.842854 
L 178.534675 220.241977 
L 178.693612 225.056957 
L 178.77308 224.91299 
L 178.852548 231.390481 
L 178.932017 221.883506 
L 179.090953 229.932085 
L 179.170421 223.790688 
L 179.24989 224.842894 
L 179.329358 226.127679 
L 179.408826 229.787407 
L 179.488295 228.848813 
L 179.567763 230.114268 
L 179.7267 220.208145 
L 179.885636 229.844976 
L 179.965105 225.503002 
L 180.044573 226.103705 
L 180.124041 225.350239 
L 180.282978 230.055514 
L 180.441914 228.661888 
L 180.521383 230.314505 
L 180.600851 225.530464 
L 180.680319 231.448262 
L 180.759788 229.017921 
L 180.839256 229.241544 
L 180.918724 229.665823 
L 180.998193 228.268592 
L 181.077661 222.669815 
L 181.157129 231.42048 
L 181.236598 231.013667 
L 181.316066 226.389088 
L 181.395534 227.2655 
L 181.475003 232.037514 
L 181.633939 228.349432 
L 181.713407 218.354136 
L 181.792876 230.930371 
L 181.872344 229.302648 
L 181.951812 220.966355 
L 182.031281 224.264638 
L 182.110749 225.880332 
L 182.190217 225.528797 
L 182.269686 231.885765 
L 182.349154 222.618231 
L 182.508091 230.428234 
L 182.587559 224.719587 
L 182.667027 225.82344 
L 182.746496 226.747174 
L 182.825964 230.360655 
L 182.905432 229.483542 
L 182.9849 230.537609 
L 183.143837 220.753932 
L 183.302774 230.326261 
L 183.382242 226.213284 
L 183.46171 226.694782 
L 183.541179 226.019355 
L 183.700115 230.704609 
L 183.859052 229.338875 
L 183.93852 230.905015 
L 184.017989 225.952136 
L 184.097457 231.872947 
L 184.176925 229.535122 
L 184.335862 230.093134 
L 184.41533 228.883718 
L 184.494798 223.61884 
L 184.574267 231.841198 
L 184.653735 231.413321 
L 184.733203 226.995048 
L 184.812672 227.962663 
L 184.89214 232.373989 
L 185.051077 228.883571 
L 185.130545 219.085676 
L 185.210013 231.394769 
L 185.289482 229.772629 
L 185.36895 221.789566 
L 185.448418 224.253854 
L 185.686823 232.333605 
L 185.766291 223.337135 
L 185.925228 230.864063 
L 186.004696 225.435996 
L 186.084165 226.547488 
L 186.163633 227.1308 
L 186.243101 230.869004 
L 186.32257 230.066481 
L 186.402038 230.886081 
L 186.560975 221.303454 
L 186.719911 230.749857 
L 186.799379 226.923327 
L 186.878848 227.17819 
L 186.958316 226.588104 
L 187.117253 231.36629 
L 187.276189 229.989733 
L 187.355658 231.414865 
L 187.435126 226.433941 
L 187.514594 232.246924 
L 187.594063 229.987367 
L 187.752999 230.49027 
L 187.832468 229.530244 
L 187.911936 224.508832 
L 187.991404 232.229917 
L 188.070872 231.76966 
L 188.150341 227.564298 
L 188.229809 228.594633 
L 188.309277 232.675345 
L 188.468214 229.471636 
L 188.547682 219.771646 
L 188.627151 231.834499 
L 188.706619 230.247597 
L 188.786087 222.816908 
L 188.865556 224.46439 
L 189.103961 232.74899 
L 189.183429 224.071756 
L 189.342365 231.278783 
L 189.421834 225.996773 
L 189.501302 227.150134 
L 189.58077 227.541784 
L 189.660239 231.316558 
L 189.739707 230.548217 
L 189.819175 231.171706 
L 189.978112 221.828772 
L 190.137049 231.142157 
L 190.295985 227.680754 
L 190.375454 227.034274 
L 190.53439 231.952039 
L 190.613858 230.533676 
L 190.693327 230.581997 
L 190.772795 231.862401 
L 190.852263 226.961007 
L 190.931732 232.571325 
L 191.0112 230.360396 
L 191.170137 230.884625 
L 191.249605 230.177782 
L 191.329073 225.291752 
L 191.408542 232.591231 
L 191.48801 232.095606 
L 191.567478 228.117554 
L 191.646947 229.161791 
L 191.726415 232.946117 
L 191.885351 230.068269 
L 191.96482 220.477867 
L 192.044288 232.224678 
L 192.123756 230.644123 
L 192.203225 223.689601 
L 192.282693 225.214068 
L 192.362161 229.072045 
L 192.44163 226.968028 
L 192.521098 233.127538 
L 192.600566 224.738289 
L 192.759503 231.732482 
L 192.838971 226.424147 
L 192.91844 227.600163 
L 192.997908 227.989694 
L 193.077376 231.687897 
L 193.156844 230.910516 
L 193.236313 231.444043 
L 193.395249 222.295286 
L 193.554186 231.483352 
L 193.713123 228.220565 
L 193.792591 227.387274 
L 193.951528 232.44592 
L 194.030996 230.927612 
L 194.110464 231.126929 
L 194.189933 232.277655 
L 194.269401 227.508606 
L 194.348869 232.847502 
L 194.428337 230.699215 
L 194.507806 231.30248 
L 194.587274 231.272834 
L 194.666742 230.736062 
L 194.746211 225.968546 
L 194.825679 232.910586 
L 194.905147 232.397014 
L 194.984616 228.646861 
L 195.064084 229.711314 
L 195.143552 233.189148 
L 195.302489 230.548218 
L 195.381957 221.268822 
L 195.461425 232.552819 
L 195.540894 230.926937 
L 195.620362 224.386674 
L 195.69983 225.674413 
L 195.779299 230.218003 
L 195.858767 227.321102 
L 195.938235 233.474018 
L 196.017704 225.378966 
L 196.17664 232.113126 
L 196.256109 226.758124 
L 196.335577 228.039938 
L 196.415045 228.534711 
L 196.494514 232.044735 
L 196.573982 231.198312 
L 196.65345 231.738677 
L 196.812387 222.673878 
L 196.971323 231.800513 
L 197.13026 228.74981 
L 197.209728 227.790591 
L 197.368665 232.712766 
L 197.448133 231.361962 
L 197.527602 231.629837 
L 197.60707 232.662116 
L 197.686538 228.070791 
L 197.766007 233.129838 
L 197.845475 231.032694 
L 197.924943 231.737814 
L 198.004411 231.636386 
L 198.08388 231.218468 
L 198.163348 226.554524 
L 198.242816 233.206753 
L 198.322285 232.690525 
L 198.401753 229.185863 
L 198.481221 230.240124 
L 198.56069 233.413098 
L 198.719626 230.939905 
L 198.799095 221.927575 
L 198.878563 232.869519 
L 198.958031 231.226581 
L 199.0375 225.036628 
L 199.116968 226.046914 
L 199.196436 230.720326 
L 199.275904 227.745603 
L 199.355373 233.782519 
L 199.434841 225.99726 
L 199.593778 232.470196 
L 199.673246 227.075571 
L 199.752714 228.44094 
L 199.832183 228.990798 
L 199.911651 232.357115 
L 199.991119 231.427387 
L 200.070588 232.024877 
L 200.229524 222.996018 
L 200.388461 232.087099 
L 200.547397 229.220529 
L 200.626866 228.20813 
L 200.785802 232.909167 
L 200.865271 231.757383 
L 200.944739 232.082771 
L 201.024207 233.00636 
L 201.103676 228.67901 
L 201.183144 233.396548 
L 201.262612 231.376253 
L 201.342081 232.136226 
L 201.501017 231.635385 
L 201.580486 227.077005 
L 201.659954 233.474876 
L 201.739422 232.974078 
L 201.81889 229.687573 
L 201.898359 230.717722 
L 201.977827 233.628771 
L 202.136764 231.290151 
L 202.216232 222.5346 
L 202.2957 233.177562 
L 202.375169 231.5369 
L 202.454637 225.638124 
L 202.534105 226.382724 
L 202.613574 231.091 
L 202.693042 228.166609 
L 202.77251 234.058291 
L 202.851979 226.641169 
L 203.010915 232.823277 
L 203.090383 227.377433 
L 203.169852 228.808474 
L 203.24932 229.411378 
L 203.328788 232.640467 
L 203.408257 231.619826 
L 203.487725 232.290028 
L 203.646662 223.273144 
L 203.805598 232.34854 
L 203.964535 229.667113 
L 204.044003 228.597875 
L 204.20294 233.136352 
L 204.282408 232.111055 
L 204.361876 232.480914 
L 204.441345 233.312716 
L 204.520813 229.299979 
L 204.600281 233.643444 
L 204.67975 231.718857 
L 204.759218 232.492918 
L 204.838686 232.243546 
L 204.918155 232.002731 
L 204.997623 227.546167 
L 205.077091 233.722871 
L 205.15656 233.248293 
L 205.236028 230.14495 
L 205.315496 231.151004 
L 205.394965 233.834636 
L 205.553901 231.625873 
L 205.633369 223.116805 
L 205.712838 233.474917 
L 205.792306 231.85774 
L 205.871774 226.177591 
L 205.951243 226.695656 
L 206.030711 231.404901 
L 206.110179 228.558411 
L 206.189648 234.308443 
L 206.269116 227.304361 
L 206.428053 233.19928 
L 206.507521 227.671835 
L 206.586989 229.1473 
L 206.666458 229.84197 
L 206.745926 232.917212 
L 206.825394 231.810859 
L 206.904862 232.541673 
L 207.063799 223.513287 
L 207.222736 232.587476 
L 207.381672 230.107222 
L 207.461141 228.964817 
L 207.620077 233.371039 
L 207.699546 232.433318 
L 207.858482 233.591849 
L 207.937951 229.898341 
L 208.017419 233.877032 
L 208.096887 232.052082 
L 208.176355 232.821579 
L 208.255824 232.524427 
L 208.335292 232.328645 
L 208.41476 227.961328 
L 208.494229 233.952525 
L 208.573697 233.514499 
L 208.653165 230.559566 
L 208.732634 231.546619 
L 208.812102 234.030205 
L 208.971039 231.955049 
L 209.050507 223.659229 
L 209.129975 233.75702 
L 209.209444 232.182623 
L 209.288912 226.656573 
L 209.36838 226.995041 
L 209.447848 231.689439 
L 209.527317 228.926322 
L 209.606785 234.537997 
L 209.686253 227.931279 
L 209.84519 233.578915 
L 209.924658 227.980914 
L 210.004127 229.46968 
L 210.083595 230.289245 
L 210.163063 233.2009 
L 210.242532 232.052161 
L 210.322 232.78675 
L 210.480937 223.730101 
L 210.639873 232.802129 
L 210.79881 230.531652 
L 210.878278 229.323718 
L 211.037215 233.600623 
L 211.116683 232.729706 
L 211.27562 233.84547 
L 211.355088 230.46569 
L 211.434556 234.104371 
L 211.514025 232.369785 
L 211.593493 233.132164 
L 211.75243 232.62427 
L 211.831898 228.33066 
L 211.911366 234.164302 
L 211.990834 233.770839 
L 212.070303 230.934652 
L 212.149771 231.909559 
L 212.229239 234.216735 
L 212.388176 232.273339 
L 212.467644 224.154154 
L 212.547113 234.016055 
L 212.626581 232.495543 
L 212.706049 227.092808 
L 212.785518 227.281942 
L 212.864986 231.961286 
L 212.944454 229.276122 
L 213.023923 234.748937 
L 213.103391 228.488915 
L 213.262327 233.917279 
L 213.341796 228.310204 
L 213.421264 229.785538 
L 213.500732 230.733048 
L 213.580201 233.483521 
L 213.659669 232.34719 
L 213.739137 233.030516 
L 213.898074 223.936534 
L 214.057011 232.994064 
L 214.136479 230.837654 
L 214.215947 230.923502 
L 214.295416 229.676664 
L 214.454352 233.833166 
L 214.53382 233.004741 
L 214.692757 234.07366 
L 214.772225 230.995188 
L 214.851694 234.32249 
L 214.931162 232.667095 
L 215.01063 233.42643 
L 215.169567 232.903273 
L 215.249035 228.668483 
L 215.328504 234.359247 
L 215.407972 234.014095 
L 215.48744 231.274174 
L 215.566909 232.243091 
L 215.646377 234.395172 
L 215.725845 233.540926 
L 215.805313 232.573103 
L 215.884782 224.606413 
L 215.96425 234.248904 
L 216.043718 232.787465 
L 216.123187 227.501854 
L 216.202655 227.554675 
L 216.282123 232.224174 
L 216.361592 229.610006 
L 216.44106 234.942798 
L 216.520528 228.988394 
L 216.679465 234.20954 
L 216.758933 228.639099 
L 216.838402 230.094157 
L 216.91787 231.160639 
L 216.997338 233.750052 
L 217.076806 232.654127 
L 217.156275 233.275126 
L 217.315211 224.136566 
L 217.474148 233.172704 
L 217.553616 231.118839 
L 217.633085 231.27991 
L 217.712553 230.020646 
L 217.87149 234.073508 
L 217.950958 233.2648 
L 218.109895 234.280941 
L 218.189363 231.480161 
L 218.268831 234.526819 
L 218.348299 232.942711 
L 218.427768 233.702916 
L 218.586704 233.170399 
L 218.666173 228.98102 
L 218.745641 234.538823 
L 218.825109 234.244832 
L 218.904578 231.581617 
L 218.984046 232.549948 
L 219.063514 234.564859 
L 219.142983 233.79635 
L 219.222451 232.851198 
L 219.301919 225.029835 
L 219.381388 234.459038 
L 219.460856 233.058821 
L 219.619792 227.813025 
L 219.699261 232.478478 
L 219.778729 229.929231 
L 219.858197 235.121687 
L 219.937666 229.443464 
L 220.096602 234.466753 
L 220.176071 228.95654 
L 220.255539 230.394781 
L 220.414476 233.997508 
L 220.573412 233.520469 
L 220.732349 224.328062 
L 220.891285 233.348012 
L 220.970754 231.379765 
L 221.050222 231.605109 
L 221.12969 230.354771 
L 221.288627 234.315822 
L 221.368095 233.513634 
L 221.527032 234.472676 
L 221.6065 231.916354 
L 221.685969 234.716458 
L 221.765437 233.198049 
L 221.844905 233.962094 
L 222.003842 233.42446 
L 222.08331 229.270433 
L 222.162778 234.705069 
L 222.242247 234.466179 
L 222.321715 231.860923 
L 222.401183 232.832571 
L 222.480652 234.725153 
L 222.56012 234.035996 
L 222.639588 233.108147 
L 222.719057 225.439235 
L 222.798525 234.65222 
L 222.877993 233.312782 
L 223.03693 228.058418 
L 223.116398 232.724183 
L 223.195867 230.235022 
L 223.275335 235.287449 
L 223.354803 229.857523 
L 223.51374 234.696761 
L 223.593208 229.260285 
L 223.672676 230.688874 
L 223.831613 234.230352 
L 223.911081 233.202208 
L 223.99055 233.765938 
L 224.149486 224.508346 
L 224.308423 233.525281 
L 224.387891 231.619761 
L 224.46736 231.903561 
L 224.546828 230.677345 
L 224.705764 234.552981 
L 224.785233 233.752681 
L 224.944169 234.652984 
L 225.023638 232.304163 
L 225.103106 234.892884 
L 225.182574 233.436825 
L 225.262043 234.205687 
L 225.341511 233.639874 
L 225.420979 233.663497 
L 225.500448 229.538977 
L 225.579916 234.859973 
L 225.659384 234.681723 
L 225.738853 232.11616 
L 225.818321 233.093481 
L 225.897789 234.875938 
L 225.977257 234.25958 
L 226.056726 233.345374 
L 226.136194 225.849699 
L 226.215662 234.834149 
L 226.295131 233.552479 
L 226.454067 228.292814 
L 226.533536 232.960986 
L 226.613004 230.528106 
L 226.692472 235.44155 
L 226.771941 230.230224 
L 226.930877 234.904577 
L 227.010346 229.549915 
L 227.089814 230.977932 
L 227.24875 234.454217 
L 227.328219 233.432234 
L 227.407687 234.010909 
L 227.566624 224.675753 
L 227.72556 233.704517 
L 227.805029 231.840199 
L 227.884497 232.180358 
L 227.963965 230.986086 
L 228.122902 234.779391 
L 228.20237 233.982761 
L 228.361307 234.824716 
L 228.440775 232.647516 
L 228.520243 235.058151 
L 228.599712 233.664102 
L 228.67918 234.435462 
L 228.758648 233.819585 
L 228.838117 233.886635 
L 228.917585 229.789227 
L 228.997053 235.005215 
L 229.076522 234.893573 
L 229.15599 232.351142 
L 229.235458 233.335417 
L 229.314927 235.017166 
L 229.394395 234.465848 
L 229.473863 233.564027 
L 229.553332 226.279771 
L 229.6328 235.00982 
L 229.712268 233.78234 
L 229.871205 228.517504 
L 229.950673 233.188103 
L 230.030141 230.807789 
L 230.10961 235.585209 
L 230.189078 230.561567 
L 230.348015 235.093932 
L 230.427483 229.825489 
L 230.506951 231.260324 
L 230.665888 234.67222 
L 230.745356 233.636588 
L 230.824825 234.254645 
L 230.983761 224.829355 
L 231.142698 233.883498 
L 231.222166 232.04499 
L 231.301634 232.441536 
L 231.381103 231.278517 
L 231.540039 234.991457 
L 231.619508 234.204265 
L 231.778444 234.989547 
L 231.857913 232.951866 
L 231.937381 235.214165 
L 232.016849 233.88408 
L 232.096317 234.652903 
L 232.175786 233.989454 
L 232.255254 234.094514 
L 232.334722 230.023693 
L 232.414191 235.142311 
L 232.493659 235.098936 
L 232.573127 232.570077 
L 232.652596 233.560999 
L 232.732064 235.148543 
L 232.811532 234.652436 
L 232.891001 233.76506 
L 232.970469 226.749712 
L 233.049937 235.184064 
L 233.129406 234.009775 
L 233.288342 228.731341 
L 233.36781 233.402651 
L 233.447279 231.071258 
L 233.526747 235.719595 
L 233.606215 230.854274 
L 233.765152 235.267854 
L 233.84462 230.08895 
L 233.924089 231.528769 
L 234.083025 234.881379 
L 234.162494 233.819602 
L 234.241962 234.49495 
L 234.400899 224.969748 
L 234.559835 234.060537 
L 234.639303 232.240143 
L 234.718772 232.692375 
L 234.79824 231.551869 
L 234.957177 235.187769 
L 235.036645 234.415178 
L 235.195582 235.147612 
L 235.27505 233.223191 
L 235.354518 235.362626 
L 235.433987 234.097132 
L 235.513455 234.859165 
L 235.592923 234.14986 
L 235.672392 234.289231 
L 235.75186 230.244149 
L 235.910796 235.288859 
L 235.990265 232.778225 
L 236.069733 233.772354 
L 236.149201 235.270053 
L 236.22867 234.81861 
L 236.308138 233.949817 
L 236.387606 227.259425 
L 236.467075 235.363908 
L 236.546543 234.242156 
L 236.70548 228.932518 
L 236.784948 233.601673 
L 236.864416 231.318335 
L 236.943885 235.846402 
L 237.023353 231.120879 
L 237.182289 235.429075 
L 237.261758 230.349595 
L 237.341226 231.774913 
L 237.500163 235.0713 
L 237.579631 233.985268 
L 237.659099 234.724457 
L 237.818036 225.101548 
L 237.897504 234.244952 
L 237.976973 234.235361 
L 238.056441 232.430091 
L 238.135909 232.932262 
L 238.215378 231.804642 
L 238.374314 235.369204 
L 238.453782 234.609239 
L 238.533251 235.32487 
L 238.612719 235.297433 
L 238.692187 233.468296 
L 238.771656 235.504741 
L 238.851124 234.300298 
L 238.930592 235.054903 
L 239.010061 234.303213 
L 239.089529 234.473843 
L 239.168997 230.451242 
L 239.327934 235.456942 
L 239.407402 232.979343 
L 239.566339 235.382723 
L 239.645807 234.968357 
L 239.725275 234.120421 
L 239.804744 227.7491 
L 239.884212 235.550632 
L 239.96368 234.479131 
L 240.122617 229.122545 
L 240.202085 233.789409 
L 240.281554 231.554485 
L 240.361022 235.968037 
L 240.44049 231.379488 
L 240.599427 235.579569 
L 240.678895 230.620814 
L 240.758364 231.996937 
L 240.9173 235.235504 
L 240.996768 234.135358 
L 241.076237 234.93109 
L 241.235173 225.231313 
L 241.314642 234.46748 
L 241.39411 234.406108 
L 241.473578 232.612556 
L 241.553047 233.157009 
L 241.632515 232.037984 
L 241.791452 235.538074 
L 241.87092 234.780138 
L 241.950388 235.563248 
L 242.029857 235.43744 
L 242.109325 233.693219 
L 242.188793 235.639944 
L 242.268261 234.491597 
L 242.34773 235.239206 
L 242.427198 234.454973 
L 242.506666 234.650889 
L 242.586135 230.645097 
L 242.745071 235.606706 
L 242.82454 233.173394 
L 242.983476 235.48849 
L 243.062945 235.107908 
L 243.142413 234.279248 
L 243.221881 228.145172 
L 243.30135 235.729814 
L 243.380818 234.71116 
L 243.539754 229.30274 
L 243.619223 233.97386 
L 243.698691 231.77849 
L 243.778159 236.085284 
L 243.857628 231.628916 
L 244.016564 235.71969 
L 244.096033 230.901277 
L 244.175501 232.196329 
L 244.334438 235.380327 
L 244.413906 234.270113 
L 244.493374 235.110821 
L 244.652311 225.360362 
L 244.731779 234.680639 
L 244.811247 234.566455 
L 244.890716 232.782968 
L 244.970184 233.373666 
L 245.049652 232.254837 
L 245.208589 235.695968 
L 245.288057 234.92962 
L 245.367526 235.776901 
L 245.446994 235.567093 
L 245.526462 233.900353 
L 245.605931 235.765911 
L 245.685399 234.671024 
L 245.764867 235.409977 
L 245.844336 234.605034 
L 245.923804 234.820995 
L 246.003272 230.826155 
L 246.162209 235.744029 
L 246.241677 233.360394 
L 246.400614 235.589533 
L 246.480082 235.240385 
L 246.55955 234.427786 
L 246.639019 228.449973 
L 246.718487 235.893209 
L 246.797955 234.93043 
L 246.956892 229.473042 
L 247.03636 234.156691 
L 247.115829 231.988995 
L 247.195297 236.197922 
L 247.274765 231.860035 
L 247.433702 235.850782 
L 247.51317 231.186321 
L 247.592638 232.376224 
L 247.751575 235.513255 
L 247.831043 234.39096 
L 247.910512 235.269107 
L 248.069448 225.487278 
L 248.148917 234.878223 
L 248.228385 234.713031 
L 248.307853 232.941585 
L 248.387322 233.595185 
L 248.46679 232.459692 
L 248.625726 235.843315 
L 248.705195 235.064697 
L 248.784663 235.964214 
L 248.864131 235.686799 
L 248.9436 234.09052 
L 249.023068 235.881356 
L 249.102536 234.839774 
L 249.182005 235.567405 
L 249.261473 234.749384 
L 249.340941 234.983735 
L 249.42041 230.995671 
L 249.579346 235.872045 
L 249.658815 233.541643 
L 249.817751 235.687203 
L 249.897219 235.365764 
L 249.976688 234.56652 
L 250.056156 228.698056 
L 250.135624 236.042244 
L 250.215093 235.135239 
L 250.374029 229.634337 
L 250.453498 234.330081 
L 250.532966 232.188106 
L 250.612434 236.306394 
L 250.691903 232.072228 
L 250.850839 235.974801 
L 250.930308 231.47721 
L 251.009776 232.540496 
L 251.168712 235.637015 
L 251.248181 234.499731 
L 251.327649 235.411568 
L 251.407117 232.540623 
L 251.486586 225.611369 
L 251.566054 235.06071 
L 251.645522 234.845948 
L 251.724991 233.092526 
L 251.804459 233.829417 
L 251.883927 232.656419 
L 252.042864 235.979986 
L 252.122332 235.190155 
L 252.201801 236.128132 
L 252.281269 235.797405 
L 252.360737 234.265182 
L 252.440205 235.986992 
L 252.519674 234.999563 
L 252.599142 235.713575 
L 252.67861 234.886697 
L 252.758079 235.138763 
L 252.837547 231.15503 
L 252.996484 235.992235 
L 253.075952 233.718134 
L 253.234889 235.781995 
L 253.314357 235.483914 
L 253.393825 234.695634 
L 253.473294 228.911945 
L 253.552762 236.178891 
L 253.63223 235.326313 
L 253.791167 229.78875 
L 253.870635 234.48951 
L 253.950103 232.377543 
L 254.029572 236.410924 
L 254.10904 232.268557 
L 254.267977 236.092843 
L 254.347445 231.776185 
L 254.426913 232.69228 
L 254.58585 235.752392 
L 254.665318 234.598367 
L 254.744787 235.541787 
L 254.824255 232.698471 
L 254.903723 225.73234 
L 254.983191 235.230624 
L 255.06266 234.966177 
L 255.142128 233.239512 
L 255.221596 234.078576 
L 255.301065 232.848696 
L 255.460001 236.105852 
L 255.53947 235.307783 
L 255.618938 236.272639 
L 255.698406 235.899674 
L 255.777875 234.426298 
L 255.857343 236.085237 
L 255.936811 235.151915 
L 256.01628 235.850127 
L 256.095748 235.018015 
L 256.175216 235.285973 
L 256.254684 231.305292 
L 256.413621 236.105157 
L 256.493089 233.890612 
L 256.652026 235.874092 
L 256.731494 235.595378 
L 256.810963 234.815627 
L 256.890431 229.103151 
L 256.969899 236.304376 
L 257.049368 235.504609 
L 257.208304 229.938179 
L 257.287773 234.634757 
L 257.367241 232.558561 
L 257.446709 236.511416 
L 257.526177 232.452607 
L 257.685114 236.205471 
L 257.764582 232.085245 
L 257.844051 232.834027 
L 258.002987 235.859945 
L 258.082456 234.689019 
L 258.161924 235.662178 
L 258.241392 232.848563 
L 258.320861 225.849962 
L 258.400329 235.390178 
L 258.479797 235.075417 
L 258.559266 233.384267 
L 258.638734 234.338522 
L 258.718202 233.03988 
L 258.877139 236.221042 
L 258.956607 235.418178 
L 259.036075 236.401485 
L 259.115544 235.994421 
L 259.195012 234.575635 
L 259.27448 236.180367 
L 259.353949 235.297279 
L 259.433417 235.977444 
L 259.512885 235.14614 
L 259.592354 235.425286 
L 259.671822 231.4476 
L 259.830759 236.21087 
L 259.910227 234.059859 
L 260.069163 235.963661 
L 260.148632 235.701079 
L 260.2281 234.927504 
L 260.307568 229.278586 
L 260.387037 236.419754 
L 260.466505 235.670148 
L 260.625442 230.083436 
L 260.70491 234.767281 
L 260.784378 232.732088 
L 260.863847 236.607786 
L 260.943315 232.627695 
L 261.102252 236.313094 
L 261.18172 232.40541 
L 261.261188 232.967911 
L 261.420125 235.960467 
L 261.499593 234.774276 
L 261.579061 235.774559 
L 261.65853 232.995516 
L 261.737998 225.963964 
L 261.817466 235.540876 
L 261.896935 235.175902 
L 261.976403 233.527044 
L 262.055871 234.597311 
L 262.13534 233.229819 
L 262.294276 236.325942 
L 262.373745 235.522064 
L 262.453213 236.517906 
L 262.532681 236.082876 
L 262.612149 234.714774 
L 262.691618 236.277139 
L 262.771086 235.434179 
L 262.850554 236.095224 
L 262.930023 235.274641 
L 263.009491 235.556546 
L 263.088959 231.583552 
L 263.247896 236.309413 
L 263.327364 234.226371 
L 263.486301 236.050941 
L 263.565769 235.801884 
L 263.645238 235.032658 
L 263.724706 229.443576 
L 263.804174 236.526176 
L 263.883642 235.821822 
L 264.042579 230.224179 
L 264.122047 234.888895 
L 264.201516 232.898481 
L 264.280984 236.700084 
L 264.360452 232.796601 
L 264.519389 236.416024 
L 264.598857 232.735484 
L 264.678326 233.096008 
L 264.837262 236.055075 
L 264.916731 234.857476 
L 264.996199 235.880347 
L 265.075667 233.141345 
L 265.155135 226.074199 
L 265.234604 235.683519 
L 265.314072 235.269819 
L 265.39354 233.667524 
L 265.473009 234.838531 
L 265.552477 233.413479 
L 265.711414 236.421327 
L 265.790882 235.620556 
L 265.87035 236.624396 
L 265.949819 236.166688 
L 266.029287 234.845311 
L 266.108755 236.377402 
L 266.188224 235.559637 
L 266.267692 236.203758 
L 266.34716 235.404424 
L 266.426628 235.679545 
L 266.506097 231.714918 
L 266.665033 236.401181 
L 266.744502 234.389646 
L 266.903438 236.136189 
L 266.982907 235.898307 
L 267.062375 235.132449 
L 267.141843 229.602223 
L 267.221312 236.62482 
L 267.30078 235.958839 
L 267.459717 230.359593 
L 267.539185 235.001047 
L 267.618653 233.057342 
L 267.698121 236.788434 
L 267.77759 232.961718 
L 267.936526 236.514431 
L 268.015995 233.072233 
L 268.095463 233.220051 
L 268.2544 236.144852 
L 268.333868 234.942259 
L 268.413336 235.980604 
L 268.492805 233.284598 
L 268.572273 226.180703 
L 268.651741 235.818342 
L 268.73121 235.358704 
L 268.810678 233.805566 
L 268.890146 235.050126 
L 268.969614 233.58551 
L 269.128551 236.508583 
L 269.208019 235.714774 
L 269.287488 236.72281 
L 269.366956 236.247688 
L 269.446424 234.968711 
L 269.525893 236.478855 
L 269.605361 235.671429 
L 269.684829 236.304494 
L 269.764298 235.532236 
L 269.843766 235.794247 
L 269.923234 231.84297 
L 270.082171 236.486884 
L 270.161639 234.548414 
L 270.320576 236.219576 
L 270.400044 235.990513 
L 270.479512 235.22779 
L 270.558981 229.757095 
L 270.638449 236.716676 
L 270.717917 236.082053 
L 270.876854 230.489233 
L 270.956322 235.104508 
L 271.035791 233.207926 
L 271.115259 236.872931 
L 271.194727 233.12542 
L 271.353664 236.608346 
L 271.5126 233.341075 
L 271.671537 236.230398 
L 271.751005 235.031098 
L 271.830474 236.076109 
L 271.909942 233.42273 
L 271.98941 226.283516 
L 272.068879 235.945332 
L 272.148347 235.443242 
L 272.227815 233.941492 
L 272.307284 235.229671 
L 272.386752 233.74463 
L 272.545688 236.589586 
L 272.625157 235.805556 
L 272.704625 236.814806 
L 272.784093 236.327668 
L 272.863562 235.086141 
L 272.94303 236.577852 
L 273.022498 235.76981 
L 273.101967 236.399249 
L 273.181435 235.654429 
L 273.260903 235.901023 
L 273.340372 231.968267 
L 273.499308 236.567208 
L 273.578777 234.701803 
L 273.737713 236.301152 
L 273.817181 236.078498 
L 273.89665 235.31916 
L 273.976118 229.909702 
L 274.055586 236.802438 
L 274.135055 236.193501 
L 274.293991 230.613165 
L 274.37346 235.199606 
L 274.452928 233.349873 
L 274.532396 236.953633 
L 274.611865 233.289973 
L 274.770801 236.697769 
L 274.929738 233.459353 
L 275.088674 236.312008 
L 275.168143 235.123941 
L 275.247611 236.167474 
L 275.327079 233.554334 
L 275.406548 226.382621 
L 275.486016 236.064507 
L 275.565484 235.523416 
L 275.644953 234.075866 
L 275.724421 235.381388 
L 275.803889 233.892619 
L 275.962826 236.666148 
L 276.042294 235.89349 
L 276.121763 236.902039 
L 276.201231 236.408058 
L 276.280699 235.198487 
L 276.360167 236.672084 
L 276.439636 235.856859 
L 276.519104 236.489312 
L 276.598572 235.769718 
L 276.678041 236.000577 
L 276.757509 232.090966 
L 276.916446 236.642624 
L 276.995914 234.849787 
L 277.154851 236.380917 
L 277.234319 236.162221 
L 277.313787 235.40685 
L 277.393256 230.061031 
L 277.472724 236.882597 
L 277.552192 236.295361 
L 277.711129 230.731626 
L 277.790597 235.286699 
L 277.870065 233.483459 
L 277.949534 237.030602 
L 278.029002 233.456877 
L 278.187939 236.782738 
L 278.346875 233.574543 
L 278.505812 236.390159 
L 278.58528 235.218081 
L 278.664749 236.255216 
L 278.744217 233.67941 
L 278.823685 226.478076 
L 278.903153 236.175928 
L 278.982622 235.598735 
L 279.06209 234.209105 
L 279.141558 235.511048 
L 279.221027 234.031859 
L 279.379963 236.739538 
L 279.459432 235.979037 
L 279.5389 236.985909 
L 279.697837 235.306371 
L 279.777305 236.760808 
L 279.856773 235.935 
L 279.936242 236.575352 
L 280.01571 235.878419 
L 280.095178 236.093721 
L 280.174646 232.21116 
L 280.333583 236.713495 
L 280.413051 234.992723 
L 280.571988 236.458867 
L 280.651456 236.241626 
L 280.730925 235.491125 
L 280.810393 230.211488 
L 280.889861 236.95759 
L 280.96933 236.389453 
L 281.128266 230.844805 
L 281.207735 235.366548 
L 281.287203 233.60939 
L 281.366671 237.103926 
L 281.446139 233.626293 
L 281.605076 236.863317 
L 281.764013 233.685956 
L 281.922949 236.465635 
L 282.002418 235.309663 
L 282.081886 236.339761 
L 282.161354 233.798692 
L 282.240823 226.5701 
L 282.320291 236.279658 
L 282.399759 235.668481 
L 282.479228 234.341223 
L 282.558696 235.623474 
L 282.638164 234.164409 
L 282.797101 236.810333 
L 282.876569 236.062631 
L 282.956037 237.06716 
L 283.114974 235.410213 
L 283.194442 236.84414 
L 283.273911 236.006195 
L 283.353379 236.657683 
L 283.432847 235.981161 
L 283.512316 236.181259 
L 283.591784 232.328997 
L 283.750721 236.780243 
L 283.830189 235.130698 
L 283.989125 236.534923 
L 284.068594 236.31664 
L 284.148062 235.572256 
L 284.22753 230.360837 
L 284.306999 237.02791 
L 284.386467 236.477219 
L 284.545404 230.952879 
L 284.624872 235.440441 
L 284.70434 233.72851 
L 284.783809 237.173683 
L 284.863277 233.797181 
L 285.022214 236.939581 
L 285.18115 233.79302 
L 285.340087 236.539139 
L 285.419555 235.396093 
L 285.499023 236.421452 
L 285.578492 233.913065 
L 285.65796 226.659065 
L 285.737428 236.375884 
L 285.816897 235.732125 
L 285.896365 234.471802 
L 285.975833 235.722358 
L 286.055302 234.292033 
L 286.214238 236.878528 
L 286.293707 236.144641 
L 286.373175 237.145745 
L 286.532111 235.510333 
L 286.61158 236.922558 
L 286.691048 236.071817 
L 286.770516 236.736492 
L 286.849985 236.078421 
L 286.929453 236.263973 
L 287.008921 232.444666 
L 287.167858 236.843396 
L 287.247326 235.263098 
L 287.406263 236.608874 
L 287.485731 236.387241 
L 287.5652 235.650498 
L 287.644668 230.508603 
L 287.724136 237.094128 
L 287.803604 236.559837 
L 287.962541 231.056114 
L 288.042009 235.510047 
L 288.121478 233.841632 
L 288.200946 237.239908 
L 288.280414 233.968197 
L 288.439351 237.011597 
L 288.598288 233.895633 
L 288.757224 236.610965 
L 288.836693 235.477237 
L 288.916161 236.500553 
L 288.995629 234.023299 
L 289.075097 226.745463 
L 289.154566 236.465059 
L 289.234034 235.789823 
L 289.313502 234.600215 
L 289.392971 235.810824 
L 289.472439 234.416102 
L 289.631376 236.943876 
L 289.710844 236.225193 
L 289.790312 237.221223 
L 289.949249 235.606941 
L 290.028717 236.996658 
L 290.108186 236.132775 
L 290.187654 236.811932 
L 290.267122 236.170599 
L 290.34659 236.342575 
L 290.426059 232.558362 
L 290.584995 236.903529 
L 290.664464 235.38864 
L 290.8234 236.6804 
L 290.902869 236.453614 
L 290.982337 235.726073 
L 291.061805 230.654571 
L 291.141274 237.15685 
L 291.220742 236.638271 
L 291.379679 231.154876 
L 291.459147 235.577084 
L 291.538615 233.949477 
L 291.618083 237.302615 
L 291.697552 234.138563 
L 291.856488 237.079407 
L 292.015425 233.994106 
L 292.174362 236.681061 
L 292.25383 235.55461 
L 292.333298 236.577252 
L 292.412767 234.12996 
L 292.492235 226.82984 
L 292.571703 236.547882 
L 292.651172 235.842596 
L 292.73064 234.725917 
L 292.810108 235.891546 
L 292.889576 234.537349 
L 293.048513 237.006278 
L 293.127981 236.304129 
L 293.20745 237.29327 
L 293.366386 235.700127 
L 293.445855 237.066974 
L 293.525323 236.189741 
L 293.604791 236.884167 
L 293.68426 236.258127 
L 293.763728 236.417647 
L 293.843196 232.670261 
L 294.002133 236.961194 
L 294.081601 235.505898 
L 294.240538 236.749181 
L 294.320006 236.516211 
L 294.399474 235.79919 
L 294.478943 230.798838 
L 294.558411 237.216637 
L 294.637879 236.713243 
L 294.796816 231.249532 
L 294.876284 235.643032 
L 294.955753 234.052672 
L 295.035221 237.361846 
L 295.114689 234.30807 
L 295.273626 237.143047 
L 295.432562 234.088828 
L 295.591499 236.749289 
L 295.670967 235.629839 
L 295.750436 236.651632 
L 295.829904 234.233256 
L 295.909372 226.912718 
L 295.988841 236.6252 
L 296.068309 235.892149 
L 296.147777 234.84855 
L 296.227246 235.966502 
L 296.306714 234.655997 
L 296.465651 237.065975 
L 296.545119 236.381167 
L 296.624587 237.361844 
L 296.704055 236.903299 
L 296.783524 235.789917 
L 296.862992 237.133912 
L 296.94246 236.24339 
L 297.021929 236.953392 
L 297.101397 236.341464 
L 297.180865 236.489601 
L 297.260334 232.780562 
L 297.41927 237.016895 
L 297.498739 235.613993 
L 297.657675 236.815003 
L 297.737144 236.575642 
L 297.816612 235.870047 
L 297.89608 230.941432 
L 297.975548 237.273929 
L 298.055017 236.785179 
L 298.213953 231.340392 
L 298.293422 235.708976 
L 298.37289 234.151739 
L 298.452358 237.417692 
L 298.531827 234.47626 
L 298.690763 237.202618 
L 298.8497 234.180008 
L 299.008637 236.815533 
L 299.088105 235.703617 
L 299.167573 236.723627 
L 299.247041 234.332866 
L 299.32651 226.994546 
L 299.405978 236.697908 
L 299.485446 235.94065 
L 299.564915 234.967867 
L 299.644383 236.036956 
L 299.723851 234.772075 
L 299.882788 237.123508 
L 299.962256 236.456038 
L 300.041725 237.427045 
L 300.121193 236.983588 
L 300.200661 235.876321 
L 300.28013 237.197771 
L 300.359598 236.294568 
L 300.439066 237.019839 
L 300.518534 236.421042 
L 300.598003 236.558702 
L 300.677471 232.889519 
L 300.756939 237.087014 
L 300.836408 237.071042 
L 300.915876 235.713007 
L 301.074813 236.877804 
L 301.154281 236.632499 
L 301.233749 235.938812 
L 301.313218 231.081848 
L 301.392686 237.328999 
L 301.472154 236.854191 
L 301.631091 231.427749 
L 301.710559 235.77551 
L 301.790027 234.247106 
L 301.869496 237.470291 
L 301.948964 234.64155 
L 302.107901 237.258366 
L 302.266837 234.267702 
L 302.425774 236.879732 
L 302.505242 235.775572 
L 302.584711 236.793039 
L 302.664179 234.428101 
L 302.743647 227.075654 
L 302.823116 236.766834 
L 302.902584 235.990366 
L 302.982052 235.083638 
L 303.06152 236.103674 
L 303.140989 234.88557 
L 303.299925 237.179566 
L 303.379394 236.528517 
L 303.458862 237.488982 
L 303.53833 237.062077 
L 303.617799 235.959327 
L 303.697267 237.258774 
L 303.776735 236.344322 
L 303.856204 237.083748 
L 303.935672 236.49724 
L 304.01514 236.625116 
L 304.094609 232.997364 
L 304.174077 237.160107 
L 304.253545 237.123899 
L 304.333013 235.803916 
L 304.49195 236.937656 
L 304.571418 236.687248 
L 304.650887 236.00561 
L 304.730355 231.218894 
L 304.809823 237.381945 
L 304.889292 236.920164 
L 305.048228 231.511968 
L 305.127697 235.842685 
L 305.207165 234.339105 
L 305.286633 237.519819 
L 305.366102 234.801118 
L 305.525038 237.310685 
L 305.683975 234.352008 
L 305.842911 236.941873 
L 305.92238 235.84492 
L 306.001848 236.859663 
L 306.081316 234.518451 
L 306.160785 227.156219 
L 306.240253 236.832617 
L 306.319721 236.042973 
L 306.39919 235.195682 
L 306.478658 236.167164 
L 306.558126 234.996432 
L 306.717063 237.234758 
L 306.796531 236.598436 
L 306.875999 237.54775 
L 306.955468 237.138242 
L 307.034936 236.038907 
L 307.114404 237.317083 
L 307.193873 236.393748 
L 307.273341 237.145326 
L 307.352809 236.57037 
L 307.432278 236.688942 
L 307.511746 233.104145 
L 307.591214 237.227821 
L 307.670683 237.175564 
L 307.750151 235.888144 
L 307.909087 236.994729 
L 307.988556 236.740203 
L 308.068024 236.070521 
L 308.147492 231.351067 
L 308.226961 237.432738 
L 308.306429 236.982968 
L 308.465366 231.59353 
L 308.544834 235.910094 
L 308.624302 234.427983 
L 308.703771 237.566491 
L 308.783239 234.951797 
L 308.942176 237.360045 
L 309.101112 234.433177 
L 309.260049 237.001979 
L 309.339517 235.911193 
L 309.418985 236.923439 
L 309.498454 234.60402 
L 309.577922 227.23628 
L 309.65739 236.895684 
L 309.736859 236.098962 
L 309.816327 235.303941 
L 309.895795 236.227853 
L 309.975264 235.104549 
L 310.1342 237.289417 
L 310.213669 236.665717 
L 310.293137 237.603466 
L 310.372605 237.211507 
L 310.452073 236.115038 
L 310.531542 237.372804 
L 310.61101 236.443706 
L 310.690478 237.204708 
L 310.769947 236.640677 
L 310.849415 236.750244 
L 310.928883 233.209625 
L 311.008352 237.290973 
L 311.08782 237.226004 
L 311.167288 235.967071 
L 311.326225 237.04924 
L 311.405693 236.791587 
L 311.485162 236.133587 
L 311.56463 231.477185 
L 311.644098 237.481329 
L 311.723566 237.042646 
L 311.882503 231.672974 
L 311.961971 235.977104 
L 312.04144 234.513909 
L 312.120908 237.61056 
L 312.200376 235.091301 
L 312.359313 237.406887 
L 312.51825 234.511549 
L 312.677186 237.060071 
L 312.756655 235.974405 
L 312.836123 236.984491 
L 312.915591 234.685435 
L 312.995059 227.315818 
L 313.074528 236.95631 
L 313.153996 236.157718 
L 313.233464 235.408483 
L 313.312933 236.286144 
L 313.392401 235.209793 
L 313.551338 237.34356 
L 313.630806 236.730377 
L 313.710274 237.656286 
L 313.789743 237.281357 
L 313.869211 236.187748 
L 313.948679 237.426006 
L 314.028148 236.494561 
L 314.107616 237.261962 
L 314.187084 236.708356 
L 314.266552 236.809085 
L 314.346021 233.313326 
L 314.425489 237.350478 
L 314.504957 237.275131 
L 314.584426 236.041757 
L 314.743362 237.101409 
L 314.822831 236.841575 
L 314.902299 236.194829 
L 314.981767 231.596769 
L 315.061236 237.527721 
L 315.140704 237.099439 
L 315.299641 231.750809 
L 315.379109 236.043111 
L 315.458577 234.597 
L 315.538045 237.652292 
L 315.617514 235.218855 
L 315.77645 237.45157 
L 315.935387 234.587445 
L 316.094324 237.116154 
L 316.173792 236.034822 
L 316.25326 237.043055 
L 316.332729 234.763459 
L 316.412197 227.394835 
L 316.491665 237.014686 
L 316.571134 236.21812 
L 316.650602 235.509436 
L 316.73007 236.342366 
L 316.809538 235.312073 
L 316.968475 237.39698 
L 317.047943 236.792507 
L 317.127412 237.706404 
L 317.20688 237.347431 
L 317.286348 236.257122 
L 317.365817 237.476762 
L 317.445285 236.546132 
L 317.524753 237.317105 
L 317.604222 236.773563 
L 317.68369 236.865541 
L 317.763158 233.414704 
L 317.842627 237.407047 
L 317.922095 237.322863 
L 318.001563 236.112919 
L 318.1605 237.15143 
L 318.319436 236.254269 
L 318.398905 231.709985 
L 318.478373 237.571981 
L 318.557841 237.153682 
L 318.716778 231.827425 
L 318.796246 236.107671 
L 318.875715 234.677338 
L 318.955183 237.691941 
L 319.034651 235.334984 
L 319.193588 237.494363 
L 319.352524 234.66111 
L 319.511461 237.170236 
L 319.590929 236.092747 
L 319.670398 237.099389 
L 319.749866 234.838706 
L 319.829334 227.47338 
L 319.908803 237.070962 
L 319.988271 236.279087 
L 320.067739 235.606938 
L 320.147208 236.396751 
L 320.226676 235.411364 
L 320.385613 237.449387 
L 320.465081 236.852221 
L 320.544549 237.754021 
L 320.624017 237.409582 
L 320.703486 236.323283 
L 320.782954 237.525167 
L 320.862422 236.597892 
L 320.941891 237.370148 
L 321.021359 236.836436 
L 321.100827 236.919711 
L 321.180296 233.513309 
L 321.259764 237.461133 
L 321.339232 237.369151 
L 321.418701 236.181031 
L 321.577637 237.199461 
L 321.736574 236.311939 
L 321.816042 231.817374 
L 321.89551 237.614229 
L 321.974979 237.205689 
L 322.133915 231.903082 
L 322.213384 236.17051 
L 322.292852 234.754991 
L 322.37232 237.729726 
L 322.451789 235.440928 
L 322.610725 237.535472 
L 322.769662 234.732718 
L 322.928599 237.222351 
L 323.008067 236.148434 
L 323.087535 237.153711 
L 323.167003 234.911574 
L 323.246472 227.551539 
L 323.32594 237.125262 
L 323.405408 236.339808 
L 323.484877 235.701124 
L 323.564345 236.449437 
L 323.643813 235.507705 
L 323.80275 237.500511 
L 323.882218 236.909639 
L 323.961687 237.799332 
L 324.041155 237.467861 
L 324.120623 236.386373 
L 324.200092 237.571345 
L 324.27956 236.649228 
L 324.359028 237.421123 
L 324.438496 236.897099 
L 324.517965 236.971703 
L 324.597433 233.608868 
L 324.676901 237.513004 
L 324.75637 237.413977 
L 324.835838 236.24642 
L 324.994775 237.24563 
L 325.153711 236.367887 
L 325.23318 231.919602 
L 325.312648 237.654603 
L 325.392116 237.25571 
L 325.551053 231.977918 
L 325.630521 236.231482 
L 325.709989 234.830026 
L 325.789458 237.765828 
L 325.868926 235.538139 
L 326.027863 237.57506 
L 326.186799 234.802386 
L 326.345736 237.27256 
L 326.425204 236.202081 
L 326.504673 237.206196 
L 326.584141 234.982289 
L 326.663609 227.629411 
L 326.743078 237.177698 
L 326.822546 236.399755 
L 326.902014 235.792137 
L 326.981482 236.500505 
L 327.060951 235.601184 
L 327.219887 237.550139 
L 327.299356 236.96487 
L 327.378824 237.842513 
L 327.458292 237.522463 
L 327.537761 236.446524 
L 327.617229 237.615438 
L 327.696697 236.69963 
L 327.776166 237.470094 
L 327.855634 236.955668 
L 327.935102 237.021626 
L 328.014571 233.701281 
L 328.094039 237.562821 
L 328.173507 237.457349 
L 328.252975 236.309339 
L 328.411912 237.290046 
L 328.570849 236.422169 
L 328.650317 232.017313 
L 328.729785 237.693245 
L 328.809254 237.303921 
L 328.96819 232.051994 
L 329.047659 236.29053 
L 329.127127 234.902515 
L 329.206595 237.800396 
L 329.286064 235.627994 
L 329.445 237.61326 
L 329.603937 234.870197 
L 329.762873 237.320943 
L 329.842342 236.253842 
L 329.92181 237.256971 
L 330.001278 235.050967 
L 330.080747 227.707097 
L 330.160215 237.228374 
L 330.239683 236.458616 
L 330.319152 235.880128 
L 330.39862 236.550009 
L 330.478088 235.691927 
L 330.637025 237.598128 
L 330.716493 237.018014 
L 330.795961 237.883724 
L 330.87543 237.57367 
L 330.954898 236.503862 
L 331.034366 237.657592 
L 331.113835 236.748755 
L 331.193303 237.517145 
L 331.272771 237.012247 
L 331.35224 237.069586 
L 331.431708 233.790578 
L 331.511176 237.610695 
L 331.590645 237.499289 
L 331.670113 236.369994 
L 331.82905 237.332804 
L 331.987986 236.474848 
L 332.067454 232.111073 
L 332.146923 237.730289 
L 332.226391 237.350446 
L 332.385328 232.125321 
L 332.464796 236.347653 
L 332.544264 234.972536 
L 332.623733 237.833552 
L 332.703201 235.711681 
L 332.862138 237.650184 
L 333.021074 234.936218 
L 333.180011 237.36759 
L 333.259479 236.303842 
L 333.338947 237.306133 
L 333.418416 235.117671 
L 333.497884 227.784699 
L 333.577352 237.277386 
L 333.656821 236.51622 
L 333.736289 235.965262 
L 333.815757 236.597994 
L 333.895226 235.780073 
L 334.054162 237.644399 
L 334.133631 237.069163 
L 334.213099 237.923109 
L 334.292567 237.621788 
L 334.372036 236.558497 
L 334.451504 237.697947 
L 334.530972 236.79641 
L 334.61044 237.562369 
L 334.689909 237.066937 
L 334.769377 237.115681 
L 334.848845 233.876869 
L 334.928314 237.656709 
L 335.007782 237.539831 
L 335.08725 236.428569 
L 335.246187 237.373993 
L 335.405124 236.525986 
L 335.484592 232.201355 
L 335.56406 237.765857 
L 335.643529 237.395377 
L 335.802465 232.197888 
L 335.881933 236.402884 
L 335.961402 235.040173 
L 336.04087 237.8654 
L 336.120338 235.790179 
L 336.279275 237.685928 
L 336.438212 235.000504 
L 336.597148 237.41259 
L 336.676617 236.352184 
L 336.756085 237.353757 
L 336.835553 235.18244 
L 336.915022 227.862322 
L 336.99449 237.324823 
L 337.073958 236.572491 
L 337.153426 236.047703 
L 337.232895 236.644503 
L 337.312363 235.865768 
L 337.4713 237.688918 
L 337.550768 237.118398 
L 337.630236 237.9608 
L 337.709705 237.667118 
L 337.789173 236.610528 
L 337.868641 237.736634 
L 337.94811 236.842517 
L 338.027578 237.605862 
L 338.107046 237.11983 
L 338.186515 237.160002 
L 338.265983 233.960308 
L 338.345451 237.700939 
L 338.424919 237.579015 
L 338.504388 236.485228 
L 338.663324 237.413698 
L 338.822261 236.575644 
L 338.901729 232.28855 
L 338.981198 237.800059 
L 339.060666 237.438786 
L 339.219603 232.269676 
L 339.299071 236.45628 
L 339.378539 235.105513 
L 339.458008 237.896034 
L 339.537476 235.864274 
L 339.696412 237.720571 
L 339.855349 235.06311 
L 340.014286 237.456031 
L 340.093754 236.398953 
L 340.173222 237.399903 
L 340.252691 235.245306 
L 340.332159 227.940082 
L 340.411627 237.370766 
L 340.491096 236.627413 
L 340.570564 236.127615 
L 340.650032 236.689582 
L 340.729501 235.949153 
L 340.888437 237.731692 
L 340.967905 237.165798 
L 341.047374 237.996919 
L 341.126842 237.709932 
L 341.20631 236.660037 
L 341.285779 237.773771 
L 341.365247 236.88707 
L 341.444715 237.647711 
L 341.524184 237.171009 
L 341.603652 237.202629 
L 341.68312 234.041071 
L 341.762589 237.743457 
L 341.842057 237.616885 
L 341.921525 236.54012 
L 342.080462 237.451997 
L 342.239398 236.623881 
L 342.318867 232.372984 
L 342.398335 237.832991 
L 342.477803 237.480737 
L 342.63674 232.340666 
L 342.716208 236.507909 
L 342.795677 235.168643 
L 342.875145 237.925533 
L 342.954613 235.934598 
L 343.11355 237.754182 
L 343.272487 235.12409 
L 343.431423 237.497994 
L 343.510891 236.444221 
L 343.59036 237.444624 
L 343.669828 235.3063 
L 343.749296 228.018124 
L 343.828765 237.415289 
L 343.908233 236.681008 
L 343.987701 236.205156 
L 344.06717 236.733277 
L 344.146638 236.030346 
L 344.305575 237.772752 
L 344.385043 237.211437 
L 344.464511 238.031578 
L 344.543979 237.750467 
L 344.623448 236.707091 
L 344.702916 237.809464 
L 344.782384 236.930105 
L 344.861853 237.687995 
L 344.941321 237.220551 
L 345.020789 237.24364 
L 345.100258 234.119338 
L 345.179726 237.784332 
L 345.259194 237.653485 
L 345.338663 236.593379 
L 345.497599 237.488969 
L 345.656536 236.670749 
L 345.736004 232.454927 
L 345.815472 237.864742 
L 345.894941 237.521289 
L 346.053877 232.410845 
L 346.133346 236.557847 
L 346.212814 235.229652 
L 346.292282 237.953973 
L 346.371751 236.001653 
L 346.530687 237.786818 
L 346.689624 235.183502 
L 346.848561 237.538556 
L 346.928029 236.488048 
L 347.007497 237.487966 
L 347.086965 235.365456 
L 347.166434 228.096627 
L 347.245902 237.458455 
L 347.32537 236.733326 
L 347.404839 236.280471 
L 347.484307 236.775635 
L 347.563775 236.109438 
L 347.722712 237.812149 
L 347.80218 237.255387 
L 347.881649 238.06488 
L 347.961117 237.788925 
L 348.040585 236.751731 
L 348.120054 237.843809 
L 348.199522 236.971676 
L 348.27899 237.726785 
L 348.358458 237.268524 
L 348.437927 237.283102 
L 348.517395 234.195288 
L 348.596863 237.823633 
L 348.676332 237.68886 
L 348.7558 236.645124 
L 348.914737 237.524686 
L 349.073673 236.716296 
L 349.153142 232.534607 
L 349.23261 237.895391 
L 349.312078 237.5605 
L 349.471015 232.480206 
L 349.550483 236.606173 
L 349.629951 235.288621 
L 349.70942 237.981418 
L 349.788888 236.065845 
L 349.947825 237.818528 
L 350.106761 235.241406 
L 350.265698 237.577784 
L 350.345166 236.530489 
L 350.424635 237.529973 
L 350.504103 235.422803 
L 350.583571 228.175824 
L 350.66304 237.50032 
L 350.742508 236.784439 
L 350.821976 236.353692 
L 350.901444 236.816703 
L 350.980913 236.186473 
L 351.139849 237.849943 
L 351.219318 237.297716 
L 351.298786 238.096923 
L 351.378254 237.825474 
L 351.457723 236.793974 
L 351.537191 237.876893 
L 351.616659 237.011851 
L 351.696128 237.76414 
L 351.855064 237.321077 
L 351.934533 234.269097 
L 352.014001 237.861424 
L 352.093469 237.723055 
L 352.172937 236.695465 
L 352.331874 237.55922 
L 352.490811 236.760566 
L 352.570279 232.612222 
L 352.649747 237.925006 
L 352.729216 237.598428 
L 352.888152 232.548748 
L 352.967621 236.652962 
L 353.047089 235.345632 
L 353.126557 238.00793 
L 353.206026 236.127505 
L 353.364962 237.849356 
L 353.44443 237.105337 
L 353.523899 235.297863 
L 353.682835 237.615741 
L 353.762304 236.571585 
L 353.841772 237.570686 
L 353.92124 235.47837 
L 354.000709 228.256026 
L 354.080177 237.540927 
L 354.159645 236.83444 
L 354.239114 236.424933 
L 354.318582 236.856522 
L 354.39805 236.26143 
L 354.556987 237.886201 
L 354.636455 237.338493 
L 354.715923 238.127798 
L 354.795392 237.860253 
L 354.87486 236.833795 
L 354.954328 237.908797 
L 355.033797 237.050698 
L 355.113265 237.800109 
L 355.272202 237.357623 
L 355.35167 234.340931 
L 355.431138 237.897769 
L 355.510607 237.756111 
L 355.590075 236.744497 
L 355.749012 237.59264 
L 355.907948 236.803595 
L 355.987416 232.687947 
L 356.066885 237.953654 
L 356.146353 237.635131 
L 356.30529 232.616477 
L 356.384758 236.69829 
L 356.464226 235.400757 
L 356.543695 238.033561 
L 356.623163 236.186906 
L 356.7821 237.879338 
L 356.861568 237.153976 
L 356.941036 235.352942 
L 357.099973 237.652485 
L 357.179441 236.611373 
L 357.258909 237.610144 
L 357.338378 235.532174 
L 357.417846 228.33765 
L 357.497314 237.580314 
L 357.576783 236.88344 
L 357.656251 236.494285 
L 357.735719 236.895133 
L 357.815188 236.334202 
L 357.974124 237.920991 
L 358.053593 237.377782 
L 358.133061 238.157591 
L 358.212529 237.89337 
L 358.291998 236.871116 
L 358.371466 237.939595 
L 358.450934 237.088285 
L 358.530402 237.834733 
L 358.689339 237.392788 
L 358.768807 234.410955 
L 358.848276 237.932723 
L 358.927744 237.788067 
L 359.007212 236.792308 
L 359.166149 237.625012 
L 359.325086 236.845414 
L 359.404554 232.761939 
L 359.484022 237.981393 
L 359.563491 237.670665 
L 359.722427 232.683402 
L 359.801895 236.742221 
L 359.881364 235.454063 
L 359.960832 238.058361 
L 360.0403 236.244275 
L 360.199237 237.908507 
L 360.278705 237.200887 
L 360.358174 235.406713 
L 360.51711 237.688068 
L 360.596579 236.649876 
L 360.676047 237.648382 
L 360.755515 235.584223 
L 360.834984 228.421262 
L 360.914452 237.618503 
L 360.99392 236.931578 
L 361.073388 236.56181 
L 361.152857 236.932572 
L 361.232325 236.404561 
L 361.391262 237.954381 
L 361.47073 237.415646 
L 361.550198 238.186384 
L 361.629667 237.924908 
L 361.709135 236.905784 
L 361.788603 237.969358 
L 361.868072 237.124674 
L 361.94754 237.868045 
L 362.106477 237.426619 
L 362.185945 234.479329 
L 362.265413 237.966338 
L 362.344881 237.818957 
L 362.42435 236.838979 
L 362.583286 237.656402 
L 362.742223 236.886048 
L 362.821691 232.834343 
L 362.90116 238.008276 
L 362.980628 237.705089 
L 363.139565 232.749543 
L 363.219033 236.78481 
L 363.298501 235.505606 
L 363.37797 238.082368 
L 363.457438 236.299808 
L 363.616374 237.936892 
L 363.695843 237.246212 
L 363.775311 235.459253 
L 363.934248 237.722537 
L 364.013716 236.687106 
L 364.093184 237.685434 
L 364.172653 235.634498 
L 364.252121 228.507648 
L 364.331589 237.655509 
L 364.411058 236.97902 
L 364.490526 236.627532 
L 364.569994 236.968873 
L 364.649463 236.47211 
L 364.808399 237.986433 
L 364.887867 237.452143 
L 364.967336 238.214258 
L 365.046804 237.954919 
L 365.126272 236.937528 
L 365.205741 237.99816 
L 365.285209 237.159926 
L 365.364677 237.900069 
L 365.523614 237.459155 
L 365.603082 234.546213 
L 365.682551 237.998659 
L 365.762019 237.848809 
L 365.841487 236.884582 
L 366.000424 237.686874 
L 366.15936 236.925511 
L 366.238829 232.905297 
L 366.318297 238.034353 
L 366.397765 237.73846 
L 366.556702 232.814924 
L 366.63617 236.826093 
L 366.715639 235.555428 
L 366.795107 238.105615 
L 366.874575 236.353677 
L 367.033512 237.964519 
L 367.11298 237.290086 
L 367.192449 235.510649 
L 367.351385 237.755934 
L 367.430853 236.723057 
L 367.510322 237.72133 
L 367.58979 235.682954 
L 367.669258 228.597913 
L 367.748727 237.691326 
L 367.828195 237.025974 
L 367.907663 236.691416 
L 367.987132 237.004065 
L 368.0666 236.536205 
L 368.225537 238.017204 
L 368.305005 237.487325 
L 368.384473 238.24129 
L 368.463942 237.983425 
L 368.54341 236.965906 
L 368.622878 238.026076 
L 368.702346 237.194095 
L 368.781815 237.930826 
L 368.940751 237.490434 
L 369.02022 234.611773 
L 369.099688 238.029718 
L 369.179156 237.877643 
L 369.258625 236.929187 
L 369.417561 237.71649 
L 369.576498 236.96381 
L 369.655966 232.974939 
L 369.735435 238.059668 
L 369.814903 237.770831 
L 369.973839 232.879581 
L 370.053308 236.866083 
L 370.132776 235.603557 
L 370.212244 238.128121 
L 370.291713 236.40603 
L 370.450649 237.991411 
L 370.530118 237.332646 
L 370.609586 235.560995 
L 370.768523 237.788297 
L 370.847991 236.7577 
L 370.927459 237.756096 
L 371.006928 235.729498 
L 371.086396 228.693653 
L 371.165864 237.725929 
L 371.245332 237.072695 
L 371.324801 236.753341 
L 371.404269 237.038174 
L 371.483737 236.595835 
L 371.642674 238.04674 
L 371.722142 237.521229 
L 371.801611 238.267559 
L 371.881079 238.010412 
L 371.960547 236.990209 
L 372.040016 238.053191 
L 372.119484 237.227232 
L 372.198952 237.960335 
L 372.357889 237.520495 
L 372.437357 234.676182 
L 372.516825 238.059538 
L 372.596294 237.905468 
L 372.675762 236.972863 
L 372.834699 237.745311 
L 372.993635 237.00094 
L 373.073104 233.043404 
L 373.152572 238.084259 
L 373.23204 237.802258 
L 373.390977 232.943564 
L 373.470445 236.904748 
L 373.549914 235.649994 
L 373.629382 238.149886 
L 373.70885 236.457004 
L 373.867787 238.017585 
L 373.947255 237.374044 
L 374.026723 235.610404 
L 374.18566 237.819659 
L 374.265128 236.790967 
L 374.344597 237.789753 
L 374.424065 235.773964 
L 374.503533 228.797248 
L 374.583002 237.759256 
L 374.66247 237.119484 
L 374.741938 236.813057 
L 374.821407 237.071225 
L 374.900875 236.649414 
L 375.059811 238.075078 
L 375.13928 237.553874 
L 375.218748 238.293148 
L 375.298216 238.035815 
L 375.377685 237.009323 
L 375.457153 238.0796 
L 375.536621 237.259383 
L 375.61609 237.988619 
L 375.775026 237.549394 
L 375.854495 234.739628 
L 375.933963 238.08812 
L 376.013431 237.932278 
L 376.0929 237.015681 
L 376.251836 237.773395 
L 376.331304 237.56331 
L 376.410773 237.036885 
L 376.490241 233.110837 
L 376.569709 238.108159 
L 376.649178 237.832791 
L 376.808114 233.006946 
L 376.887583 236.941984 
L 376.967051 235.694704 
L 377.046519 238.170882 
L 377.125988 236.506721 
L 377.284924 238.043055 
L 377.364393 237.414462 
L 377.443861 235.659004 
L 377.602797 237.850046 
L 377.682266 236.822736 
L 377.761734 237.822314 
L 377.841202 235.816081 
L 377.920671 228.912359 
L 378.000139 237.791187 
L 378.079607 237.166644 
L 378.159076 236.870128 
L 378.238544 237.103242 
L 378.318012 236.694427 
L 378.476949 238.102236 
L 378.556417 237.585248 
L 378.635886 238.318142 
L 378.715354 238.059511 
L 378.794822 237.021531 
L 378.87429 238.105417 
L 378.953759 237.290586 
L 379.033227 238.015711 
L 379.192164 237.577217 
L 379.271632 234.802322 
L 379.3511 238.115439 
L 379.430569 237.958048 
L 379.510037 237.057716 
L 379.668974 237.800797 
L 379.748442 237.593901 
L 379.82791 237.071612 
L 379.907379 233.177391 
L 379.986847 238.131392 
L 380.066315 237.862474 
L 380.225252 233.069839 
L 380.30472 236.977561 
L 380.384188 235.737595 
L 380.463657 238.191028 
L 380.543125 236.555291 
L 380.543125 236.555291 
" style="fill:none;stroke:#000000;stroke-linecap:round;stroke-width:0.5;">
         </path>
        </g>
        <g id="line2d_13">
         <path clip-path="url(#p31b5e10f6c)" d="M 49.159452 41.386058 
L 52.575778 43.006744 
L 55.992105 45.318076 
L 59.408431 48.845314 
L 62.824758 54.238121 
L 66.241084 61.813217 
L 69.657411 71.294973 
L 73.073737 81.978017 
L 76.490064 93.343761 
L 79.90639 105.156525 
L 83.322717 117.194283 
L 86.739043 129.03426 
L 90.15537 140.137121 
L 93.571696 150.231381 
L 96.988023 159.257012 
L 100.404349 167.238468 
L 103.820676 174.303683 
L 107.237003 180.693767 
L 110.653329 186.393096 
L 114.069656 191.420547 
L 117.485982 195.960563 
L 120.902309 199.729975 
L 124.318635 202.900591 
L 127.734962 205.758829 
L 131.151288 208.504926 
L 134.567615 210.714371 
L 137.983941 212.530699 
L 141.400268 214.127078 
L 144.816594 215.533566 
L 148.232921 216.804998 
L 151.649247 218.020479 
L 155.065574 219.168589 
L 158.481901 220.263974 
L 161.898227 221.281786 
L 165.314554 222.193 
L 168.73088 223.161366 
L 172.147207 223.932051 
L 175.563533 224.668982 
L 178.97986 225.336495 
L 182.396186 225.935474 
L 185.812513 226.484023 
L 189.228839 226.915896 
L 192.645166 227.223288 
L 196.061492 227.538923 
L 199.477819 227.805753 
L 202.894145 228.09324 
L 206.310472 228.489474 
L 209.726798 229.003326 
L 213.143125 229.54918 
L 216.559452 230.064416 
L 219.975778 230.533749 
L 223.392105 230.955155 
L 226.808431 231.330605 
L 230.224758 231.664385 
L 233.641084 231.961345 
L 237.057411 232.231205 
L 240.473737 232.486463 
L 243.890064 232.726756 
L 247.30639 232.948631 
L 250.722717 233.153472 
L 254.139043 233.344172 
L 257.55537 233.523412 
L 260.971696 233.693276 
L 264.388023 233.855247 
L 267.804349 234.010316 
L 271.220676 234.15919 
L 274.637003 234.302397 
L 278.053329 234.440189 
L 281.469656 234.572453 
L 284.885982 234.698866 
L 288.302309 234.819258 
L 291.718635 234.933836 
L 295.134962 235.043111 
L 298.551288 235.147653 
L 301.967615 235.24788 
L 305.383941 235.344003 
L 308.800268 235.436117 
L 312.216594 235.524334 
L 315.632921 235.608851 
L 319.049247 235.689931 
L 322.465574 235.767853 
L 325.881901 235.842884 
L 329.298227 235.915266 
L 332.714554 235.985208 
L 336.13088 236.052896 
L 339.547207 236.118491 
L 342.963533 236.182139 
L 346.37986 236.24397 
L 349.796186 236.304107 
L 353.212513 236.362662 
L 356.628839 236.419744 
L 360.045166 236.475462 
L 363.461492 236.529924 
L 366.877819 236.583245 
L 370.294145 236.635548 
L 373.710472 236.686965 
L 377.126798 236.737648 
L 380.543125 236.787755 
" style="fill:none;stroke:#ff0000;stroke-linecap:round;stroke-width:2;">
         </path>
        </g>
        <g id="line2d_14">
         <path clip-path="url(#p31b5e10f6c)" d="M 49.159452 40.841677 
L 52.575778 42.490081 
L 55.992105 44.906964 
L 59.408431 48.629263 
L 62.824758 54.33878 
L 66.241084 62.261728 
L 69.657411 71.997844 
L 73.073737 82.699422 
L 76.490064 93.762438 
L 79.90639 104.992732 
L 83.322717 116.258914 
L 86.739043 127.195725 
L 90.15537 137.359087 
L 93.571696 146.551925 
L 96.988023 154.742646 
L 100.404349 161.876533 
L 103.820676 168.088885 
L 107.237003 173.673719 
L 110.653329 178.561161 
L 114.069656 182.799717 
L 117.485982 186.62307 
L 120.902309 189.709772 
L 124.318635 192.262799 
L 127.734962 194.650459 
L 131.151288 197.071458 
L 134.567615 198.935087 
L 137.983941 200.472079 
L 141.400268 201.923412 
L 144.816594 203.237815 
L 148.232921 204.370495 
L 151.649247 205.436801 
L 155.065574 206.420222 
L 158.481901 207.386544 
L 161.898227 208.230101 
L 165.314554 208.824201 
L 168.73088 209.578491 
L 172.147207 210.060782 
L 175.563533 210.514705 
L 178.97986 210.865038 
L 182.396186 211.139489 
L 185.812513 211.435303 
L 189.228839 211.629744 
L 192.645166 211.709514 
L 196.061492 211.948668 
L 199.477819 212.163494 
L 202.894145 212.388525 
L 206.310472 212.636611 
L 209.726798 212.906665 
L 213.143125 213.179791 
L 216.559452 213.446359 
L 219.975778 213.707197 
L 223.392105 213.964059 
L 226.808431 214.216761 
L 230.224758 214.463978 
L 233.641084 214.704352 
L 237.057411 214.936443 
L 240.473737 215.154167 
L 243.890064 215.350443 
L 247.30639 215.525328 
L 250.722717 215.681674 
L 254.139043 215.82206 
L 257.55537 215.947944 
L 260.971696 216.059796 
L 264.388023 216.158 
L 267.804349 216.243893 
L 271.220676 216.319984 
L 274.637003 216.389264 
L 278.053329 216.454354 
L 281.469656 216.517012 
L 284.885982 216.578074 
L 288.302309 216.637726 
L 291.718635 216.695833 
L 295.134962 216.752033 
L 298.551288 216.805695 
L 301.967615 216.855993 
L 305.383941 216.902179 
L 308.800268 216.943913 
L 312.216594 216.981379 
L 315.632921 217.015122 
L 319.049247 217.045779 
L 322.465574 217.073898 
L 325.881901 217.099888 
L 329.298227 217.12403 
L 332.714554 217.146511 
L 336.13088 217.167451 
L 339.547207 217.186923 
L 342.963533 217.204961 
L 346.37986 217.22157 
L 349.796186 217.236722 
L 353.212513 217.250363 
L 356.628839 217.262407 
L 360.045166 217.272733 
L 363.461492 217.281189 
L 366.877819 217.287579 
L 370.294145 217.291669 
L 373.710472 217.293174 
L 377.126798 217.291746 
L 380.543125 217.286943 
" style="fill:none;stroke:#0000ff;stroke-linecap:round;stroke-width:3;">
         </path>
        </g>
        <g id="patch_3">
         <path d="M 45.743125 239.758125 
L 45.743125 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_4">
         <path d="M 380.543125 239.758125 
L 380.543125 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_5">
         <path d="M 45.743125 239.758125 
L 380.543125 239.758125 
" style="fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_6">
         <path d="M 45.743125 22.318125 
L 380.543125 22.318125 
" style="fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="text_14">
         <!-- Decrease of cost over backprop iteration -->
         <g style="fill:#262626;" transform="translate(90.64 16.318125)scale(0.12 -0.12)">
          <defs>
           <path d="M 1259 4147 
L 1259 519 
L 2022 519 
Q 2988 519 3436 956 
Q 3884 1394 3884 2338 
Q 3884 3275 3436 3711 
Q 2988 4147 2022 4147 
L 1259 4147 
z
M 628 4666 
L 1925 4666 
Q 3281 4666 3915 4102 
Q 4550 3538 4550 2338 
Q 4550 1131 3912 565 
Q 3275 0 1925 0 
L 628 0 
L 628 4666 
z
" id="DejaVuSans-44" transform="scale(0.015625)">
           </path>
           <path d="M 3122 3366 
L 3122 2828 
Q 2878 2963 2633 3030 
Q 2388 3097 2138 3097 
Q 1578 3097 1268 2742 
Q 959 2388 959 1747 
Q 959 1106 1268 751 
Q 1578 397 2138 397 
Q 2388 397 2633 464 
Q 2878 531 3122 666 
L 3122 134 
Q 2881 22 2623 -34 
Q 2366 -91 2075 -91 
Q 1284 -91 818 406 
Q 353 903 353 1747 
Q 353 2603 823 3093 
Q 1294 3584 2113 3584 
Q 2378 3584 2631 3529 
Q 2884 3475 3122 3366 
z
" id="DejaVuSans-63" transform="scale(0.015625)">
           </path>
           <path d="M 2834 3397 
L 2834 2853 
Q 2591 2978 2328 3040 
Q 2066 3103 1784 3103 
Q 1356 3103 1142 2972 
Q 928 2841 928 2578 
Q 928 2378 1081 2264 
Q 1234 2150 1697 2047 
L 1894 2003 
Q 2506 1872 2764 1633 
Q 3022 1394 3022 966 
Q 3022 478 2636 193 
Q 2250 -91 1575 -91 
Q 1294 -91 989 -36 
Q 684 19 347 128 
L 347 722 
Q 666 556 975 473 
Q 1284 391 1588 391 
Q 1994 391 2212 530 
Q 2431 669 2431 922 
Q 2431 1156 2273 1281 
Q 2116 1406 1581 1522 
L 1381 1569 
Q 847 1681 609 1914 
Q 372 2147 372 2553 
Q 372 3047 722 3315 
Q 1072 3584 1716 3584 
Q 2034 3584 2315 3537 
Q 2597 3491 2834 3397 
z
" id="DejaVuSans-73" transform="scale(0.015625)">
           </path>
           <path id="DejaVuSans-20" transform="scale(0.015625)">
           </path>
           <path d="M 2375 4863 
L 2375 4384 
L 1825 4384 
Q 1516 4384 1395 4259 
Q 1275 4134 1275 3809 
L 1275 3500 
L 2222 3500 
L 2222 3053 
L 1275 3053 
L 1275 0 
L 697 0 
L 697 3053 
L 147 3053 
L 147 3500 
L 697 3500 
L 697 3744 
Q 697 4328 969 4595 
Q 1241 4863 1831 4863 
L 2375 4863 
z
" id="DejaVuSans-66" transform="scale(0.015625)">
           </path>
           <path d="M 191 3500 
L 800 3500 
L 1894 563 
L 2988 3500 
L 3597 3500 
L 2284 0 
L 1503 0 
L 191 3500 
z
" id="DejaVuSans-76" transform="scale(0.015625)">
           </path>
           <path d="M 3116 1747 
Q 3116 2381 2855 2742 
Q 2594 3103 2138 3103 
Q 1681 3103 1420 2742 
Q 1159 2381 1159 1747 
Q 1159 1113 1420 752 
Q 1681 391 2138 391 
Q 2594 391 2855 752 
Q 3116 1113 3116 1747 
z
M 1159 2969 
Q 1341 3281 1617 3432 
Q 1894 3584 2278 3584 
Q 2916 3584 3314 3078 
Q 3713 2572 3713 1747 
Q 3713 922 3314 415 
Q 2916 -91 2278 -91 
Q 1894 -91 1617 61 
Q 1341 213 1159 525 
L 1159 0 
L 581 0 
L 581 4863 
L 1159 4863 
L 1159 2969 
z
" id="DejaVuSans-62" transform="scale(0.015625)">
           </path>
           <path d="M 581 4863 
L 1159 4863 
L 1159 1991 
L 2875 3500 
L 3609 3500 
L 1753 1863 
L 3688 0 
L 2938 0 
L 1159 1709 
L 1159 0 
L 581 0 
L 581 4863 
z
" id="DejaVuSans-6b" transform="scale(0.015625)">
           </path>
           <path d="M 1159 525 
L 1159 -1331 
L 581 -1331 
L 581 3500 
L 1159 3500 
L 1159 2969 
Q 1341 3281 1617 3432 
Q 1894 3584 2278 3584 
Q 2916 3584 3314 3078 
Q 3713 2572 3713 1747 
Q 3713 922 3314 415 
Q 2916 -91 2278 -91 
Q 1894 -91 1617 61 
Q 1341 213 1159 525 
z
M 3116 1747 
Q 3116 2381 2855 2742 
Q 2594 3103 2138 3103 
Q 1681 3103 1420 2742 
Q 1159 2381 1159 1747 
Q 1159 1113 1420 752 
Q 1681 391 2138 391 
Q 2594 391 2855 752 
Q 3116 1113 3116 1747 
z
" id="DejaVuSans-70" transform="scale(0.015625)">
           </path>
          </defs>
          <use xlink:href="#DejaVuSans-44">
          </use>
          <use x="77.001953" xlink:href="#DejaVuSans-65">
          </use>
          <use x="138.525391" xlink:href="#DejaVuSans-63">
          </use>
          <use x="193.505859" xlink:href="#DejaVuSans-72">
          </use>
          <use x="232.369141" xlink:href="#DejaVuSans-65">
          </use>
          <use x="293.892578" xlink:href="#DejaVuSans-61">
          </use>
          <use x="355.171875" xlink:href="#DejaVuSans-73">
          </use>
          <use x="407.271484" xlink:href="#DejaVuSans-65">
          </use>
          <use x="468.794922" xlink:href="#DejaVuSans-20">
          </use>
          <use x="500.582031" xlink:href="#DejaVuSans-6f">
          </use>
          <use x="561.763672" xlink:href="#DejaVuSans-66">
          </use>
          <use x="596.96875" xlink:href="#DejaVuSans-20">
          </use>
          <use x="628.755859" xlink:href="#DejaVuSans-63">
          </use>
          <use x="683.736328" xlink:href="#DejaVuSans-6f">
          </use>
          <use x="744.917969" xlink:href="#DejaVuSans-73">
          </use>
          <use x="797.017578" xlink:href="#DejaVuSans-74">
          </use>
          <use x="836.226562" xlink:href="#DejaVuSans-20">
          </use>
          <use x="868.013672" xlink:href="#DejaVuSans-6f">
          </use>
          <use x="929.195312" xlink:href="#DejaVuSans-76">
          </use>
          <use x="988.375" xlink:href="#DejaVuSans-65">
          </use>
          <use x="1049.898438" xlink:href="#DejaVuSans-72">
          </use>
          <use x="1091.011719" xlink:href="#DejaVuSans-20">
          </use>
          <use x="1122.798828" xlink:href="#DejaVuSans-62">
          </use>
          <use x="1186.275391" xlink:href="#DejaVuSans-61">
          </use>
          <use x="1247.554688" xlink:href="#DejaVuSans-63">
          </use>
          <use x="1302.535156" xlink:href="#DejaVuSans-6b">
          </use>
          <use x="1360.445312" xlink:href="#DejaVuSans-70">
          </use>
          <use x="1423.921875" xlink:href="#DejaVuSans-72">
          </use>
          <use x="1462.785156" xlink:href="#DejaVuSans-6f">
          </use>
          <use x="1523.966797" xlink:href="#DejaVuSans-70">
          </use>
          <use x="1587.443359" xlink:href="#DejaVuSans-20">
          </use>
          <use x="1619.230469" xlink:href="#DejaVuSans-69">
          </use>
          <use x="1647.013672" xlink:href="#DejaVuSans-74">
          </use>
          <use x="1686.222656" xlink:href="#DejaVuSans-65">
          </use>
          <use x="1747.746094" xlink:href="#DejaVuSans-72">
          </use>
          <use x="1788.859375" xlink:href="#DejaVuSans-61">
          </use>
          <use x="1850.138672" xlink:href="#DejaVuSans-74">
          </use>
          <use x="1889.347656" xlink:href="#DejaVuSans-69">
          </use>
          <use x="1917.130859" xlink:href="#DejaVuSans-6f">
          </use>
          <use x="1978.3125" xlink:href="#DejaVuSans-6e">
          </use>
         </g>
        </g>
        <g id="legend_1">
         <g id="patch_7">
          <path d="M 241.825938 74.3525 
L 373.543125 74.3525 
Q 375.543125 74.3525 375.543125 72.3525 
L 375.543125 29.318125 
Q 375.543125 27.318125 373.543125 27.318125 
L 241.825938 27.318125 
Q 239.825938 27.318125 239.825938 29.318125 
L 239.825938 72.3525 
Q 239.825938 74.3525 241.825938 74.3525 
z
" style="fill:#eaeaf2;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;">
          </path>
         </g>
         <g id="line2d_15">
          <path d="M 243.825938 35.416562 
L 263.825938 35.416562 
" style="fill:none;stroke:#000000;stroke-linecap:round;stroke-width:0.5;">
          </path>
         </g>
         <g id="line2d_16">
         </g>
         <g id="text_15">
          <!-- cost minibatches -->
          <g style="fill:#262626;" transform="translate(271.825938 38.916562)scale(0.1 -0.1)">
           <defs>
            <path d="M 3328 2828 
Q 3544 3216 3844 3400 
Q 4144 3584 4550 3584 
Q 5097 3584 5394 3201 
Q 5691 2819 5691 2113 
L 5691 0 
L 5113 0 
L 5113 2094 
Q 5113 2597 4934 2840 
Q 4756 3084 4391 3084 
Q 3944 3084 3684 2787 
Q 3425 2491 3425 1978 
L 3425 0 
L 2847 0 
L 2847 2094 
Q 2847 2600 2669 2842 
Q 2491 3084 2119 3084 
Q 1678 3084 1418 2786 
Q 1159 2488 1159 1978 
L 1159 0 
L 581 0 
L 581 3500 
L 1159 3500 
L 1159 2956 
Q 1356 3278 1631 3431 
Q 1906 3584 2284 3584 
Q 2666 3584 2933 3390 
Q 3200 3197 3328 2828 
z
" id="DejaVuSans-6d" transform="scale(0.015625)">
            </path>
            <path d="M 3513 2113 
L 3513 0 
L 2938 0 
L 2938 2094 
Q 2938 2591 2744 2837 
Q 2550 3084 2163 3084 
Q 1697 3084 1428 2787 
Q 1159 2491 1159 1978 
L 1159 0 
L 581 0 
L 581 4863 
L 1159 4863 
L 1159 2956 
Q 1366 3272 1645 3428 
Q 1925 3584 2291 3584 
Q 2894 3584 3203 3211 
Q 3513 2838 3513 2113 
z
" id="DejaVuSans-68" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-63">
           </use>
           <use x="54.980469" xlink:href="#DejaVuSans-6f">
           </use>
           <use x="116.162109" xlink:href="#DejaVuSans-73">
           </use>
           <use x="168.261719" xlink:href="#DejaVuSans-74">
           </use>
           <use x="207.470703" xlink:href="#DejaVuSans-20">
           </use>
           <use x="239.257812" xlink:href="#DejaVuSans-6d">
           </use>
           <use x="336.669922" xlink:href="#DejaVuSans-69">
           </use>
           <use x="364.453125" xlink:href="#DejaVuSans-6e">
           </use>
           <use x="427.832031" xlink:href="#DejaVuSans-69">
           </use>
           <use x="455.615234" xlink:href="#DejaVuSans-62">
           </use>
           <use x="519.091797" xlink:href="#DejaVuSans-61">
           </use>
           <use x="580.371094" xlink:href="#DejaVuSans-74">
           </use>
           <use x="619.580078" xlink:href="#DejaVuSans-63">
           </use>
           <use x="674.560547" xlink:href="#DejaVuSans-68">
           </use>
           <use x="737.939453" xlink:href="#DejaVuSans-65">
           </use>
           <use x="799.462891" xlink:href="#DejaVuSans-73">
           </use>
          </g>
         </g>
         <g id="line2d_17">
          <path d="M 243.825938 50.094687 
L 263.825938 50.094687 
" style="fill:none;stroke:#ff0000;stroke-linecap:round;stroke-width:2;">
          </path>
         </g>
         <g id="line2d_18">
         </g>
         <g id="text_16">
          <!-- cost full training set -->
          <g style="fill:#262626;" transform="translate(271.825938 53.594687)scale(0.1 -0.1)">
           <defs>
            <path d="M 544 1381 
L 544 3500 
L 1119 3500 
L 1119 1403 
Q 1119 906 1312 657 
Q 1506 409 1894 409 
Q 2359 409 2629 706 
Q 2900 1003 2900 1516 
L 2900 3500 
L 3475 3500 
L 3475 0 
L 2900 0 
L 2900 538 
Q 2691 219 2414 64 
Q 2138 -91 1772 -91 
Q 1169 -91 856 284 
Q 544 659 544 1381 
z
M 1991 3584 
L 1991 3584 
z
" id="DejaVuSans-75" transform="scale(0.015625)">
            </path>
            <path d="M 603 4863 
L 1178 4863 
L 1178 0 
L 603 0 
L 603 4863 
z
" id="DejaVuSans-6c" transform="scale(0.015625)">
            </path>
            <path d="M 2906 1791 
Q 2906 2416 2648 2759 
Q 2391 3103 1925 3103 
Q 1463 3103 1205 2759 
Q 947 2416 947 1791 
Q 947 1169 1205 825 
Q 1463 481 1925 481 
Q 2391 481 2648 825 
Q 2906 1169 2906 1791 
z
M 3481 434 
Q 3481 -459 3084 -895 
Q 2688 -1331 1869 -1331 
Q 1566 -1331 1297 -1286 
Q 1028 -1241 775 -1147 
L 775 -588 
Q 1028 -725 1275 -790 
Q 1522 -856 1778 -856 
Q 2344 -856 2625 -561 
Q 2906 -266 2906 331 
L 2906 616 
Q 2728 306 2450 153 
Q 2172 0 1784 0 
Q 1141 0 747 490 
Q 353 981 353 1791 
Q 353 2603 747 3093 
Q 1141 3584 1784 3584 
Q 2172 3584 2450 3431 
Q 2728 3278 2906 2969 
L 2906 3500 
L 3481 3500 
L 3481 434 
z
" id="DejaVuSans-67" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-63">
           </use>
           <use x="54.980469" xlink:href="#DejaVuSans-6f">
           </use>
           <use x="116.162109" xlink:href="#DejaVuSans-73">
           </use>
           <use x="168.261719" xlink:href="#DejaVuSans-74">
           </use>
           <use x="207.470703" xlink:href="#DejaVuSans-20">
           </use>
           <use x="239.257812" xlink:href="#DejaVuSans-66">
           </use>
           <use x="274.462891" xlink:href="#DejaVuSans-75">
           </use>
           <use x="337.841797" xlink:href="#DejaVuSans-6c">
           </use>
           <use x="365.625" xlink:href="#DejaVuSans-6c">
           </use>
           <use x="393.408203" xlink:href="#DejaVuSans-20">
           </use>
           <use x="425.195312" xlink:href="#DejaVuSans-74">
           </use>
           <use x="464.404297" xlink:href="#DejaVuSans-72">
           </use>
           <use x="505.517578" xlink:href="#DejaVuSans-61">
           </use>
           <use x="566.796875" xlink:href="#DejaVuSans-69">
           </use>
           <use x="594.580078" xlink:href="#DejaVuSans-6e">
           </use>
           <use x="657.958984" xlink:href="#DejaVuSans-69">
           </use>
           <use x="685.742188" xlink:href="#DejaVuSans-6e">
           </use>
           <use x="749.121094" xlink:href="#DejaVuSans-67">
           </use>
           <use x="812.597656" xlink:href="#DejaVuSans-20">
           </use>
           <use x="844.384766" xlink:href="#DejaVuSans-73">
           </use>
           <use x="896.484375" xlink:href="#DejaVuSans-65">
           </use>
           <use x="958.007812" xlink:href="#DejaVuSans-74">
           </use>
          </g>
         </g>
         <g id="line2d_19">
          <path d="M 243.825938 64.772812 
L 263.825938 64.772812 
" style="fill:none;stroke:#0000ff;stroke-linecap:round;stroke-width:3;">
          </path>
         </g>
         <g id="line2d_20">
         </g>
         <g id="text_17">
          <!-- cost validation set -->
          <g style="fill:#262626;" transform="translate(271.825938 68.272812)scale(0.1 -0.1)">
           <defs>
            <path d="M 2906 2969 
L 2906 4863 
L 3481 4863 
L 3481 0 
L 2906 0 
L 2906 525 
Q 2725 213 2448 61 
Q 2172 -91 1784 -91 
Q 1150 -91 751 415 
Q 353 922 353 1747 
Q 353 2572 751 3078 
Q 1150 3584 1784 3584 
Q 2172 3584 2448 3432 
Q 2725 3281 2906 2969 
z
M 947 1747 
Q 947 1113 1208 752 
Q 1469 391 1925 391 
Q 2381 391 2643 752 
Q 2906 1113 2906 1747 
Q 2906 2381 2643 2742 
Q 2381 3103 1925 3103 
Q 1469 3103 1208 2742 
Q 947 2381 947 1747 
z
" id="DejaVuSans-64" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-63">
           </use>
           <use x="54.980469" xlink:href="#DejaVuSans-6f">
           </use>
           <use x="116.162109" xlink:href="#DejaVuSans-73">
           </use>
           <use x="168.261719" xlink:href="#DejaVuSans-74">
           </use>
           <use x="207.470703" xlink:href="#DejaVuSans-20">
           </use>
           <use x="239.257812" xlink:href="#DejaVuSans-76">
           </use>
           <use x="298.4375" xlink:href="#DejaVuSans-61">
           </use>
           <use x="359.716797" xlink:href="#DejaVuSans-6c">
           </use>
           <use x="387.5" xlink:href="#DejaVuSans-69">
           </use>
           <use x="415.283203" xlink:href="#DejaVuSans-64">
           </use>
           <use x="478.759766" xlink:href="#DejaVuSans-61">
           </use>
           <use x="540.039062" xlink:href="#DejaVuSans-74">
           </use>
           <use x="579.248047" xlink:href="#DejaVuSans-69">
           </use>
           <use x="607.03125" xlink:href="#DejaVuSans-6f">
           </use>
           <use x="668.212891" xlink:href="#DejaVuSans-6e">
           </use>
           <use x="731.591797" xlink:href="#DejaVuSans-20">
           </use>
           <use x="763.378906" xlink:href="#DejaVuSans-73">
           </use>
           <use x="815.478516" xlink:href="#DejaVuSans-65">
           </use>
           <use x="877.001953" xlink:href="#DejaVuSans-74">
           </use>
          </g>
         </g>
        </g>
       </g>
      </g>
      <defs>
       <clippath id="p31b5e10f6c">
        <rect height="217.44" width="334.8" x="45.743125" y="22.318125">
        </rect>
       </clippath>
      </defs>
     </svg>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <h2 id="Performance-on-the-test-set">
    Performance on the test set
    <a class="anchor-link" href="#Performance-on-the-test-set">
     <i class="fas fa-sm fa-link">
     </i>
    </a>
   </h2>
   <p>
    Finally, the
    <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">
     accuracy
    </a>
    on the independent test set is computed to measure the performance of the model. The
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">
     scikit-learn
    </a>
    <code>
     accuracy_score
    </code>
    method is used to compute this accuracy from the predictions made by the model. The final accuracy on the test set is $96\%$ as shown below.
   </p>
   <p>
    The results can be analyzed in more detail with the help of a
    <a href="https://en.wikipedia.org/wiki/Confusion_matrix">
     confusion table
    </a>
    . This table shows how many samples of which class are classified as one of the possible classes. The confusion table is shown in the figure below. It is computed by the
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">
     scikit-learn
    </a>
    <code>
     confusion_matrix
    </code>
    method.
    <br/>
    Notice that the digit '8' is misclassified five times, two times as '2', two times as '5', and one time as '9'.
   </p>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [17]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Get results of test data</span>
<span class="c1"># Get the target outputs</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">T_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Get activation of test samples</span>
<span class="n">activations</span> <span class="o">=</span> <span class="n">forward_step</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">layers</span><span class="p">)</span>
<span class="c1"># Get the predictions made by the network</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Test set accuracy</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'The accuracy on the test set is </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">:</span><span class="s1">.0%</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>The accuracy on the test set is 96%
</pre>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [18]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Show confusion table</span>
<span class="c1"># Get confustion matrix</span>
<span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="c1"># Plot the confusion table</span>
<span class="c1"># Digit class names</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">'$</span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="s1">d</span><span class="si">}</span><span class="s1">$'</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>
<span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">axes_style</span><span class="p">(</span><span class="s2">"white"</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Show class labels on each axis</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">tick_top</span><span class="p">()</span>
    <span class="n">major_ticks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">minor_ticks</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">major_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">major_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">minor_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">(</span><span class="n">minor_ticks</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="n">minor</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="c1"># Set plot labels</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_label_position</span><span class="p">(</span><span class="s2">"right"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Predicted label'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'True label'</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">'Confusion table'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="c1"># Show a grid to seperate digits</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">b</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="sa">u</span><span class="s1">'minor'</span><span class="p">)</span>
    <span class="c1"># Color each grid cell according to the number classes predicted</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'binary'</span><span class="p">)</span>
    <span class="c1"># Show the number of samples in each cell</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">conf_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">conf_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">'w'</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="n">y</span> <span class="k">else</span> <span class="s1">'k'</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">conf_matrix</span><span class="p">[</span><span class="n">y</span><span class="p">,</span><span class="n">x</span><span class="p">],</span> 
                    <span class="n">ha</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_svg output_subarea">
     <?xml version="1.0" encoding="utf-8" standalone="no"?>
     <!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
     <svg height="288.718125pt" version="1.1" viewbox="0 0 260.198125 288.718125" width="260.198125pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <metadata>
       <rdf:rdf xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
        <cc:work>
         <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage">
         </dc:type>
         <dc:date>
          2021-05-13T19:57:19.258015
         </dc:date>
         <dc:format>
          image/svg+xml
         </dc:format>
         <dc:creator>
          <cc:agent>
           <dc:title>
            Matplotlib v3.4.2, https://matplotlib.org/
           </dc:title>
          </cc:agent>
         </dc:creator>
        </cc:work>
       </rdf:rdf>
      </metadata>
      <defs>
       <style type="text/css">
        *{stroke-linecap:butt;stroke-linejoin:round;}
       </style>
      </defs>
      <g id="figure_1">
       <g id="patch_1">
        <path d="M 0 288.718125 
L 260.198125 288.718125 
L 260.198125 0 
L 0 0 
z
" style="fill:#ffffff;">
        </path>
       </g>
       <g id="axes_1">
        <g id="patch_2">
         <path d="M 21.88 267.84 
L 239.32 267.84 
L 239.32 50.4 
L 21.88 50.4 
z
" style="fill:#ffffff;">
         </path>
        </g>
        <g clip-path="url(#p7e321d4209)">
         <image height="218" id="image6289292836" transform="scale(1 -1)translate(0 -218)" width="218" x="21.88" xlink:href="data:image/png;base64,
iVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAADVUlEQVR4nO3csW3bYBRGUTIw6MadFvACHkULeG133sGV3IgNUyYpA+jdgMg5A3wgQFz83VuP4ziW/9y+7yO727aN7J7te8/k9fV1ZPfHyCrwB6FBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBoGnf/0Bf+v7+/vhmy8vLw/fnOQK1rJcr9eR3c/Pz5FdLxoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBNbjOI6JYZeaZi52Lcv5rnY9Pz8/fPN+vz98c5IXDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQIPE0Nn+mIzpSzHdF5f38f2T3bIZ0JXjQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CIxdwTqTfd9Hdqcugb29vY3sfnx8jOxOmPpnU7xoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBBYj+M4JobPdllqwrquI7tDv4xBXjQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CDxNDU9dq5q4rnW5XB6+uSyuVfGLFw0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAmNXsKZcr9eHb95ut4dvwu+8aBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBYOw4z+VyGdn9+voa2YVJXjQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CKzbth0Tw/f7fWL2VPZ9H9ndtm1klzleNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQI/ATxM02Vp6ZRYwAAAABJRU5ErkJggg==" y="-49.84"/>
        </g>
        <g id="matplotlib.axis_1">
         <g id="xtick_1">
          <g id="line2d_1">
           <defs>
            <path d="M 0 0 
L 0 -3.5 
" id="m9cd2abacc1" style="stroke:#262626;stroke-width:0.8;">
            </path>
           </defs>
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="32.752" xlink:href="#m9cd2abacc1" y="50.4">
            </use>
           </g>
          </g>
          <g id="text_1">
           <!-- $0$ -->
           <g style="fill:#262626;" transform="translate(28.912 40.904375)scale(0.12 -0.12)">
            <defs>
             <path d="M 2034 4250 
Q 1547 4250 1301 3770 
Q 1056 3291 1056 2328 
Q 1056 1369 1301 889 
Q 1547 409 2034 409 
Q 2525 409 2770 889 
Q 3016 1369 3016 2328 
Q 3016 3291 2770 3770 
Q 2525 4250 2034 4250 
z
M 2034 4750 
Q 2819 4750 3233 4129 
Q 3647 3509 3647 2328 
Q 3647 1150 3233 529 
Q 2819 -91 2034 -91 
Q 1250 -91 836 529 
Q 422 1150 422 2328 
Q 422 3509 836 4129 
Q 1250 4750 2034 4750 
z
" id="DejaVuSans-30" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_2">
          <g id="line2d_2">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="54.496" xlink:href="#m9cd2abacc1" y="50.4">
            </use>
           </g>
          </g>
          <g id="text_2">
           <!-- $1$ -->
           <g style="fill:#262626;" transform="translate(50.656 40.904375)scale(0.12 -0.12)">
            <defs>
             <path d="M 794 531 
L 1825 531 
L 1825 4091 
L 703 3866 
L 703 4441 
L 1819 4666 
L 2450 4666 
L 2450 531 
L 3481 531 
L 3481 0 
L 794 0 
L 794 531 
z
" id="DejaVuSans-31" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-31">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_3">
          <g id="line2d_3">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="76.24" xlink:href="#m9cd2abacc1" y="50.4">
            </use>
           </g>
          </g>
          <g id="text_3">
           <!-- $2$ -->
           <g style="fill:#262626;" transform="translate(72.4 40.904375)scale(0.12 -0.12)">
            <defs>
             <path d="M 1228 531 
L 3431 531 
L 3431 0 
L 469 0 
L 469 531 
Q 828 903 1448 1529 
Q 2069 2156 2228 2338 
Q 2531 2678 2651 2914 
Q 2772 3150 2772 3378 
Q 2772 3750 2511 3984 
Q 2250 4219 1831 4219 
Q 1534 4219 1204 4116 
Q 875 4013 500 3803 
L 500 4441 
Q 881 4594 1212 4672 
Q 1544 4750 1819 4750 
Q 2544 4750 2975 4387 
Q 3406 4025 3406 3419 
Q 3406 3131 3298 2873 
Q 3191 2616 2906 2266 
Q 2828 2175 2409 1742 
Q 1991 1309 1228 531 
z
" id="DejaVuSans-32" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-32">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_4">
          <g id="line2d_4">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="97.984" xlink:href="#m9cd2abacc1" y="50.4">
            </use>
           </g>
          </g>
          <g id="text_4">
           <!-- $3$ -->
           <g style="fill:#262626;" transform="translate(94.144 40.904375)scale(0.12 -0.12)">
            <defs>
             <path d="M 2597 2516 
Q 3050 2419 3304 2112 
Q 3559 1806 3559 1356 
Q 3559 666 3084 287 
Q 2609 -91 1734 -91 
Q 1441 -91 1130 -33 
Q 819 25 488 141 
L 488 750 
Q 750 597 1062 519 
Q 1375 441 1716 441 
Q 2309 441 2620 675 
Q 2931 909 2931 1356 
Q 2931 1769 2642 2001 
Q 2353 2234 1838 2234 
L 1294 2234 
L 1294 2753 
L 1863 2753 
Q 2328 2753 2575 2939 
Q 2822 3125 2822 3475 
Q 2822 3834 2567 4026 
Q 2313 4219 1838 4219 
Q 1578 4219 1281 4162 
Q 984 4106 628 3988 
L 628 4550 
Q 988 4650 1302 4700 
Q 1616 4750 1894 4750 
Q 2613 4750 3031 4423 
Q 3450 4097 3450 3541 
Q 3450 3153 3228 2886 
Q 3006 2619 2597 2516 
z
" id="DejaVuSans-33" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-33">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_5">
          <g id="line2d_5">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="119.728" xlink:href="#m9cd2abacc1" y="50.4">
            </use>
           </g>
          </g>
          <g id="text_5">
           <!-- $4$ -->
           <g style="fill:#262626;" transform="translate(115.888 40.904375)scale(0.12 -0.12)">
            <defs>
             <path d="M 2419 4116 
L 825 1625 
L 2419 1625 
L 2419 4116 
z
M 2253 4666 
L 3047 4666 
L 3047 1625 
L 3713 1625 
L 3713 1100 
L 3047 1100 
L 3047 0 
L 2419 0 
L 2419 1100 
L 313 1100 
L 313 1709 
L 2253 4666 
z
" id="DejaVuSans-34" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-34">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_6">
          <g id="line2d_6">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="141.472" xlink:href="#m9cd2abacc1" y="50.4">
            </use>
           </g>
          </g>
          <g id="text_6">
           <!-- $5$ -->
           <g style="fill:#262626;" transform="translate(137.632 40.904375)scale(0.12 -0.12)">
            <defs>
             <path d="M 691 4666 
L 3169 4666 
L 3169 4134 
L 1269 4134 
L 1269 2991 
Q 1406 3038 1543 3061 
Q 1681 3084 1819 3084 
Q 2600 3084 3056 2656 
Q 3513 2228 3513 1497 
Q 3513 744 3044 326 
Q 2575 -91 1722 -91 
Q 1428 -91 1123 -41 
Q 819 9 494 109 
L 494 744 
Q 775 591 1075 516 
Q 1375 441 1709 441 
Q 2250 441 2565 725 
Q 2881 1009 2881 1497 
Q 2881 1984 2565 2268 
Q 2250 2553 1709 2553 
Q 1456 2553 1204 2497 
Q 953 2441 691 2322 
L 691 4666 
z
" id="DejaVuSans-35" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-35">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_7">
          <g id="line2d_7">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="163.216" xlink:href="#m9cd2abacc1" y="50.4">
            </use>
           </g>
          </g>
          <g id="text_7">
           <!-- $6$ -->
           <g style="fill:#262626;" transform="translate(159.376 40.904375)scale(0.12 -0.12)">
            <defs>
             <path d="M 2113 2584 
Q 1688 2584 1439 2293 
Q 1191 2003 1191 1497 
Q 1191 994 1439 701 
Q 1688 409 2113 409 
Q 2538 409 2786 701 
Q 3034 994 3034 1497 
Q 3034 2003 2786 2293 
Q 2538 2584 2113 2584 
z
M 3366 4563 
L 3366 3988 
Q 3128 4100 2886 4159 
Q 2644 4219 2406 4219 
Q 1781 4219 1451 3797 
Q 1122 3375 1075 2522 
Q 1259 2794 1537 2939 
Q 1816 3084 2150 3084 
Q 2853 3084 3261 2657 
Q 3669 2231 3669 1497 
Q 3669 778 3244 343 
Q 2819 -91 2113 -91 
Q 1303 -91 875 529 
Q 447 1150 447 2328 
Q 447 3434 972 4092 
Q 1497 4750 2381 4750 
Q 2619 4750 2861 4703 
Q 3103 4656 3366 4563 
z
" id="DejaVuSans-36" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-36">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_8">
          <g id="line2d_8">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="184.96" xlink:href="#m9cd2abacc1" y="50.4">
            </use>
           </g>
          </g>
          <g id="text_8">
           <!-- $7$ -->
           <g style="fill:#262626;" transform="translate(181.12 40.904375)scale(0.12 -0.12)">
            <defs>
             <path d="M 525 4666 
L 3525 4666 
L 3525 4397 
L 1831 0 
L 1172 0 
L 2766 4134 
L 525 4134 
L 525 4666 
z
" id="DejaVuSans-37" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-37">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_9">
          <g id="line2d_9">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="206.704" xlink:href="#m9cd2abacc1" y="50.4">
            </use>
           </g>
          </g>
          <g id="text_9">
           <!-- $8$ -->
           <g style="fill:#262626;" transform="translate(202.864 40.904375)scale(0.12 -0.12)">
            <defs>
             <path d="M 2034 2216 
Q 1584 2216 1326 1975 
Q 1069 1734 1069 1313 
Q 1069 891 1326 650 
Q 1584 409 2034 409 
Q 2484 409 2743 651 
Q 3003 894 3003 1313 
Q 3003 1734 2745 1975 
Q 2488 2216 2034 2216 
z
M 1403 2484 
Q 997 2584 770 2862 
Q 544 3141 544 3541 
Q 544 4100 942 4425 
Q 1341 4750 2034 4750 
Q 2731 4750 3128 4425 
Q 3525 4100 3525 3541 
Q 3525 3141 3298 2862 
Q 3072 2584 2669 2484 
Q 3125 2378 3379 2068 
Q 3634 1759 3634 1313 
Q 3634 634 3220 271 
Q 2806 -91 2034 -91 
Q 1263 -91 848 271 
Q 434 634 434 1313 
Q 434 1759 690 2068 
Q 947 2378 1403 2484 
z
M 1172 3481 
Q 1172 3119 1398 2916 
Q 1625 2713 2034 2713 
Q 2441 2713 2670 2916 
Q 2900 3119 2900 3481 
Q 2900 3844 2670 4047 
Q 2441 4250 2034 4250 
Q 1625 4250 1398 4047 
Q 1172 3844 1172 3481 
z
" id="DejaVuSans-38" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-38">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_10">
          <g id="line2d_10">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.8;" x="228.448" xlink:href="#m9cd2abacc1" y="50.4">
            </use>
           </g>
          </g>
          <g id="text_10">
           <!-- $9$ -->
           <g style="fill:#262626;" transform="translate(224.608 40.904375)scale(0.12 -0.12)">
            <defs>
             <path d="M 703 97 
L 703 672 
Q 941 559 1184 500 
Q 1428 441 1663 441 
Q 2288 441 2617 861 
Q 2947 1281 2994 2138 
Q 2813 1869 2534 1725 
Q 2256 1581 1919 1581 
Q 1219 1581 811 2004 
Q 403 2428 403 3163 
Q 403 3881 828 4315 
Q 1253 4750 1959 4750 
Q 2769 4750 3195 4129 
Q 3622 3509 3622 2328 
Q 3622 1225 3098 567 
Q 2575 -91 1691 -91 
Q 1453 -91 1209 -44 
Q 966 3 703 97 
z
M 1959 2075 
Q 2384 2075 2632 2365 
Q 2881 2656 2881 3163 
Q 2881 3666 2632 3958 
Q 2384 4250 1959 4250 
Q 1534 4250 1286 3958 
Q 1038 3666 1038 3163 
Q 1038 2656 1286 2365 
Q 1534 2075 1959 2075 
z
" id="DejaVuSans-39" transform="scale(0.015625)">
             </path>
            </defs>
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-39">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_11">
          <g id="line2d_11">
           <path clip-path="url(#p7e321d4209)" d="M 43.624 267.84 
L 43.624 50.4 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_12">
           <defs>
            <path d="M 0 0 
L 0 -2 
" id="me15e7f7fee" style="stroke:#262626;stroke-width:0.6;">
            </path>
           </defs>
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="43.624" xlink:href="#me15e7f7fee" y="50.4">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_12">
          <g id="line2d_13">
           <path clip-path="url(#p7e321d4209)" d="M 65.368 267.84 
L 65.368 50.4 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_14">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="65.368" xlink:href="#me15e7f7fee" y="50.4">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_13">
          <g id="line2d_15">
           <path clip-path="url(#p7e321d4209)" d="M 87.112 267.84 
L 87.112 50.4 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_16">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="87.112" xlink:href="#me15e7f7fee" y="50.4">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_14">
          <g id="line2d_17">
           <path clip-path="url(#p7e321d4209)" d="M 108.856 267.84 
L 108.856 50.4 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_18">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="108.856" xlink:href="#me15e7f7fee" y="50.4">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_15">
          <g id="line2d_19">
           <path clip-path="url(#p7e321d4209)" d="M 130.6 267.84 
L 130.6 50.4 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_20">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="130.6" xlink:href="#me15e7f7fee" y="50.4">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_16">
          <g id="line2d_21">
           <path clip-path="url(#p7e321d4209)" d="M 152.344 267.84 
L 152.344 50.4 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_22">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="152.344" xlink:href="#me15e7f7fee" y="50.4">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_17">
          <g id="line2d_23">
           <path clip-path="url(#p7e321d4209)" d="M 174.088 267.84 
L 174.088 50.4 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_24">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="174.088" xlink:href="#me15e7f7fee" y="50.4">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_18">
          <g id="line2d_25">
           <path clip-path="url(#p7e321d4209)" d="M 195.832 267.84 
L 195.832 50.4 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_26">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="195.832" xlink:href="#me15e7f7fee" y="50.4">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_19">
          <g id="line2d_27">
           <path clip-path="url(#p7e321d4209)" d="M 217.576 267.84 
L 217.576 50.4 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_28">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="217.576" xlink:href="#me15e7f7fee" y="50.4">
            </use>
           </g>
          </g>
         </g>
         <g id="xtick_20">
          <g id="line2d_29">
           <path clip-path="url(#p7e321d4209)" d="M 239.32 267.84 
L 239.32 50.4 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
          <g id="line2d_30">
           <g>
            <use style="fill:#262626;stroke:#262626;stroke-width:0.6;" x="239.32" xlink:href="#me15e7f7fee" y="50.4">
            </use>
           </g>
          </g>
         </g>
         <g id="text_11">
          <!-- Predicted label -->
          <g style="fill:#262626;" transform="translate(93.447656 279.438437)scale(0.1 -0.1)">
           <defs>
            <path d="M 1259 4147 
L 1259 2394 
L 2053 2394 
Q 2494 2394 2734 2622 
Q 2975 2850 2975 3272 
Q 2975 3691 2734 3919 
Q 2494 4147 2053 4147 
L 1259 4147 
z
M 628 4666 
L 2053 4666 
Q 2838 4666 3239 4311 
Q 3641 3956 3641 3272 
Q 3641 2581 3239 2228 
Q 2838 1875 2053 1875 
L 1259 1875 
L 1259 0 
L 628 0 
L 628 4666 
z
" id="DejaVuSans-50" transform="scale(0.015625)">
            </path>
            <path d="M 2631 2963 
Q 2534 3019 2420 3045 
Q 2306 3072 2169 3072 
Q 1681 3072 1420 2755 
Q 1159 2438 1159 1844 
L 1159 0 
L 581 0 
L 581 3500 
L 1159 3500 
L 1159 2956 
Q 1341 3275 1631 3429 
Q 1922 3584 2338 3584 
Q 2397 3584 2469 3576 
Q 2541 3569 2628 3553 
L 2631 2963 
z
" id="DejaVuSans-72" transform="scale(0.015625)">
            </path>
            <path d="M 3597 1894 
L 3597 1613 
L 953 1613 
Q 991 1019 1311 708 
Q 1631 397 2203 397 
Q 2534 397 2845 478 
Q 3156 559 3463 722 
L 3463 178 
Q 3153 47 2828 -22 
Q 2503 -91 2169 -91 
Q 1331 -91 842 396 
Q 353 884 353 1716 
Q 353 2575 817 3079 
Q 1281 3584 2069 3584 
Q 2775 3584 3186 3129 
Q 3597 2675 3597 1894 
z
M 3022 2063 
Q 3016 2534 2758 2815 
Q 2500 3097 2075 3097 
Q 1594 3097 1305 2825 
Q 1016 2553 972 2059 
L 3022 2063 
z
" id="DejaVuSans-65" transform="scale(0.015625)">
            </path>
            <path d="M 2906 2969 
L 2906 4863 
L 3481 4863 
L 3481 0 
L 2906 0 
L 2906 525 
Q 2725 213 2448 61 
Q 2172 -91 1784 -91 
Q 1150 -91 751 415 
Q 353 922 353 1747 
Q 353 2572 751 3078 
Q 1150 3584 1784 3584 
Q 2172 3584 2448 3432 
Q 2725 3281 2906 2969 
z
M 947 1747 
Q 947 1113 1208 752 
Q 1469 391 1925 391 
Q 2381 391 2643 752 
Q 2906 1113 2906 1747 
Q 2906 2381 2643 2742 
Q 2381 3103 1925 3103 
Q 1469 3103 1208 2742 
Q 947 2381 947 1747 
z
" id="DejaVuSans-64" transform="scale(0.015625)">
            </path>
            <path d="M 603 3500 
L 1178 3500 
L 1178 0 
L 603 0 
L 603 3500 
z
M 603 4863 
L 1178 4863 
L 1178 4134 
L 603 4134 
L 603 4863 
z
" id="DejaVuSans-69" transform="scale(0.015625)">
            </path>
            <path d="M 3122 3366 
L 3122 2828 
Q 2878 2963 2633 3030 
Q 2388 3097 2138 3097 
Q 1578 3097 1268 2742 
Q 959 2388 959 1747 
Q 959 1106 1268 751 
Q 1578 397 2138 397 
Q 2388 397 2633 464 
Q 2878 531 3122 666 
L 3122 134 
Q 2881 22 2623 -34 
Q 2366 -91 2075 -91 
Q 1284 -91 818 406 
Q 353 903 353 1747 
Q 353 2603 823 3093 
Q 1294 3584 2113 3584 
Q 2378 3584 2631 3529 
Q 2884 3475 3122 3366 
z
" id="DejaVuSans-63" transform="scale(0.015625)">
            </path>
            <path d="M 1172 4494 
L 1172 3500 
L 2356 3500 
L 2356 3053 
L 1172 3053 
L 1172 1153 
Q 1172 725 1289 603 
Q 1406 481 1766 481 
L 2356 481 
L 2356 0 
L 1766 0 
Q 1100 0 847 248 
Q 594 497 594 1153 
L 594 3053 
L 172 3053 
L 172 3500 
L 594 3500 
L 594 4494 
L 1172 4494 
z
" id="DejaVuSans-74" transform="scale(0.015625)">
            </path>
            <path id="DejaVuSans-20" transform="scale(0.015625)">
            </path>
            <path d="M 603 4863 
L 1178 4863 
L 1178 0 
L 603 0 
L 603 4863 
z
" id="DejaVuSans-6c" transform="scale(0.015625)">
            </path>
            <path d="M 2194 1759 
Q 1497 1759 1228 1600 
Q 959 1441 959 1056 
Q 959 750 1161 570 
Q 1363 391 1709 391 
Q 2188 391 2477 730 
Q 2766 1069 2766 1631 
L 2766 1759 
L 2194 1759 
z
M 3341 1997 
L 3341 0 
L 2766 0 
L 2766 531 
Q 2569 213 2275 61 
Q 1981 -91 1556 -91 
Q 1019 -91 701 211 
Q 384 513 384 1019 
Q 384 1609 779 1909 
Q 1175 2209 1959 2209 
L 2766 2209 
L 2766 2266 
Q 2766 2663 2505 2880 
Q 2244 3097 1772 3097 
Q 1472 3097 1187 3025 
Q 903 2953 641 2809 
L 641 3341 
Q 956 3463 1253 3523 
Q 1550 3584 1831 3584 
Q 2591 3584 2966 3190 
Q 3341 2797 3341 1997 
z
" id="DejaVuSans-61" transform="scale(0.015625)">
            </path>
            <path d="M 3116 1747 
Q 3116 2381 2855 2742 
Q 2594 3103 2138 3103 
Q 1681 3103 1420 2742 
Q 1159 2381 1159 1747 
Q 1159 1113 1420 752 
Q 1681 391 2138 391 
Q 2594 391 2855 752 
Q 3116 1113 3116 1747 
z
M 1159 2969 
Q 1341 3281 1617 3432 
Q 1894 3584 2278 3584 
Q 2916 3584 3314 3078 
Q 3713 2572 3713 1747 
Q 3713 922 3314 415 
Q 2916 -91 2278 -91 
Q 1894 -91 1617 61 
Q 1341 213 1159 525 
L 1159 0 
L 581 0 
L 581 4863 
L 1159 4863 
L 1159 2969 
z
" id="DejaVuSans-62" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-50">
           </use>
           <use x="58.552734" xlink:href="#DejaVuSans-72">
           </use>
           <use x="97.416016" xlink:href="#DejaVuSans-65">
           </use>
           <use x="158.939453" xlink:href="#DejaVuSans-64">
           </use>
           <use x="222.416016" xlink:href="#DejaVuSans-69">
           </use>
           <use x="250.199219" xlink:href="#DejaVuSans-63">
           </use>
           <use x="305.179688" xlink:href="#DejaVuSans-74">
           </use>
           <use x="344.388672" xlink:href="#DejaVuSans-65">
           </use>
           <use x="405.912109" xlink:href="#DejaVuSans-64">
           </use>
           <use x="469.388672" xlink:href="#DejaVuSans-20">
           </use>
           <use x="501.175781" xlink:href="#DejaVuSans-6c">
           </use>
           <use x="528.958984" xlink:href="#DejaVuSans-61">
           </use>
           <use x="590.238281" xlink:href="#DejaVuSans-62">
           </use>
           <use x="653.714844" xlink:href="#DejaVuSans-65">
           </use>
           <use x="715.238281" xlink:href="#DejaVuSans-6c">
           </use>
          </g>
         </g>
        </g>
        <g id="matplotlib.axis_2">
         <g id="ytick_1">
          <g id="text_12">
           <!-- $0$ -->
           <g style="fill:#262626;" transform="translate(7.2 65.831062)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-30">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_2">
          <g id="text_13">
           <!-- $1$ -->
           <g style="fill:#262626;" transform="translate(7.2 87.575062)scale(0.12 -0.12)">
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-31">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_3">
          <g id="text_14">
           <!-- $2$ -->
           <g style="fill:#262626;" transform="translate(7.2 109.319062)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-32">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_4">
          <g id="text_15">
           <!-- $3$ -->
           <g style="fill:#262626;" transform="translate(7.2 131.063062)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-33">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_5">
          <g id="text_16">
           <!-- $4$ -->
           <g style="fill:#262626;" transform="translate(7.2 152.807062)scale(0.12 -0.12)">
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-34">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_6">
          <g id="text_17">
           <!-- $5$ -->
           <g style="fill:#262626;" transform="translate(7.2 174.551062)scale(0.12 -0.12)">
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-35">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_7">
          <g id="text_18">
           <!-- $6$ -->
           <g style="fill:#262626;" transform="translate(7.2 196.295062)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-36">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_8">
          <g id="text_19">
           <!-- $7$ -->
           <g style="fill:#262626;" transform="translate(7.2 218.039062)scale(0.12 -0.12)">
            <use transform="translate(0 0.09375)" xlink:href="#DejaVuSans-37">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_9">
          <g id="text_20">
           <!-- $8$ -->
           <g style="fill:#262626;" transform="translate(7.2 239.783062)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-38">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_10">
          <g id="text_21">
           <!-- $9$ -->
           <g style="fill:#262626;" transform="translate(7.2 261.527062)scale(0.12 -0.12)">
            <use transform="translate(0 0.78125)" xlink:href="#DejaVuSans-39">
            </use>
           </g>
          </g>
         </g>
         <g id="ytick_11">
          <g id="line2d_31">
           <path clip-path="url(#p7e321d4209)" d="M 21.88 72.144 
L 239.32 72.144 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_12">
          <g id="line2d_32">
           <path clip-path="url(#p7e321d4209)" d="M 21.88 93.888 
L 239.32 93.888 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_13">
          <g id="line2d_33">
           <path clip-path="url(#p7e321d4209)" d="M 21.88 115.632 
L 239.32 115.632 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_14">
          <g id="line2d_34">
           <path clip-path="url(#p7e321d4209)" d="M 21.88 137.376 
L 239.32 137.376 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_15">
          <g id="line2d_35">
           <path clip-path="url(#p7e321d4209)" d="M 21.88 159.12 
L 239.32 159.12 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_16">
          <g id="line2d_36">
           <path clip-path="url(#p7e321d4209)" d="M 21.88 180.864 
L 239.32 180.864 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_17">
          <g id="line2d_37">
           <path clip-path="url(#p7e321d4209)" d="M 21.88 202.608 
L 239.32 202.608 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_18">
          <g id="line2d_38">
           <path clip-path="url(#p7e321d4209)" d="M 21.88 224.352 
L 239.32 224.352 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_19">
          <g id="line2d_39">
           <path clip-path="url(#p7e321d4209)" d="M 21.88 246.096 
L 239.32 246.096 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="ytick_20">
          <g id="line2d_40">
           <path clip-path="url(#p7e321d4209)" d="M 21.88 267.84 
L 239.32 267.84 
" style="fill:none;stroke:#cccccc;stroke-linecap:round;stroke-width:0.8;">
           </path>
          </g>
         </g>
         <g id="text_22">
          <!-- True label -->
          <g style="fill:#262626;" transform="translate(250.918438 183.419219)rotate(-90)scale(0.1 -0.1)">
           <defs>
            <path d="M -19 4666 
L 3928 4666 
L 3928 4134 
L 2272 4134 
L 2272 0 
L 1638 0 
L 1638 4134 
L -19 4134 
L -19 4666 
z
" id="DejaVuSans-54" transform="scale(0.015625)">
            </path>
            <path d="M 544 1381 
L 544 3500 
L 1119 3500 
L 1119 1403 
Q 1119 906 1312 657 
Q 1506 409 1894 409 
Q 2359 409 2629 706 
Q 2900 1003 2900 1516 
L 2900 3500 
L 3475 3500 
L 3475 0 
L 2900 0 
L 2900 538 
Q 2691 219 2414 64 
Q 2138 -91 1772 -91 
Q 1169 -91 856 284 
Q 544 659 544 1381 
z
M 1991 3584 
L 1991 3584 
z
" id="DejaVuSans-75" transform="scale(0.015625)">
            </path>
           </defs>
           <use xlink:href="#DejaVuSans-54">
           </use>
           <use x="46.333984" xlink:href="#DejaVuSans-72">
           </use>
           <use x="87.447266" xlink:href="#DejaVuSans-75">
           </use>
           <use x="150.826172" xlink:href="#DejaVuSans-65">
           </use>
           <use x="212.349609" xlink:href="#DejaVuSans-20">
           </use>
           <use x="244.136719" xlink:href="#DejaVuSans-6c">
           </use>
           <use x="271.919922" xlink:href="#DejaVuSans-61">
           </use>
           <use x="333.199219" xlink:href="#DejaVuSans-62">
           </use>
           <use x="396.675781" xlink:href="#DejaVuSans-65">
           </use>
           <use x="458.199219" xlink:href="#DejaVuSans-6c">
           </use>
          </g>
         </g>
        </g>
        <g id="patch_3">
         <path d="M 21.88 267.84 
L 21.88 50.4 
" style="fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_4">
         <path d="M 239.32 267.84 
L 239.32 50.4 
" style="fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_5">
         <path d="M 21.88 267.84 
L 239.32 267.84 
" style="fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="patch_6">
         <path d="M 21.88 50.4 
L 239.32 50.4 
" style="fill:none;stroke:#262626;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;">
         </path>
        </g>
        <g id="text_23">
         <!-- 39 -->
         <g style="fill:#ffffff;" transform="translate(26.3895 64.031375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-39">
          </use>
         </g>
        </g>
        <g id="text_24">
         <!-- 0 -->
         <g transform="translate(29.57075 85.775375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_25">
         <!-- 0 -->
         <g transform="translate(29.57075 107.519375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_26">
         <!-- 0 -->
         <g transform="translate(29.57075 129.263375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_27">
         <!-- 0 -->
         <g transform="translate(29.57075 151.007375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_28">
         <!-- 0 -->
         <g transform="translate(29.57075 172.751375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_29">
         <!-- 0 -->
         <g transform="translate(29.57075 194.495375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_30">
         <!-- 0 -->
         <g transform="translate(29.57075 216.239375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_31">
         <!-- 0 -->
         <g transform="translate(29.57075 237.983375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_32">
         <!-- 0 -->
         <g transform="translate(29.57075 259.727375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_33">
         <!-- 0 -->
         <g transform="translate(51.31475 64.031375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_34">
         <!-- 37 -->
         <g style="fill:#ffffff;" transform="translate(48.1335 85.775375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-37">
          </use>
         </g>
        </g>
        <g id="text_35">
         <!-- 0 -->
         <g transform="translate(51.31475 107.519375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_36">
         <!-- 0 -->
         <g transform="translate(51.31475 129.263375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_37">
         <!-- 1 -->
         <g transform="translate(51.31475 151.007375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_38">
         <!-- 0 -->
         <g transform="translate(51.31475 172.751375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_39">
         <!-- 0 -->
         <g transform="translate(51.31475 194.495375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_40">
         <!-- 1 -->
         <g transform="translate(51.31475 216.239375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_41">
         <!-- 0 -->
         <g transform="translate(51.31475 237.983375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_42">
         <!-- 0 -->
         <g transform="translate(51.31475 259.727375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_43">
         <!-- 0 -->
         <g transform="translate(73.05875 64.031375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_44">
         <!-- 0 -->
         <g transform="translate(73.05875 85.775375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_45">
         <!-- 29 -->
         <g style="fill:#ffffff;" transform="translate(69.8775 107.519375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-39">
          </use>
         </g>
        </g>
        <g id="text_46">
         <!-- 1 -->
         <g transform="translate(73.05875 129.263375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_47">
         <!-- 0 -->
         <g transform="translate(73.05875 151.007375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_48">
         <!-- 0 -->
         <g transform="translate(73.05875 172.751375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_49">
         <!-- 0 -->
         <g transform="translate(73.05875 194.495375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_50">
         <!-- 0 -->
         <g transform="translate(73.05875 216.239375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_51">
         <!-- 2 -->
         <g transform="translate(73.05875 237.983375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
         </g>
        </g>
        <g id="text_52">
         <!-- 0 -->
         <g transform="translate(73.05875 259.727375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_53">
         <!-- 0 -->
         <g transform="translate(94.80275 64.031375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_54">
         <!-- 0 -->
         <g transform="translate(94.80275 85.775375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_55">
         <!-- 0 -->
         <g transform="translate(94.80275 107.519375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_56">
         <!-- 38 -->
         <g style="fill:#ffffff;" transform="translate(91.6215 129.263375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-38">
          </use>
         </g>
        </g>
        <g id="text_57">
         <!-- 0 -->
         <g transform="translate(94.80275 151.007375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_58">
         <!-- 1 -->
         <g transform="translate(94.80275 172.751375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_59">
         <!-- 0 -->
         <g transform="translate(94.80275 194.495375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_60">
         <!-- 0 -->
         <g transform="translate(94.80275 216.239375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_61">
         <!-- 0 -->
         <g transform="translate(94.80275 237.983375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_62">
         <!-- 1 -->
         <g transform="translate(94.80275 259.727375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_63">
         <!-- 1 -->
         <g transform="translate(116.54675 64.031375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_64">
         <!-- 0 -->
         <g transform="translate(116.54675 85.775375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_65">
         <!-- 0 -->
         <g transform="translate(116.54675 107.519375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_66">
         <!-- 0 -->
         <g transform="translate(116.54675 129.263375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_67">
         <!-- 40 -->
         <g style="fill:#ffffff;" transform="translate(113.3655 151.007375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-34">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_68">
         <!-- 0 -->
         <g transform="translate(116.54675 172.751375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_69">
         <!-- 0 -->
         <g transform="translate(116.54675 194.495375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_70">
         <!-- 2 -->
         <g transform="translate(116.54675 216.239375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
         </g>
        </g>
        <g id="text_71">
         <!-- 0 -->
         <g transform="translate(116.54675 237.983375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_72">
         <!-- 0 -->
         <g transform="translate(116.54675 259.727375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_73">
         <!-- 0 -->
         <g transform="translate(138.29075 64.031375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_74">
         <!-- 0 -->
         <g transform="translate(138.29075 85.775375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_75">
         <!-- 0 -->
         <g transform="translate(138.29075 107.519375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_76">
         <!-- 0 -->
         <g transform="translate(138.29075 129.263375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_77">
         <!-- 0 -->
         <g transform="translate(138.29075 151.007375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_78">
         <!-- 33 -->
         <g style="fill:#ffffff;" transform="translate(135.1095 172.751375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-33">
          </use>
         </g>
        </g>
        <g id="text_79">
         <!-- 0 -->
         <g transform="translate(138.29075 194.495375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_80">
         <!-- 0 -->
         <g transform="translate(138.29075 216.239375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_81">
         <!-- 2 -->
         <g transform="translate(138.29075 237.983375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
         </g>
        </g>
        <g id="text_82">
         <!-- 1 -->
         <g transform="translate(138.29075 259.727375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_83">
         <!-- 0 -->
         <g transform="translate(160.03475 64.031375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_84">
         <!-- 0 -->
         <g transform="translate(160.03475 85.775375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_85">
         <!-- 0 -->
         <g transform="translate(160.03475 107.519375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_86">
         <!-- 0 -->
         <g transform="translate(160.03475 129.263375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_87">
         <!-- 0 -->
         <g transform="translate(160.03475 151.007375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_88">
         <!-- 0 -->
         <g transform="translate(160.03475 172.751375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_89">
         <!-- 26 -->
         <g style="fill:#ffffff;" transform="translate(156.8535 194.495375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-36">
          </use>
         </g>
        </g>
        <g id="text_90">
         <!-- 0 -->
         <g transform="translate(160.03475 216.239375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_91">
         <!-- 0 -->
         <g transform="translate(160.03475 237.983375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_92">
         <!-- 0 -->
         <g transform="translate(160.03475 259.727375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_93">
         <!-- 0 -->
         <g transform="translate(181.77875 64.031375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_94">
         <!-- 0 -->
         <g transform="translate(181.77875 85.775375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_95">
         <!-- 0 -->
         <g transform="translate(181.77875 107.519375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_96">
         <!-- 0 -->
         <g transform="translate(181.77875 129.263375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_97">
         <!-- 0 -->
         <g transform="translate(181.77875 151.007375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_98">
         <!-- 0 -->
         <g transform="translate(181.77875 172.751375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_99">
         <!-- 0 -->
         <g transform="translate(181.77875 194.495375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_100">
         <!-- 39 -->
         <g style="fill:#ffffff;" transform="translate(178.5975 216.239375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-39">
          </use>
         </g>
        </g>
        <g id="text_101">
         <!-- 0 -->
         <g transform="translate(181.77875 237.983375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_102">
         <!-- 0 -->
         <g transform="translate(181.77875 259.727375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_103">
         <!-- 0 -->
         <g transform="translate(203.52275 64.031375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_104">
         <!-- 0 -->
         <g transform="translate(203.52275 85.775375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_105">
         <!-- 0 -->
         <g transform="translate(203.52275 107.519375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_106">
         <!-- 0 -->
         <g transform="translate(203.52275 129.263375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_107">
         <!-- 0 -->
         <g transform="translate(203.52275 151.007375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_108">
         <!-- 1 -->
         <g transform="translate(203.52275 172.751375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_109">
         <!-- 0 -->
         <g transform="translate(203.52275 194.495375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_110">
         <!-- 0 -->
         <g transform="translate(203.52275 216.239375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_111">
         <!-- 28 -->
         <g style="fill:#ffffff;" transform="translate(200.3415 237.983375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-32">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-38">
          </use>
         </g>
        </g>
        <g id="text_112">
         <!-- 0 -->
         <g transform="translate(203.52275 259.727375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_113">
         <!-- 0 -->
         <g transform="translate(225.26675 64.031375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_114">
         <!-- 0 -->
         <g transform="translate(225.26675 85.775375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_115">
         <!-- 0 -->
         <g transform="translate(225.26675 107.519375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_116">
         <!-- 0 -->
         <g transform="translate(225.26675 129.263375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_117">
         <!-- 0 -->
         <g transform="translate(225.26675 151.007375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_118">
         <!-- 1 -->
         <g transform="translate(225.26675 172.751375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_119">
         <!-- 0 -->
         <g transform="translate(225.26675 194.495375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_120">
         <!-- 0 -->
         <g transform="translate(225.26675 216.239375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-30">
          </use>
         </g>
        </g>
        <g id="text_121">
         <!-- 1 -->
         <g transform="translate(225.26675 237.983375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-31">
          </use>
         </g>
        </g>
        <g id="text_122">
         <!-- 35 -->
         <g style="fill:#ffffff;" transform="translate(222.0855 259.727375)scale(0.1 -0.1)">
          <use xlink:href="#DejaVuSans-33">
          </use>
          <use x="63.623047" xlink:href="#DejaVuSans-35">
          </use>
         </g>
        </g>
       </g>
       <g id="text_123">
        <!-- Confusion table -->
        <g style="fill:#262626;" transform="translate(78.251875 16.318125)scale(0.12 -0.12)">
         <defs>
          <path d="M 4122 4306 
L 4122 3641 
Q 3803 3938 3442 4084 
Q 3081 4231 2675 4231 
Q 1875 4231 1450 3742 
Q 1025 3253 1025 2328 
Q 1025 1406 1450 917 
Q 1875 428 2675 428 
Q 3081 428 3442 575 
Q 3803 722 4122 1019 
L 4122 359 
Q 3791 134 3420 21 
Q 3050 -91 2638 -91 
Q 1578 -91 968 557 
Q 359 1206 359 2328 
Q 359 3453 968 4101 
Q 1578 4750 2638 4750 
Q 3056 4750 3426 4639 
Q 3797 4528 4122 4306 
z
" id="DejaVuSans-43" transform="scale(0.015625)">
          </path>
          <path d="M 1959 3097 
Q 1497 3097 1228 2736 
Q 959 2375 959 1747 
Q 959 1119 1226 758 
Q 1494 397 1959 397 
Q 2419 397 2687 759 
Q 2956 1122 2956 1747 
Q 2956 2369 2687 2733 
Q 2419 3097 1959 3097 
z
M 1959 3584 
Q 2709 3584 3137 3096 
Q 3566 2609 3566 1747 
Q 3566 888 3137 398 
Q 2709 -91 1959 -91 
Q 1206 -91 779 398 
Q 353 888 353 1747 
Q 353 2609 779 3096 
Q 1206 3584 1959 3584 
z
" id="DejaVuSans-6f" transform="scale(0.015625)">
          </path>
          <path d="M 3513 2113 
L 3513 0 
L 2938 0 
L 2938 2094 
Q 2938 2591 2744 2837 
Q 2550 3084 2163 3084 
Q 1697 3084 1428 2787 
Q 1159 2491 1159 1978 
L 1159 0 
L 581 0 
L 581 3500 
L 1159 3500 
L 1159 2956 
Q 1366 3272 1645 3428 
Q 1925 3584 2291 3584 
Q 2894 3584 3203 3211 
Q 3513 2838 3513 2113 
z
" id="DejaVuSans-6e" transform="scale(0.015625)">
          </path>
          <path d="M 2375 4863 
L 2375 4384 
L 1825 4384 
Q 1516 4384 1395 4259 
Q 1275 4134 1275 3809 
L 1275 3500 
L 2222 3500 
L 2222 3053 
L 1275 3053 
L 1275 0 
L 697 0 
L 697 3053 
L 147 3053 
L 147 3500 
L 697 3500 
L 697 3744 
Q 697 4328 969 4595 
Q 1241 4863 1831 4863 
L 2375 4863 
z
" id="DejaVuSans-66" transform="scale(0.015625)">
          </path>
          <path d="M 2834 3397 
L 2834 2853 
Q 2591 2978 2328 3040 
Q 2066 3103 1784 3103 
Q 1356 3103 1142 2972 
Q 928 2841 928 2578 
Q 928 2378 1081 2264 
Q 1234 2150 1697 2047 
L 1894 2003 
Q 2506 1872 2764 1633 
Q 3022 1394 3022 966 
Q 3022 478 2636 193 
Q 2250 -91 1575 -91 
Q 1294 -91 989 -36 
Q 684 19 347 128 
L 347 722 
Q 666 556 975 473 
Q 1284 391 1588 391 
Q 1994 391 2212 530 
Q 2431 669 2431 922 
Q 2431 1156 2273 1281 
Q 2116 1406 1581 1522 
L 1381 1569 
Q 847 1681 609 1914 
Q 372 2147 372 2553 
Q 372 3047 722 3315 
Q 1072 3584 1716 3584 
Q 2034 3584 2315 3537 
Q 2597 3491 2834 3397 
z
" id="DejaVuSans-73" transform="scale(0.015625)">
          </path>
         </defs>
         <use xlink:href="#DejaVuSans-43">
         </use>
         <use x="69.824219" xlink:href="#DejaVuSans-6f">
         </use>
         <use x="131.005859" xlink:href="#DejaVuSans-6e">
         </use>
         <use x="194.384766" xlink:href="#DejaVuSans-66">
         </use>
         <use x="229.589844" xlink:href="#DejaVuSans-75">
         </use>
         <use x="292.96875" xlink:href="#DejaVuSans-73">
         </use>
         <use x="345.068359" xlink:href="#DejaVuSans-69">
         </use>
         <use x="372.851562" xlink:href="#DejaVuSans-6f">
         </use>
         <use x="434.033203" xlink:href="#DejaVuSans-6e">
         </use>
         <use x="497.412109" xlink:href="#DejaVuSans-20">
         </use>
         <use x="529.199219" xlink:href="#DejaVuSans-74">
         </use>
         <use x="568.408203" xlink:href="#DejaVuSans-61">
         </use>
         <use x="629.6875" xlink:href="#DejaVuSans-62">
         </use>
         <use x="693.164062" xlink:href="#DejaVuSans-6c">
         </use>
         <use x="720.947266" xlink:href="#DejaVuSans-65">
         </use>
        </g>
       </g>
      </g>
      <defs>
       <clippath id="p7e321d4209">
        <rect height="217.44" width="217.44" x="21.88" y="50.4">
        </rect>
       </clippath>
      </defs>
     </svg>
    </div>
   </div>
  </div>
 </div>
</div>
<div class="cell border-box-sizing text_cell rendered">
 <div class="prompt input_prompt">
 </div>
 <div class="inner_cell">
  <div class="text_cell_render border-box-sizing rendered_html">
   <p>
    This was the last part of a 5-part tutorial on how to implement neural networks from scratch in Python:
   </p>
   <ul>
    <li>
     <a href="{% post_url /blog/neural_net_implementation/2015-06-12-neural-network-implementation-part01 %}">
      Part 1: Gradient descent
     </a>
    </li>
    <li>
     <a href="{% post_url /blog/neural_net_implementation/2015-06-13-neural-network-implementation-part02 %}">
      Part 2: Classification
     </a>
    </li>
    <li>
     <a href="{% post_url /blog/neural_net_implementation/2015-06-14-neural-network-implementation-part03 %}">
      Part 3: Hidden layers trained by backpropagation
     </a>
    </li>
    <li>
     <a href="{% post_url /blog/neural_net_implementation/2015-06-15-neural-network-implementation-part04 %}">
      Part 4: Vectorization of the operations
     </a>
    </li>
    <li>
     <a href="{% post_url /blog/neural_net_implementation/2015-06-16-neural-network-implementation-part05 %}">
      Part 5: Generalization to multiple layers (this)
     </a>
    </li>
   </ul>
  </div>
 </div>
</div>
<div class="cell border-box-sizing code_cell rendered">
 <div class="input">
  <div class="prompt input_prompt">
   In [19]:
  </div>
  <div class="inner_cell">
   <div class="input_area collapsed">
    <div class="collapse_expand_button fa-1x">
    </div>
    <div class="highlight hl-ipython3">
     <pre><span></span><span class="c1"># Python package versions used</span>
<span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> --python
<span class="o">%</span><span class="k">watermark</span> --iversions
<span class="c1">#</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class="output_wrapper">
  <div class="output">
   <div class="output_area">
    <div class="prompt">
    </div>
    <div class="output_subarea output_stream output_stdout output_text">
     <pre>Python implementation: CPython
Python version       : 3.9.4
IPython version      : 7.23.1

numpy     : 1.20.2
sklearn   : 0.24.2
seaborn   : 0.11.1
matplotlib: 3.4.2

</pre>
    </div>
   </div>
  </div>
 </div>
</div>
